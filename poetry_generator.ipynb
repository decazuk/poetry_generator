{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poetry Generation\n",
    "In this project, I'll present a way of generating Chinese poetry using RNNs.\n",
    "## Get the Data\n",
    "The data is alread privoded. This dataset include more than 40000 Chinese poetries without title. Poetries are seperated by '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helper\n",
    "\n",
    "data_dir = \"./data/five_words_poetries.txt\"\n",
    "text = helper.load_data(data_dir)\n",
    "text = text.replace('\\n','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique Chinese Character: 3015\n",
      "Number of poetries: 1\n",
      "Averate number of charactors in each poetry: 55224.0\n",
      "\n",
      "The sentences 100 to 110:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "view_poetry_range = (100, 110)\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique Chinese Character: {}'.format(len({word: None for word in list(text)})))\n",
    "\n",
    "poetries = text.split('\\n')\n",
    "print('Number of poetries: {}'.format(len(poetries)))\n",
    "\n",
    "charactors_count = [len(list(poetry)) for poetry in poetries]\n",
    "print(\"Averate number of charactors in each poetry: {}\".format(np.average(charactors_count)))\n",
    "print()\n",
    "print('The sentences {} to {}:'.format(*view_poetry_range))\n",
    "print('\\n'.join(poetries[view_poetry_range[0]:view_poetry_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Functions\n",
    "- Lookup Table\n",
    "\n",
    "### Lookup Table\n",
    "- Dictionary to go from the charactors to an id, we'll call `vocab_to_int`\n",
    "- Dictionary to go from the id to charactors, we'll call `int_to_vocab`\n",
    "\n",
    "Return these dictionaries in the following tuple (vocab_to_int, int_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    vocab = set(text)\n",
    "    vocab_to_int = {word: i for i, word in enumerate(vocab)}\n",
    "    int_to_vocab = dict(enumerate(vocab))\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "helper.preprocess_and_save_data(data_dir, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Point\n",
    "The preprocessed data has been saved to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helper\n",
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "Build the network by following functions below:\n",
    "- get_inputs\n",
    "- get_init_cell\n",
    "- get_embed\n",
    "- build_rnn\n",
    "- build_nn\n",
    "- get_batches\n",
    "\n",
    "### Check the Version of Tensorflow and Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 1.1.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use Tensorflow version 1.0 or newer'\n",
    "print('Tensorflow Version: {}'.format(tf.__version__))\n",
    "\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input\n",
    "Input create the following placeholders:\n",
    "- Input text placeholder named \"input\" using the TF Placeholder name paramteter.\n",
    "- Targets placeholder\n",
    "- Learning Rate placeholder\n",
    "\n",
    "return the placeholders in the following tuple (Input, Targets, LearningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?)\n",
      "Test Passed\n"
     ]
    }
   ],
   "source": [
    "def get_inputs():\n",
    "    input_placeholder = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    target_placeholder = tf.placeholder(tf.int32, [None, None], name='target')\n",
    "    learning_rate_placeholder = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    return input_placeholder, target_placeholder, learning_rate_placeholder\n",
    "\n",
    "tests.test_get_inputs(get_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build RNN Cell and Initialize\n",
    "Stack one or more BasicLSTMCells in a MultiRNNCell\n",
    "- The Rnn size shoulde be set using rnn_size\n",
    "- Initialize Cell State using the MultiRNNCell's zero_state() function\n",
    "    - Apply the name \"initial_state\" to the initial state using tf.identity()\n",
    "   \n",
    "Return the cell and inital state in the following tuple `(Cell, InitialState)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed\n"
     ]
    }
   ],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    num_layers = 2\n",
    "    def create_cell(rnn_size):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        lstm = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=0.75)\n",
    "        return lstm\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([create_cell(rnn_size) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    initial_state = tf.identity(initial_state, name=\"initial_state\")\n",
    "    return cell, initial_state\n",
    "\n",
    "tests.test_get_init_cell(get_init_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "Apply embedding to input_data using Tensorflow. Return the embedded sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed\n"
     ]
    }
   ],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    embed = tf.contrib.layers.embed_sequence(input_data, vocab_size=vocab_size, embed_dim=embed_dim)\n",
    "    return embed\n",
    "\n",
    "tests.test_get_embed(get_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build RNN\n",
    "Time to use the cell to create a RNN.\n",
    "- Build the RNN using the tf.nn.dynamic_rnn()\n",
    "    - Apply the name \"final_state\" to the final state using tf.identity()\n",
    "    \n",
    "Return the outputs and final_state state in the folowing tuple `(Outpus, FinalState)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed\n"
     ]
    }
   ],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "    final_state = tf.identity(state, name=\"final_state\")\n",
    "    return outputs, final_state\n",
    "\n",
    "tests.test_build_rnn(build_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "\n",
    "- Apply embedding to input_data using `get_embed(input_data, vocab_size, embed_dim)` function.\n",
    "- Build RNN using `cell` and your `build_rnn(cell, inputs)` function\n",
    "- Apply a fully connected laye3r with a linear activation and `vocab_size` as the number of outputs.\n",
    "\n",
    "Return the logits and final state in the following tuple (Logits, FinalState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed\n"
     ]
    }
   ],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    embed = get_embed(input_data, vocab_size, embed_dim)\n",
    "    rnn_outputs, final_state = build_rnn(cell, embed)\n",
    "    logits = tf.contrib.layers.fully_connected(rnn_outputs, vocab_size, activation_fn=None)\n",
    "    return logits, final_state\n",
    "\n",
    "tests.test_build_nn(build_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed\n"
     ]
    }
   ],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    n_batches = int(len(int_text) / (batch_size * seq_length))\n",
    "    xdata = np.array(int_text[:n_batches * batch_size * seq_length])\n",
    "    ydata = np.array(int_text[1:n_batches * batch_size * seq_length + 1])\n",
    "    x_batches = np.split(xdata.reshape(batch_size, -1), n_batches, 1)\n",
    "    y_batches = np.split(ydata.reshape(batch_size, -1), n_batches, 1)\n",
    "    \n",
    "    y_batches[-1][-1][-1] = int_text[0]\n",
    "    \n",
    "    return np.array(list(zip(x_batches, y_batches)))\n",
    "\n",
    "tests.test_get_batches(get_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 128\n",
    "batch_size = 256\n",
    "rnn_size = 512\n",
    "embed_dim = 512\n",
    "seq_length = 24\n",
    "learning_rate = 0.005\n",
    "show_every_n_batches = 50\n",
    "\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Graph\n",
    "Build the graph using the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "    \n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "    \n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]])\n",
    "    )\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    \n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Train the neural network on the rpeprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 256, 24)\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "print(batches[0].shape)\n",
    "# with tf.Session(graph=train_graph) as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#     for epoch_i in range(num_epochs):\n",
    "#         state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "#         for batch_i, (x, y) in enumerate(batches):\n",
    "#             feed = {\n",
    "#                 input_text: x,\n",
    "#                 targets: y,\n",
    "#                 initial_state: state,\n",
    "#                 lr: learning_rate\n",
    "#             }\n",
    "#             train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "            \n",
    "#             if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "#                 print('Epoch {:>3} Batch {:>4}/{} train_loss = {:.3f}'.format(\n",
    "#                     epoch_i,\n",
    "#                     batch_i,\n",
    "#                     len(batches),\n",
    "#                     train_loss))\n",
    "    \n",
    "#     saver = tf.train.Saver()\n",
    "#     saver.save(sess, save_dir)\n",
    "#     print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Parameters\n",
    "Save seq_length and save_dir for generating a new TV script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "helper.save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab = helper.load_preprocess()\n",
    "seq_length = 25\n",
    "load_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Functions\n",
    "### Get Tensors\n",
    "\n",
    "Get tensors from loaded_graph using the function get_tensor_by_name(). Get the tensors using the following names:\n",
    "- \"input:0\"\n",
    "- \"initial_state:0\"\n",
    "- \"final_state:0\"\n",
    "- \"probs:0\"\n",
    "\n",
    "Return the tensor in the following tuple `(InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed\n"
     ]
    }
   ],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    input_tensor = loaded_graph.get_tensor_by_name(\"input:0\")\n",
    "    initial_state = loaded_graph.get_tensor_by_name(\"initial_state:0\")\n",
    "    final_state = loaded_graph.get_tensor_by_name(\"final_state:0\")\n",
    "    probs = loaded_graph.get_tensor_by_name(\"probs:0\")\n",
    "    return input_tensor, initial_state, final_state, probs\n",
    "\n",
    "tests.test_get_tensors(get_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Word\n",
    "Select the next word using probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed\n"
     ]
    }
   ],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    return int_to_vocab[np.random.choice(len(probabilities), size=1, p=probabilities)[0]]\n",
    "\n",
    "tests.test_pick_word(pick_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_5_words_poetry_word(probabilities, int_to_vocab, gen_sentences_length):\n",
    "    result = \"\"\n",
    "    if gen_sentences_length == 5 or gen_sentences_length == 17:\n",
    "        result = '，'\n",
    "    elif gen_sentences_length == 11 or gen_sentences_length == 23:\n",
    "        result = '。'\n",
    "    else:\n",
    "        choose = np.random.choice(len(probabilities), size=30, p=probabilities)\n",
    "        for i in range(30):\n",
    "            word = int_to_vocab[choose[i]]\n",
    "            if (word is not '，') and (word is not '。'):\n",
    "                result = word\n",
    "                break;\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "青山，巴云，钓台。行吹。相忆，乱生，昨夜独夜归。两\n"
     ]
    }
   ],
   "source": [
    "gen_length = 24\n",
    "\n",
    "prim_word = '青'\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "    \n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "    \n",
    "    gen_sentences = [prim_word]\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "    \n",
    "    for n in range(gen_length):\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "        probabilities, prev_state = sess.run([probs, final_state],\n",
    "                                            {input_text: dyn_input, initial_state: prev_state})\n",
    "        pred_word = pick_5_words_poetry_word(probabilities[dyn_seq_length-1], int_to_vocab, len(gen_sentences))\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    poetry = ''.join(gen_sentences)\n",
    "    print(poetry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Chinese Poetry\n",
    "This will generate chinese poetry. Set get_length to the length of poetry you want generate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Generate Five words poetry\n",
    "This will generate five words poetry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_poetry(prim_words, gen_length, load_dir, seq_length):\n",
    "    loaded_graph = tf.Graph()\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "        input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "        gen_sentences = list(prim_words)\n",
    "        prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "        for n in range(gen_length-len(gen_sentences)):\n",
    "            dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "            dyn_seq_length = len(dyn_input[0])\n",
    "            probabilities, prev_state = sess.run([probs, final_state],\n",
    "                                                {input_text: dyn_input, initial_state: prev_state})\n",
    "            pred_word = pick_5_words_poetry_word(probabilities[dyn_seq_length-1], int_to_vocab, len(gen_sentences))\n",
    "            gen_sentences.append(pred_word)\n",
    "        poetry = ''.join(gen_sentences)\n",
    "        return poetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'月明阶水里，更是到秋至。\\n叶出星云，，松草连门。'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_poetry(\"月明阶\", gen_length=24, load_dir=load_dir, seq_length=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
