{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poetry Generation\n",
    "In this project, I'll present a way of generating Chinese poetry using RNNs.\n",
    "## Get the Data\n",
    "The data is alread privoded. This dataset include more than 40000 Chinese poetries without title. Poetries are seperated by '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "\n",
    "data_dir = \"./data/five_words_poetries.txt\"\n",
    "text = helper.load_data(data_dir)\n",
    "text = text.replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique Chinese Character: 3015\n",
      "Number of poetries: 1\n",
      "Averate number of charactors in each poetry: 55224.0\n",
      "\n",
      "The sentences 100 to 110:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "view_poetry_range = (100, 110)\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique Chinese Character: {}'.format(len({word: None for word in list(text)})))\n",
    "\n",
    "poetries = text.split('\\n')\n",
    "print('Number of poetries: {}'.format(len(poetries)))\n",
    "\n",
    "charactors_count = [len(list(poetry)) for poetry in poetries]\n",
    "print(\"Averate number of charactors in each poetry: {}\".format(np.average(charactors_count)))\n",
    "print()\n",
    "print('The sentences {} to {}:'.format(*view_poetry_range))\n",
    "print('\\n'.join(poetries[view_poetry_range[0]:view_poetry_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Functions\n",
    "- Lookup Table\n",
    "\n",
    "### Lookup Table\n",
    "- Dictionary to go from the charactors to an id, we'll call `vocab_to_int`\n",
    "- Dictionary to go from the id to charactors, we'll call `int_to_vocab`\n",
    "\n",
    "Return these dictionaries in the following tuple (vocab_to_int, int_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    vocab = set(text)\n",
    "    vocab_to_int = {word: i for i, word in enumerate(vocab)}\n",
    "    int_to_vocab = dict(enumerate(vocab))\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.preprocess_and_save_data(data_dir, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Point\n",
    "The preprocessed data has been saved to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "Build the network by following functions below:\n",
    "- get_inputs\n",
    "- get_init_cell\n",
    "- get_embed\n",
    "- build_rnn\n",
    "- build_nn\n",
    "- get_batches\n",
    "\n",
    "### Check the Version of Tensorflow and Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 1.1.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use Tensorflow version 1.0 or newer'\n",
    "print('Tensorflow Version: {}'.format(tf.__version__))\n",
    "\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input\n",
    "Input create the following placeholders:\n",
    "- Input text placeholder named \"input\" using the TF Placeholder name paramteter.\n",
    "- Targets placeholder\n",
    "- Learning Rate placeholder\n",
    "\n",
    "return the placeholders in the following tuple (Input, Targets, LearningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?)\n",
      "Test Passed\n"
     ]
    }
   ],
   "source": [
    "def get_inputs():\n",
    "    input_placeholder = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    target_placeholder = tf.placeholder(tf.int32, [None, None], name='target')\n",
    "    learning_rate_placeholder = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    return input_placeholder, target_placeholder, learning_rate_placeholder\n",
    "\n",
    "tests.test_get_inputs(get_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build RNN Cell and Initialize\n",
    "Stack one or more BasicLSTMCells in a MultiRNNCell\n",
    "- The Rnn size shoulde be set using rnn_size\n",
    "- Initialize Cell State using the MultiRNNCell's zero_state() function\n",
    "    - Apply the name \"initial_state\" to the initial state using tf.identity()\n",
    "   \n",
    "Return the cell and inital state in the following tuple `(Cell, InitialState)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed\n"
     ]
    }
   ],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    num_layers = 2\n",
    "    def create_cell(rnn_size):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        lstm = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=0.75)\n",
    "        return lstm\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([create_cell(rnn_size) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    initial_state = tf.identity(initial_state, name=\"initial_state\")\n",
    "    return cell, initial_state\n",
    "\n",
    "tests.test_get_init_cell(get_init_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "Apply embedding to input_data using Tensorflow. Return the embedded sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed\n"
     ]
    }
   ],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    embed = tf.contrib.layers.embed_sequence(input_data, vocab_size=vocab_size, embed_dim=embed_dim)\n",
    "    return embed\n",
    "\n",
    "tests.test_get_embed(get_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build RNN\n",
    "Time to use the cell to create a RNN.\n",
    "- Build the RNN using the tf.nn.dynamic_rnn()\n",
    "    - Apply the name \"final_state\" to the final state using tf.identity()\n",
    "    \n",
    "Return the outputs and final_state state in the folowing tuple `(Outpus, FinalState)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed\n"
     ]
    }
   ],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "    final_state = tf.identity(state, name=\"final_state\")\n",
    "    return outputs, final_state\n",
    "\n",
    "tests.test_build_rnn(build_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "\n",
    "- Apply embedding to input_data using `get_embed(input_data, vocab_size, embed_dim)` function.\n",
    "- Build RNN using `cell` and your `build_rnn(cell, inputs)` function\n",
    "- Apply a fully connected laye3r with a linear activation and `vocab_size` as the number of outputs.\n",
    "\n",
    "Return the logits and final state in the following tuple (Logits, FinalState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed\n"
     ]
    }
   ],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    embed = get_embed(input_data, vocab_size, embed_dim)\n",
    "    rnn_outputs, final_state = build_rnn(cell, embed)\n",
    "    logits = tf.contrib.layers.fully_connected(rnn_outputs, vocab_size, activation_fn=None)\n",
    "    return logits, final_state\n",
    "\n",
    "tests.test_build_nn(build_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed\n"
     ]
    }
   ],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    n_batches = int(len(int_text) / (batch_size * seq_length))\n",
    "    xdata = np.array(int_text[:n_batches * batch_size * seq_length])\n",
    "    ydata = np.array(int_text[1:n_batches * batch_size * seq_length + 1])\n",
    "    x_batches = np.split(xdata.reshape(batch_size, -1), n_batches, 1)\n",
    "    y_batches = np.split(ydata.reshape(batch_size, -1), n_batches, 1)\n",
    "    \n",
    "    y_batches[-1][-1][-1] = int_text[0]\n",
    "    \n",
    "    return np.array(list(zip(x_batches, y_batches)))\n",
    "\n",
    "tests.test_get_batches(get_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 256\n",
    "batch_size = 256\n",
    "rnn_size = 512\n",
    "embed_dim = 512\n",
    "seq_length = 7\n",
    "learning_rate = 0.005\n",
    "show_every_n_batches = 50\n",
    "\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Graph\n",
    "Build the graph using the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "    \n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "    \n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]])\n",
    "    )\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    \n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Train the neural network on the rpeprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/32 train_loss = 8.012\n",
      "Epoch   1 Batch   18/32 train_loss = 5.930\n",
      "Epoch   3 Batch    4/32 train_loss = 5.655\n",
      "Epoch   4 Batch   22/32 train_loss = 5.496\n",
      "Epoch   6 Batch    8/32 train_loss = 5.291\n",
      "Epoch   7 Batch   26/32 train_loss = 5.125\n",
      "Epoch   9 Batch   12/32 train_loss = 4.984\n",
      "Epoch  10 Batch   30/32 train_loss = 4.737\n",
      "Epoch  12 Batch   16/32 train_loss = 4.440\n",
      "Epoch  14 Batch    2/32 train_loss = 4.207\n",
      "Epoch  15 Batch   20/32 train_loss = 3.921\n",
      "Epoch  17 Batch    6/32 train_loss = 3.645\n",
      "Epoch  18 Batch   24/32 train_loss = 3.410\n",
      "Epoch  20 Batch   10/32 train_loss = 3.158\n",
      "Epoch  21 Batch   28/32 train_loss = 2.902\n",
      "Epoch  23 Batch   14/32 train_loss = 2.601\n",
      "Epoch  25 Batch    0/32 train_loss = 2.494\n",
      "Epoch  26 Batch   18/32 train_loss = 2.320\n",
      "Epoch  28 Batch    4/32 train_loss = 2.193\n",
      "Epoch  29 Batch   22/32 train_loss = 1.953\n",
      "Epoch  31 Batch    8/32 train_loss = 1.909\n",
      "Epoch  32 Batch   26/32 train_loss = 1.777\n",
      "Epoch  34 Batch   12/32 train_loss = 1.689\n",
      "Epoch  35 Batch   30/32 train_loss = 1.551\n",
      "Epoch  37 Batch   16/32 train_loss = 1.452\n",
      "Epoch  39 Batch    2/32 train_loss = 1.399\n",
      "Epoch  40 Batch   20/32 train_loss = 1.335\n",
      "Epoch  42 Batch    6/32 train_loss = 1.331\n",
      "Epoch  43 Batch   24/32 train_loss = 1.214\n",
      "Epoch  45 Batch   10/32 train_loss = 1.157\n",
      "Epoch  46 Batch   28/32 train_loss = 1.182\n",
      "Epoch  48 Batch   14/32 train_loss = 1.127\n",
      "Epoch  50 Batch    0/32 train_loss = 1.093\n",
      "Epoch  51 Batch   18/32 train_loss = 1.073\n",
      "Epoch  53 Batch    4/32 train_loss = 1.074\n",
      "Epoch  54 Batch   22/32 train_loss = 0.975\n",
      "Epoch  56 Batch    8/32 train_loss = 0.993\n",
      "Epoch  57 Batch   26/32 train_loss = 0.989\n",
      "Epoch  59 Batch   12/32 train_loss = 1.003\n",
      "Epoch  60 Batch   30/32 train_loss = 0.917\n",
      "Epoch  62 Batch   16/32 train_loss = 0.943\n",
      "Epoch  64 Batch    2/32 train_loss = 0.862\n",
      "Epoch  65 Batch   20/32 train_loss = 0.912\n",
      "Epoch  67 Batch    6/32 train_loss = 0.880\n",
      "Epoch  68 Batch   24/32 train_loss = 0.895\n",
      "Epoch  70 Batch   10/32 train_loss = 0.838\n",
      "Epoch  71 Batch   28/32 train_loss = 0.860\n",
      "Epoch  73 Batch   14/32 train_loss = 0.930\n",
      "Epoch  75 Batch    0/32 train_loss = 0.863\n",
      "Epoch  76 Batch   18/32 train_loss = 0.836\n",
      "Epoch  78 Batch    4/32 train_loss = 0.851\n",
      "Epoch  79 Batch   22/32 train_loss = 0.867\n",
      "Epoch  81 Batch    8/32 train_loss = 0.902\n",
      "Epoch  82 Batch   26/32 train_loss = 0.864\n",
      "Epoch  84 Batch   12/32 train_loss = 0.888\n",
      "Epoch  85 Batch   30/32 train_loss = 0.849\n",
      "Epoch  87 Batch   16/32 train_loss = 0.884\n",
      "Epoch  89 Batch    2/32 train_loss = 0.846\n",
      "Epoch  90 Batch   20/32 train_loss = 0.850\n",
      "Epoch  92 Batch    6/32 train_loss = 0.876\n",
      "Epoch  93 Batch   24/32 train_loss = 0.836\n",
      "Epoch  95 Batch   10/32 train_loss = 0.841\n",
      "Epoch  96 Batch   28/32 train_loss = 0.873\n",
      "Epoch  98 Batch   14/32 train_loss = 0.867\n",
      "Epoch 100 Batch    0/32 train_loss = 0.869\n",
      "Epoch 101 Batch   18/32 train_loss = 0.857\n",
      "Epoch 103 Batch    4/32 train_loss = 0.880\n",
      "Epoch 104 Batch   22/32 train_loss = 0.850\n",
      "Epoch 106 Batch    8/32 train_loss = 0.895\n",
      "Epoch 107 Batch   26/32 train_loss = 0.879\n",
      "Epoch 109 Batch   12/32 train_loss = 0.920\n",
      "Epoch 110 Batch   30/32 train_loss = 0.878\n",
      "Epoch 112 Batch   16/32 train_loss = 0.851\n",
      "Epoch 114 Batch    2/32 train_loss = 0.856\n",
      "Epoch 115 Batch   20/32 train_loss = 0.852\n",
      "Epoch 117 Batch    6/32 train_loss = 0.869\n",
      "Epoch 118 Batch   24/32 train_loss = 0.877\n",
      "Epoch 120 Batch   10/32 train_loss = 0.874\n",
      "Epoch 121 Batch   28/32 train_loss = 0.875\n",
      "Epoch 123 Batch   14/32 train_loss = 0.855\n",
      "Epoch 125 Batch    0/32 train_loss = 0.852\n",
      "Epoch 126 Batch   18/32 train_loss = 0.881\n",
      "Epoch 128 Batch    4/32 train_loss = 0.876\n",
      "Epoch 129 Batch   22/32 train_loss = 0.839\n",
      "Epoch 131 Batch    8/32 train_loss = 0.878\n",
      "Epoch 132 Batch   26/32 train_loss = 0.882\n",
      "Epoch 134 Batch   12/32 train_loss = 0.917\n",
      "Epoch 135 Batch   30/32 train_loss = 0.841\n",
      "Epoch 137 Batch   16/32 train_loss = 0.868\n",
      "Epoch 139 Batch    2/32 train_loss = 0.853\n",
      "Epoch 140 Batch   20/32 train_loss = 0.868\n",
      "Epoch 142 Batch    6/32 train_loss = 0.892\n",
      "Epoch 143 Batch   24/32 train_loss = 0.825\n",
      "Epoch 145 Batch   10/32 train_loss = 0.821\n",
      "Epoch 146 Batch   28/32 train_loss = 0.866\n",
      "Epoch 148 Batch   14/32 train_loss = 0.898\n",
      "Epoch 150 Batch    0/32 train_loss = 0.789\n",
      "Epoch 151 Batch   18/32 train_loss = 0.856\n",
      "Epoch 153 Batch    4/32 train_loss = 0.883\n",
      "Epoch 154 Batch   22/32 train_loss = 0.905\n",
      "Epoch 156 Batch    8/32 train_loss = 0.862\n",
      "Epoch 157 Batch   26/32 train_loss = 0.892\n",
      "Epoch 159 Batch   12/32 train_loss = 0.940\n",
      "Epoch 160 Batch   30/32 train_loss = 0.874\n",
      "Epoch 162 Batch   16/32 train_loss = 0.902\n",
      "Epoch 164 Batch    2/32 train_loss = 0.840\n",
      "Epoch 165 Batch   20/32 train_loss = 0.849\n",
      "Epoch 167 Batch    6/32 train_loss = 0.864\n",
      "Epoch 168 Batch   24/32 train_loss = 0.852\n",
      "Epoch 170 Batch   10/32 train_loss = 0.813\n",
      "Epoch 171 Batch   28/32 train_loss = 0.863\n",
      "Epoch 173 Batch   14/32 train_loss = 0.894\n",
      "Epoch 175 Batch    0/32 train_loss = 0.864\n",
      "Epoch 176 Batch   18/32 train_loss = 0.861\n",
      "Epoch 178 Batch    4/32 train_loss = 0.895\n",
      "Epoch 179 Batch   22/32 train_loss = 0.881\n",
      "Epoch 181 Batch    8/32 train_loss = 0.914\n",
      "Epoch 182 Batch   26/32 train_loss = 0.847\n",
      "Epoch 184 Batch   12/32 train_loss = 0.940\n",
      "Epoch 185 Batch   30/32 train_loss = 0.884\n",
      "Epoch 187 Batch   16/32 train_loss = 0.910\n",
      "Epoch 189 Batch    2/32 train_loss = 0.882\n",
      "Epoch 190 Batch   20/32 train_loss = 0.836\n",
      "Epoch 192 Batch    6/32 train_loss = 0.846\n",
      "Epoch 193 Batch   24/32 train_loss = 0.892\n",
      "Epoch 195 Batch   10/32 train_loss = 0.885\n",
      "Epoch 196 Batch   28/32 train_loss = 0.879\n",
      "Epoch 198 Batch   14/32 train_loss = 0.846\n",
      "Epoch 200 Batch    0/32 train_loss = 0.859\n",
      "Epoch 201 Batch   18/32 train_loss = 0.865\n",
      "Epoch 203 Batch    4/32 train_loss = 0.919\n",
      "Epoch 204 Batch   22/32 train_loss = 0.871\n",
      "Epoch 206 Batch    8/32 train_loss = 0.885\n",
      "Epoch 207 Batch   26/32 train_loss = 0.845\n",
      "Epoch 209 Batch   12/32 train_loss = 0.919\n",
      "Epoch 210 Batch   30/32 train_loss = 0.921\n",
      "Epoch 212 Batch   16/32 train_loss = 0.888\n",
      "Epoch 214 Batch    2/32 train_loss = 0.871\n",
      "Epoch 215 Batch   20/32 train_loss = 0.867\n",
      "Epoch 217 Batch    6/32 train_loss = 0.852\n",
      "Epoch 218 Batch   24/32 train_loss = 0.885\n",
      "Epoch 220 Batch   10/32 train_loss = 0.823\n",
      "Epoch 221 Batch   28/32 train_loss = 0.896\n",
      "Epoch 223 Batch   14/32 train_loss = 0.912\n",
      "Epoch 225 Batch    0/32 train_loss = 0.905\n",
      "Epoch 226 Batch   18/32 train_loss = 0.894\n",
      "Epoch 228 Batch    4/32 train_loss = 0.858\n",
      "Epoch 229 Batch   22/32 train_loss = 0.899\n",
      "Epoch 231 Batch    8/32 train_loss = 0.936\n",
      "Epoch 232 Batch   26/32 train_loss = 0.894\n",
      "Epoch 234 Batch   12/32 train_loss = 0.897\n",
      "Epoch 235 Batch   30/32 train_loss = 0.856\n",
      "Epoch 237 Batch   16/32 train_loss = 0.915\n",
      "Epoch 239 Batch    2/32 train_loss = 0.887\n",
      "Epoch 240 Batch   20/32 train_loss = 0.904\n",
      "Epoch 242 Batch    6/32 train_loss = 0.901\n",
      "Epoch 243 Batch   24/32 train_loss = 0.850\n",
      "Epoch 245 Batch   10/32 train_loss = 0.896\n",
      "Epoch 246 Batch   28/32 train_loss = 0.924\n",
      "Epoch 248 Batch   14/32 train_loss = 0.940\n",
      "Epoch 250 Batch    0/32 train_loss = 0.904\n",
      "Epoch 251 Batch   18/32 train_loss = 0.884\n",
      "Epoch 253 Batch    4/32 train_loss = 0.928\n",
      "Epoch 254 Batch   22/32 train_loss = 0.912\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate\n",
    "            }\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "            \n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{} train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Parameters\n",
    "Save seq_length and save_dir for generating a new TV script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "helper.save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab = helper.load_preprocess()\n",
    "seq_length = 25\n",
    "load_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Functions\n",
    "### Get Tensors\n",
    "\n",
    "Get tensors from loaded_graph using the function get_tensor_by_name(). Get the tensors using the following names:\n",
    "- \"input:0\"\n",
    "- \"initial_state:0\"\n",
    "- \"final_state:0\"\n",
    "- \"probs:0\"\n",
    "\n",
    "Return the tensor in the following tuple `(InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed\n"
     ]
    }
   ],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    input_tensor = loaded_graph.get_tensor_by_name(\"input:0\")\n",
    "    initial_state = loaded_graph.get_tensor_by_name(\"initial_state:0\")\n",
    "    final_state = loaded_graph.get_tensor_by_name(\"final_state:0\")\n",
    "    probs = loaded_graph.get_tensor_by_name(\"probs:0\")\n",
    "    return input_tensor, initial_state, final_state, probs\n",
    "\n",
    "tests.test_get_tensors(get_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Word\n",
    "Select the next word using probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed\n"
     ]
    }
   ],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    return int_to_vocab[np.random.choice(len(probabilities), size=1, p=probabilities)[0]]\n",
    "\n",
    "tests.test_pick_word(pick_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_5_words_poetry_word(probabilities, int_to_vocab, gen_sentences_length):\n",
    "    result = \"\"\n",
    "    if gen_sentences_length == 5 or gen_sentences_length == 17:\n",
    "        result = '，'\n",
    "    elif gen_sentences_length == 11 or gen_sentences_length == 23:\n",
    "        result = '。'\n",
    "    else:\n",
    "        choose = np.random.choice(len(probabilities), size=10, p=probabilities)\n",
    "        print(choose)\n",
    "        for i in range(10):\n",
    "            word = int_to_vocab[choose[i]]\n",
    "            if (word is not '，') and (word is not '。'):\n",
    "                result = word\n",
    "                break;\n",
    "          \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "青山上。\n",
      "，轩双国孤情。\n",
      "桥南菰叶，残景奈赊何。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gen_length = 24\n",
    "\n",
    "prim_word = '青'\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "    \n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "    \n",
    "    gen_sentences = [prim_word]\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "    \n",
    "    for n in range(gen_length):\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "        \n",
    "        probabilities, prev_state = sess.run([probs, final_state],\n",
    "                                            {input_text: dyn_input, initial_state: prev_state})\n",
    "        pred_word = pick_5_words_poetry_word(probabilities[dyn_seq_length-1], int_to_vocab, len(gen_sentences))\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    poetry = ''.join(gen_sentences)\n",
    "    print(poetry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Chinese Poetry\n",
    "This will generate chinese poetry. Set get_length to the length of poetry you want generate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Generate Five words poetry\n",
    "This will generate five words poetry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
