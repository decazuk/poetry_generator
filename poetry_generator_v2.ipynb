{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poetry Generation v2\n",
    "In this project, I'll use new method for getting batches and try to get a better result.\n",
    "\n",
    "### Get the Data\n",
    "I will use full poetry dataset this point, not only the five words poetries. Poetries are seperated by '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helper\n",
    "\n",
    "data_dir = './data/poetry_data.txt'\n",
    "\n",
    "text = helper.load_data(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique Chinese Character: 7077\n",
      "Number of poetries: 41134\n",
      "Averate number of charactors in each poetry: 61.19105849175864\n",
      "Max number of charactors in poetry: 255\n",
      "\n",
      "The sentences 100 to 110:\n",
      "自古逢秋悲寂寥，我言秋日胜春朝。晴空一鹤排云上，便引诗情到碧宵。\n",
      "男儿何不带吴钩，收取关山五十州。请君暂上凌烟阁，若个书生万户侯。\n",
      "早梅发高树，回映楚天碧。朔吹飘夜香，繁霜滋晓白。欲为万里赠，杳杳山水隔。寒英坐销落，何用慰远客？\n",
      "孤山寺北贾亭西，水面初平云脚低。几处早莺争暖树，谁家新燕啄春泥。乱花渐欲迷人眼，浅草才能没马蹄。最爱湖东行不足，绿杨阴里白沙堤。\n",
      "闽国扬帆后，蟾蜍亏复圆。秋风吹渭水，落叶满长安。此地聚会夕，当时雷雨寒。兰桡殊未返，消息海云端。\n",
      "昔看黄菊与君别，今听玄蝉我却回。五夜飕溜枕前觉，一夜颜妆镜中来。马思边草拳毛动，雕眄青云睡眼开。天地肃清堪开望，为君扶病上高台。\n",
      "巴山楚水凄凉地，二十三年弃置身。怀旧空吟闻笛赋，到乡翻似烂柯人。沉舟侧畔千帆过，病树前头万木春。今日听君歌一曲，暂凭杯酒长精神。\n",
      "荒村带返照，落叶乱纷纷。古路无行客，寒山独见君。野桥经雨断，涧水向田分。不为怜同病，何人到白云。\n",
      "一封朝奏九重天，夕贬潮州路八千。欲为圣明除弊事，肯将衰朽惜残年。云横秦岭家何在，雪拥蓝关马不前。知汝远来应有意，好收吾骨瘴江边。\n",
      "清晨入古寺，初日照高林。竹径通幽处，禅房花木深。山光悦鸟性，潭影空人心。万籁此俱寂，但余钟磬声。\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "view_poetry_range = (100, 110)\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique Chinese Character: {}'.format(len({word: None for word in list(text)})))\n",
    "\n",
    "poetries = text.split('\\n')\n",
    "print('Number of poetries: {}'.format(len(poetries)))\n",
    "\n",
    "charactors_count = [len(list(poetry)) for poetry in poetries]\n",
    "print(\"Averate number of charactors in each poetry: {}\".format(np.average(charactors_count)))\n",
    "print(\"Max number of charactors in poetry: {}\".format(np.max(charactors_count)))\n",
    "print()\n",
    "print('The sentences {} to {}:'.format(*view_poetry_range))\n",
    "print('\\n'.join(poetries[view_poetry_range[0]:view_poetry_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Functions\n",
    "- Lookup Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    vocab = set(text)\n",
    "    vocab_to_int = {word: i for i, word in enumerate(vocab)}\n",
    "    int_to_vocab = dict(enumerate(vocab))\n",
    "    vocab_to_int['PAD'] = len(vocab_to_int)\n",
    "    vocab_to_int['EOS'] = len(vocab_to_int)\n",
    "    int_to_vocab[vocab_to_int['PAD']] = 'PAD'\n",
    "    int_to_vocab[vocab_to_int['EOS']] = 'EOS'\n",
    "    return vocab_to_int, int_to_vocab\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_to_int, int_to_vocab = create_lookup_tables(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "### Check the version of Tensorflow and Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 1.1.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use Tensorflow version 1.0 or newer'\n",
    "print('Tensorflow Version: {}'.format(tf.__version__))\n",
    "\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?)\n",
      "Test Passed\n"
     ]
    }
   ],
   "source": [
    "def get_inputs():\n",
    "    input_placeholder = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    target_placeholder = tf.placeholder(tf.int32, [None, None], name='target')\n",
    "    learning_rate_placeholder = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    return input_placeholder, target_placeholder, learning_rate_placeholder\n",
    "\n",
    "tests.test_get_inputs(get_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed\n"
     ]
    }
   ],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    embed = tf.contrib.layers.embed_sequence(input_data, vocab_size=vocab_size, embed_dim=embed_dim)\n",
    "    return embed\n",
    "\n",
    "tests.test_get_embed(get_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RNN Cell and Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_init_cell(batch_size, rnn_size, num_layers):\n",
    "    def create_cell(rnn_size):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        lstm = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=0.75)\n",
    "        return lstm\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([create_cell(rnn_size) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    initial_state = tf.identity(initial_state, name=\"initial_state\")\n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed\n"
     ]
    }
   ],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "    final_state = tf.identity(state, name=\"final_state\")\n",
    "    return outputs, final_state\n",
    "\n",
    "tests.test_build_rnn(build_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed\n"
     ]
    }
   ],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    embed = get_embed(input_data, vocab_size, embed_dim)\n",
    "    rnn_outputs, final_state = build_rnn(cell, embed)\n",
    "    logits = tf.contrib.layers.fully_connected(rnn_outputs, vocab_size, activation_fn=None)\n",
    "    return logits, final_state\n",
    "\n",
    "tests.test_build_nn(build_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches\n",
    "This is the most important part in this project.Both X data and Y data are from one whole poetry. For training length, I will use '<PAD>' to fill the short poetries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_poetry_batch(poetry_batch, pad_int, vocab_to_int):\n",
    "    int_poetry_batch = [[vocab_to_int[word] for word in list(poetry)] for poetry in poetry_batch]\n",
    "    max_poetry_length = max([len(poetry) for poetry in int_poetry_batch])\n",
    "    return [poetry + [pad_int] * (max_poetry_length - len(poetry)) for poetry in int_poetry_batch]\n",
    "\n",
    "# tests.test_pad_poetry_batch(pad_poetry_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_validate_poetries(text):\n",
    "    poetries = text.split(\"\\n\")\n",
    "    validate_poetries = []\n",
    "    for poetry in poetries:\n",
    "        if len(poetry) > 1:\n",
    "            validate_poetries.append(poetry)\n",
    "    return validate_poetries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(text, batch_size, vocab_to_int, pad_id, eos_id):\n",
    "    poetries = get_validate_poetries(text)\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    x_batches_lengths = []\n",
    "    y_batches_lengths = []\n",
    "    for batch_i in range(0, len(poetries) // batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        poetry_batch = poetries[start_i:start_i+batch_size]\n",
    "        source_batch = poetry_batch\n",
    "        target_batch = [poetry[1:]+poetry[0] for poetry in source_batch]\n",
    "        pad_sources_batch = pad_poetry_batch(poetry_batch, pad_id, vocab_to_int)\n",
    "        pad_targets_batch = pad_poetry_batch(target_batch, pad_id, vocab_to_int)\n",
    "        \n",
    "        pad_sources_lengths = [len(source) for source in pad_sources_batch]\n",
    "        pad_targets_lengths = [len(target) for target in pad_targets_batch]\n",
    "        \n",
    "        x_batches.append(np.array(pad_sources_batch))\n",
    "        y_batches.append(np.array(pad_targets_batch))\n",
    "        x_batches_lengths.append(pad_sources_lengths)\n",
    "        y_batches_lengths.append(pad_targets_lengths)\n",
    "    \n",
    "    return x_batches, y_batches, np.array(x_batches_lengths), np.array(y_batches_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "\n",
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 128\n",
    "batch_size = 15\n",
    "rnn_size = 512\n",
    "num_layers = 2\n",
    "embed_dim = 512\n",
    "learning_rate = 0.003\n",
    "show_every_n_batches = 50\n",
    "\n",
    "save_dir = './save_v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size, num_layers)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "    \n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "    \n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]])\n",
    "    )\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    \n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/2739 train_loss = 8.865\n",
      "Epoch   0 Batch   50/2739 train_loss = 4.824\n",
      "Epoch   0 Batch  100/2739 train_loss = 4.426\n",
      "Epoch   0 Batch  150/2739 train_loss = 4.471\n",
      "Epoch   0 Batch  200/2739 train_loss = 3.748\n",
      "Epoch   0 Batch  250/2739 train_loss = 3.043\n",
      "Epoch   0 Batch  300/2739 train_loss = 3.432\n",
      "Epoch   0 Batch  350/2739 train_loss = 3.538\n",
      "Epoch   0 Batch  400/2739 train_loss = 2.403\n",
      "Epoch   0 Batch  450/2739 train_loss = 3.943\n",
      "Epoch   0 Batch  500/2739 train_loss = 4.262\n",
      "Epoch   0 Batch  550/2739 train_loss = 3.410\n",
      "Epoch   0 Batch  600/2739 train_loss = 1.993\n",
      "Epoch   0 Batch  650/2739 train_loss = 2.777\n",
      "Epoch   0 Batch  700/2739 train_loss = 3.658\n",
      "Epoch   0 Batch  750/2739 train_loss = 4.233\n",
      "Epoch   0 Batch  800/2739 train_loss = 2.488\n",
      "Epoch   0 Batch  850/2739 train_loss = 3.012\n",
      "Epoch   0 Batch  900/2739 train_loss = 3.393\n",
      "Epoch   0 Batch  950/2739 train_loss = 2.316\n",
      "Epoch   0 Batch 1000/2739 train_loss = 3.038\n",
      "Epoch   0 Batch 1050/2739 train_loss = 5.574\n",
      "Epoch   0 Batch 1100/2739 train_loss = 5.753\n",
      "Epoch   0 Batch 1150/2739 train_loss = 3.356\n",
      "Epoch   0 Batch 1200/2739 train_loss = 2.191\n",
      "Epoch   0 Batch 1250/2739 train_loss = 2.344\n",
      "Epoch   0 Batch 1300/2739 train_loss = 3.123\n",
      "Epoch   0 Batch 1350/2739 train_loss = 2.973\n",
      "Epoch   0 Batch 1400/2739 train_loss = 5.699\n",
      "Epoch   0 Batch 1450/2739 train_loss = 2.807\n",
      "Epoch   0 Batch 1500/2739 train_loss = 3.375\n",
      "Epoch   0 Batch 1550/2739 train_loss = 3.713\n",
      "Epoch   0 Batch 1600/2739 train_loss = 5.770\n",
      "Epoch   0 Batch 1650/2739 train_loss = 4.067\n",
      "Epoch   0 Batch 1700/2739 train_loss = 3.973\n",
      "Epoch   0 Batch 1750/2739 train_loss = 4.584\n",
      "Epoch   0 Batch 1800/2739 train_loss = 2.763\n",
      "Epoch   0 Batch 1850/2739 train_loss = 1.436\n",
      "Epoch   0 Batch 1900/2739 train_loss = 1.866\n",
      "Epoch   0 Batch 1950/2739 train_loss = 1.621\n",
      "Epoch   0 Batch 2000/2739 train_loss = 3.209\n",
      "Epoch   0 Batch 2050/2739 train_loss = 5.588\n",
      "Epoch   0 Batch 2100/2739 train_loss = 4.508\n",
      "Epoch   0 Batch 2150/2739 train_loss = 5.253\n",
      "Epoch   0 Batch 2200/2739 train_loss = 5.063\n",
      "Epoch   0 Batch 2250/2739 train_loss = 2.440\n",
      "Epoch   0 Batch 2300/2739 train_loss = 4.011\n",
      "Epoch   0 Batch 2350/2739 train_loss = 3.454\n",
      "Epoch   0 Batch 2400/2739 train_loss = 3.681\n",
      "Epoch   0 Batch 2450/2739 train_loss = 3.314\n",
      "Epoch   0 Batch 2500/2739 train_loss = 4.862\n",
      "Epoch   0 Batch 2550/2739 train_loss = 2.966\n",
      "Epoch   0 Batch 2600/2739 train_loss = 1.519\n",
      "Epoch   0 Batch 2650/2739 train_loss = 2.964\n",
      "Epoch   0 Batch 2700/2739 train_loss = 4.896\n",
      "Epoch   1 Batch   11/2739 train_loss = 1.590\n",
      "Epoch   1 Batch   61/2739 train_loss = 3.586\n",
      "Epoch   1 Batch  111/2739 train_loss = 2.614\n",
      "Epoch   1 Batch  161/2739 train_loss = 2.891\n",
      "Epoch   1 Batch  211/2739 train_loss = 3.277\n",
      "Epoch   1 Batch  261/2739 train_loss = 2.143\n",
      "Epoch   1 Batch  311/2739 train_loss = 3.656\n",
      "Epoch   1 Batch  361/2739 train_loss = 2.067\n",
      "Epoch   1 Batch  411/2739 train_loss = 2.410\n",
      "Epoch   1 Batch  461/2739 train_loss = 2.993\n",
      "Epoch   1 Batch  511/2739 train_loss = 2.712\n",
      "Epoch   1 Batch  561/2739 train_loss = 2.298\n",
      "Epoch   1 Batch  611/2739 train_loss = 2.305\n",
      "Epoch   1 Batch  661/2739 train_loss = 2.166\n",
      "Epoch   1 Batch  711/2739 train_loss = 3.872\n",
      "Epoch   1 Batch  761/2739 train_loss = 4.858\n",
      "Epoch   1 Batch  811/2739 train_loss = 2.864\n",
      "Epoch   1 Batch  861/2739 train_loss = 3.525\n",
      "Epoch   1 Batch  911/2739 train_loss = 3.315\n",
      "Epoch   1 Batch  961/2739 train_loss = 5.199\n",
      "Epoch   1 Batch 1011/2739 train_loss = 2.465\n",
      "Epoch   1 Batch 1061/2739 train_loss = 1.362\n",
      "Epoch   1 Batch 1111/2739 train_loss = 3.042\n",
      "Epoch   1 Batch 1161/2739 train_loss = 2.095\n",
      "Epoch   1 Batch 1211/2739 train_loss = 1.956\n",
      "Epoch   1 Batch 1261/2739 train_loss = 1.949\n",
      "Epoch   1 Batch 1311/2739 train_loss = 3.142\n",
      "Epoch   1 Batch 1361/2739 train_loss = 1.857\n",
      "Epoch   1 Batch 1411/2739 train_loss = 5.126\n",
      "Epoch   1 Batch 1461/2739 train_loss = 2.268\n",
      "Epoch   1 Batch 1511/2739 train_loss = 1.853\n",
      "Epoch   1 Batch 1561/2739 train_loss = 4.131\n",
      "Epoch   1 Batch 1611/2739 train_loss = 2.751\n",
      "Epoch   1 Batch 1661/2739 train_loss = 2.570\n",
      "Epoch   1 Batch 1711/2739 train_loss = 2.334\n",
      "Epoch   1 Batch 1761/2739 train_loss = 5.282\n",
      "Epoch   1 Batch 1811/2739 train_loss = 3.192\n",
      "Epoch   1 Batch 1861/2739 train_loss = 4.669\n",
      "Epoch   1 Batch 1911/2739 train_loss = 4.728\n",
      "Epoch   1 Batch 1961/2739 train_loss = 2.488\n",
      "Epoch   1 Batch 2011/2739 train_loss = 4.698\n",
      "Epoch   1 Batch 2061/2739 train_loss = 2.095\n",
      "Epoch   1 Batch 2111/2739 train_loss = 3.822\n",
      "Epoch   1 Batch 2161/2739 train_loss = 2.782\n",
      "Epoch   1 Batch 2211/2739 train_loss = 2.393\n",
      "Epoch   1 Batch 2261/2739 train_loss = 3.253\n",
      "Epoch   1 Batch 2311/2739 train_loss = 3.413\n",
      "Epoch   1 Batch 2361/2739 train_loss = 2.751\n",
      "Epoch   1 Batch 2411/2739 train_loss = 3.038\n",
      "Epoch   1 Batch 2461/2739 train_loss = 2.987\n",
      "Epoch   1 Batch 2511/2739 train_loss = 3.052\n",
      "Epoch   1 Batch 2561/2739 train_loss = 2.807\n",
      "Epoch   1 Batch 2611/2739 train_loss = 2.000\n",
      "Epoch   1 Batch 2661/2739 train_loss = 1.711\n",
      "Epoch   1 Batch 2711/2739 train_loss = 3.788\n",
      "Epoch   2 Batch   22/2739 train_loss = 2.706\n",
      "Epoch   2 Batch   72/2739 train_loss = 3.311\n",
      "Epoch   2 Batch  122/2739 train_loss = 2.960\n",
      "Epoch   2 Batch  172/2739 train_loss = 2.372\n",
      "Epoch   2 Batch  222/2739 train_loss = 1.742\n",
      "Epoch   2 Batch  272/2739 train_loss = 2.944\n",
      "Epoch   2 Batch  322/2739 train_loss = 4.351\n",
      "Epoch   2 Batch  372/2739 train_loss = 2.009\n",
      "Epoch   2 Batch  422/2739 train_loss = 3.741\n",
      "Epoch   2 Batch  472/2739 train_loss = 1.441\n",
      "Epoch   2 Batch  522/2739 train_loss = 1.466\n",
      "Epoch   2 Batch  572/2739 train_loss = 2.553\n",
      "Epoch   2 Batch  622/2739 train_loss = 2.815\n",
      "Epoch   2 Batch  672/2739 train_loss = 3.918\n",
      "Epoch   2 Batch  722/2739 train_loss = 2.374\n",
      "Epoch   2 Batch  772/2739 train_loss = 4.216\n",
      "Epoch   2 Batch  822/2739 train_loss = 2.022\n",
      "Epoch   2 Batch  872/2739 train_loss = 2.419\n",
      "Epoch   2 Batch  922/2739 train_loss = 2.034\n",
      "Epoch   2 Batch  972/2739 train_loss = 4.520\n",
      "Epoch   2 Batch 1022/2739 train_loss = 2.531\n",
      "Epoch   2 Batch 1072/2739 train_loss = 4.274\n",
      "Epoch   2 Batch 1122/2739 train_loss = 4.472\n",
      "Epoch   2 Batch 1172/2739 train_loss = 4.922\n",
      "Epoch   2 Batch 1222/2739 train_loss = 3.839\n",
      "Epoch   2 Batch 1272/2739 train_loss = 3.076\n",
      "Epoch   2 Batch 1322/2739 train_loss = 2.349\n",
      "Epoch   2 Batch 1372/2739 train_loss = 2.145\n",
      "Epoch   2 Batch 1422/2739 train_loss = 2.503\n",
      "Epoch   2 Batch 1472/2739 train_loss = 2.013\n",
      "Epoch   2 Batch 1522/2739 train_loss = 4.748\n",
      "Epoch   2 Batch 1572/2739 train_loss = 2.700\n",
      "Epoch   2 Batch 1622/2739 train_loss = 4.389\n",
      "Epoch   2 Batch 1672/2739 train_loss = 2.286\n",
      "Epoch   2 Batch 1722/2739 train_loss = 4.065\n",
      "Epoch   2 Batch 1772/2739 train_loss = 2.244\n",
      "Epoch   2 Batch 1822/2739 train_loss = 3.328\n",
      "Epoch   2 Batch 1872/2739 train_loss = 4.332\n",
      "Epoch   2 Batch 1922/2739 train_loss = 1.898\n",
      "Epoch   2 Batch 1972/2739 train_loss = 1.496\n",
      "Epoch   2 Batch 2022/2739 train_loss = 2.080\n",
      "Epoch   2 Batch 2072/2739 train_loss = 2.433\n",
      "Epoch   2 Batch 2122/2739 train_loss = 1.127\n",
      "Epoch   2 Batch 2172/2739 train_loss = 3.014\n",
      "Epoch   2 Batch 2222/2739 train_loss = 2.110\n",
      "Epoch   2 Batch 2272/2739 train_loss = 2.015\n",
      "Epoch   2 Batch 2322/2739 train_loss = 5.680\n",
      "Epoch   2 Batch 2372/2739 train_loss = 3.106\n",
      "Epoch   2 Batch 2422/2739 train_loss = 1.858\n",
      "Epoch   2 Batch 2472/2739 train_loss = 3.414\n",
      "Epoch   2 Batch 2522/2739 train_loss = 3.603\n",
      "Epoch   2 Batch 2572/2739 train_loss = 1.807\n",
      "Epoch   2 Batch 2622/2739 train_loss = 3.447\n",
      "Epoch   2 Batch 2672/2739 train_loss = 3.065\n",
      "Epoch   2 Batch 2722/2739 train_loss = 4.926\n",
      "Epoch   3 Batch   33/2739 train_loss = 3.117\n",
      "Epoch   3 Batch   83/2739 train_loss = 2.211\n",
      "Epoch   3 Batch  133/2739 train_loss = 2.702\n",
      "Epoch   3 Batch  183/2739 train_loss = 2.324\n",
      "Epoch   3 Batch  233/2739 train_loss = 3.274\n",
      "Epoch   3 Batch  283/2739 train_loss = 1.746\n",
      "Epoch   3 Batch  333/2739 train_loss = 3.495\n",
      "Epoch   3 Batch  383/2739 train_loss = 2.366\n",
      "Epoch   3 Batch  433/2739 train_loss = 3.307\n",
      "Epoch   3 Batch  483/2739 train_loss = 1.988\n",
      "Epoch   3 Batch  533/2739 train_loss = 2.658\n",
      "Epoch   3 Batch  583/2739 train_loss = 1.363\n",
      "Epoch   3 Batch  633/2739 train_loss = 2.263\n",
      "Epoch   3 Batch  683/2739 train_loss = 1.457\n",
      "Epoch   3 Batch  733/2739 train_loss = 3.617\n",
      "Epoch   3 Batch  783/2739 train_loss = 2.566\n",
      "Epoch   3 Batch  833/2739 train_loss = 3.384\n",
      "Epoch   3 Batch  883/2739 train_loss = 4.250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3 Batch  933/2739 train_loss = 2.156\n",
      "Epoch   3 Batch  983/2739 train_loss = 4.513\n",
      "Epoch   3 Batch 1033/2739 train_loss = 1.435\n",
      "Epoch   3 Batch 1083/2739 train_loss = 1.709\n",
      "Epoch   3 Batch 1133/2739 train_loss = 3.589\n",
      "Epoch   3 Batch 1183/2739 train_loss = 2.487\n",
      "Epoch   3 Batch 1233/2739 train_loss = 1.956\n",
      "Epoch   3 Batch 1283/2739 train_loss = 5.002\n",
      "Epoch   3 Batch 1333/2739 train_loss = 4.647\n",
      "Epoch   3 Batch 1383/2739 train_loss = 2.672\n",
      "Epoch   3 Batch 1433/2739 train_loss = 2.021\n",
      "Epoch   3 Batch 1483/2739 train_loss = 4.044\n",
      "Epoch   3 Batch 1533/2739 train_loss = 2.474\n",
      "Epoch   3 Batch 1583/2739 train_loss = 2.588\n",
      "Epoch   3 Batch 1633/2739 train_loss = 1.516\n",
      "Epoch   3 Batch 1683/2739 train_loss = 2.472\n",
      "Epoch   3 Batch 1733/2739 train_loss = 3.790\n",
      "Epoch   3 Batch 1783/2739 train_loss = 4.516\n",
      "Epoch   3 Batch 1833/2739 train_loss = 3.521\n",
      "Epoch   3 Batch 1883/2739 train_loss = 4.164\n",
      "Epoch   3 Batch 1933/2739 train_loss = 3.557\n",
      "Epoch   3 Batch 1983/2739 train_loss = 4.736\n",
      "Epoch   3 Batch 2033/2739 train_loss = 4.790\n",
      "Epoch   3 Batch 2083/2739 train_loss = 3.901\n",
      "Epoch   3 Batch 2133/2739 train_loss = 3.813\n",
      "Epoch   3 Batch 2183/2739 train_loss = 2.787\n",
      "Epoch   3 Batch 2233/2739 train_loss = 2.796\n",
      "Epoch   3 Batch 2283/2739 train_loss = 2.525\n",
      "Epoch   3 Batch 2333/2739 train_loss = 2.672\n",
      "Epoch   3 Batch 2383/2739 train_loss = 1.577\n",
      "Epoch   3 Batch 2433/2739 train_loss = 4.705\n",
      "Epoch   3 Batch 2483/2739 train_loss = 1.410\n",
      "Epoch   3 Batch 2533/2739 train_loss = 2.908\n",
      "Epoch   3 Batch 2583/2739 train_loss = 2.363\n",
      "Epoch   3 Batch 2633/2739 train_loss = 2.374\n",
      "Epoch   3 Batch 2683/2739 train_loss = 1.754\n",
      "Epoch   3 Batch 2733/2739 train_loss = 1.467\n",
      "Epoch   4 Batch   44/2739 train_loss = 2.197\n",
      "Epoch   4 Batch   94/2739 train_loss = 2.383\n",
      "Epoch   4 Batch  144/2739 train_loss = 2.800\n",
      "Epoch   4 Batch  194/2739 train_loss = 3.306\n",
      "Epoch   4 Batch  244/2739 train_loss = 2.970\n",
      "Epoch   4 Batch  294/2739 train_loss = 2.658\n",
      "Epoch   4 Batch  344/2739 train_loss = 2.198\n",
      "Epoch   4 Batch  394/2739 train_loss = 2.397\n",
      "Epoch   4 Batch  444/2739 train_loss = 2.127\n",
      "Epoch   4 Batch  494/2739 train_loss = 1.545\n",
      "Epoch   4 Batch  544/2739 train_loss = 2.584\n",
      "Epoch   4 Batch  594/2739 train_loss = 3.475\n",
      "Epoch   4 Batch  644/2739 train_loss = 1.412\n",
      "Epoch   4 Batch  694/2739 train_loss = 1.657\n",
      "Epoch   4 Batch  744/2739 train_loss = 2.282\n",
      "Epoch   4 Batch  794/2739 train_loss = 2.562\n",
      "Epoch   4 Batch  844/2739 train_loss = 3.247\n",
      "Epoch   4 Batch  894/2739 train_loss = 2.368\n",
      "Epoch   4 Batch  944/2739 train_loss = 2.052\n",
      "Epoch   4 Batch  994/2739 train_loss = 2.440\n",
      "Epoch   4 Batch 1044/2739 train_loss = 2.439\n",
      "Epoch   4 Batch 1094/2739 train_loss = 1.969\n",
      "Epoch   4 Batch 1144/2739 train_loss = 4.229\n",
      "Epoch   4 Batch 1194/2739 train_loss = 4.190\n",
      "Epoch   4 Batch 1244/2739 train_loss = 1.738\n",
      "Epoch   4 Batch 1294/2739 train_loss = 0.909\n",
      "Epoch   4 Batch 1344/2739 train_loss = 4.381\n",
      "Epoch   4 Batch 1394/2739 train_loss = 3.241\n",
      "Epoch   4 Batch 1444/2739 train_loss = 1.483\n",
      "Epoch   4 Batch 1494/2739 train_loss = 2.681\n",
      "Epoch   4 Batch 1544/2739 train_loss = 2.683\n",
      "Epoch   4 Batch 1594/2739 train_loss = 2.443\n",
      "Epoch   4 Batch 1644/2739 train_loss = 4.247\n",
      "Epoch   4 Batch 1694/2739 train_loss = 2.191\n",
      "Epoch   4 Batch 1744/2739 train_loss = 2.389\n",
      "Epoch   4 Batch 1794/2739 train_loss = 2.586\n",
      "Epoch   4 Batch 1844/2739 train_loss = 2.822\n",
      "Epoch   4 Batch 1894/2739 train_loss = 3.958\n",
      "Epoch   4 Batch 1944/2739 train_loss = 3.853\n",
      "Epoch   4 Batch 1994/2739 train_loss = 1.645\n",
      "Epoch   4 Batch 2044/2739 train_loss = 3.200\n",
      "Epoch   4 Batch 2094/2739 train_loss = 1.791\n",
      "Epoch   4 Batch 2144/2739 train_loss = 4.030\n",
      "Epoch   4 Batch 2194/2739 train_loss = 4.090\n",
      "Epoch   4 Batch 2244/2739 train_loss = 1.697\n",
      "Epoch   4 Batch 2294/2739 train_loss = 2.017\n",
      "Epoch   4 Batch 2344/2739 train_loss = 4.722\n",
      "Epoch   4 Batch 2394/2739 train_loss = 3.735\n",
      "Epoch   4 Batch 2444/2739 train_loss = 2.136\n",
      "Epoch   4 Batch 2494/2739 train_loss = 2.172\n",
      "Epoch   4 Batch 2544/2739 train_loss = 1.521\n",
      "Epoch   4 Batch 2594/2739 train_loss = 1.707\n",
      "Epoch   4 Batch 2644/2739 train_loss = 1.565\n",
      "Epoch   4 Batch 2694/2739 train_loss = 2.307\n",
      "Epoch   5 Batch    5/2739 train_loss = 1.880\n",
      "Epoch   5 Batch   55/2739 train_loss = 1.774\n",
      "Epoch   5 Batch  105/2739 train_loss = 3.087\n",
      "Epoch   5 Batch  155/2739 train_loss = 2.554\n",
      "Epoch   5 Batch  205/2739 train_loss = 2.829\n",
      "Epoch   5 Batch  255/2739 train_loss = 2.074\n",
      "Epoch   5 Batch  305/2739 train_loss = 1.429\n",
      "Epoch   5 Batch  355/2739 train_loss = 1.841\n",
      "Epoch   5 Batch  405/2739 train_loss = 2.143\n",
      "Epoch   5 Batch  455/2739 train_loss = 3.584\n",
      "Epoch   5 Batch  505/2739 train_loss = 3.061\n",
      "Epoch   5 Batch  555/2739 train_loss = 2.750\n",
      "Epoch   5 Batch  605/2739 train_loss = 2.244\n",
      "Epoch   5 Batch  655/2739 train_loss = 1.308\n",
      "Epoch   5 Batch  705/2739 train_loss = 2.674\n",
      "Epoch   5 Batch  755/2739 train_loss = 3.796\n",
      "Epoch   5 Batch  805/2739 train_loss = 2.079\n",
      "Epoch   5 Batch  855/2739 train_loss = 2.580\n",
      "Epoch   5 Batch  905/2739 train_loss = 2.900\n",
      "Epoch   5 Batch  955/2739 train_loss = 3.960\n",
      "Epoch   5 Batch 1005/2739 train_loss = 2.531\n",
      "Epoch   5 Batch 1055/2739 train_loss = 4.278\n",
      "Epoch   5 Batch 1105/2739 train_loss = 1.517\n",
      "Epoch   5 Batch 1155/2739 train_loss = 3.493\n",
      "Epoch   5 Batch 1205/2739 train_loss = 2.456\n",
      "Epoch   5 Batch 1255/2739 train_loss = 4.290\n",
      "Epoch   5 Batch 1305/2739 train_loss = 2.492\n",
      "Epoch   5 Batch 1355/2739 train_loss = 2.653\n",
      "Epoch   5 Batch 1405/2739 train_loss = 3.784\n",
      "Epoch   5 Batch 1455/2739 train_loss = 1.372\n",
      "Epoch   5 Batch 1505/2739 train_loss = 1.192\n",
      "Epoch   5 Batch 1555/2739 train_loss = 2.780\n",
      "Epoch   5 Batch 1605/2739 train_loss = 2.464\n",
      "Epoch   5 Batch 1655/2739 train_loss = 2.791\n",
      "Epoch   5 Batch 1705/2739 train_loss = 3.506\n",
      "Epoch   5 Batch 1755/2739 train_loss = 2.698\n",
      "Epoch   5 Batch 1805/2739 train_loss = 2.348\n",
      "Epoch   5 Batch 1855/2739 train_loss = 1.916\n",
      "Epoch   5 Batch 1905/2739 train_loss = 1.818\n",
      "Epoch   5 Batch 1955/2739 train_loss = 2.139\n",
      "Epoch   5 Batch 2005/2739 train_loss = 1.959\n",
      "Epoch   5 Batch 2055/2739 train_loss = 2.502\n",
      "Epoch   5 Batch 2105/2739 train_loss = 2.823\n",
      "Epoch   5 Batch 2155/2739 train_loss = 4.022\n",
      "Epoch   5 Batch 2205/2739 train_loss = 2.360\n",
      "Epoch   5 Batch 2255/2739 train_loss = 1.700\n",
      "Epoch   5 Batch 2305/2739 train_loss = 2.006\n",
      "Epoch   5 Batch 2355/2739 train_loss = 2.773\n",
      "Epoch   5 Batch 2405/2739 train_loss = 1.512\n",
      "Epoch   5 Batch 2455/2739 train_loss = 4.519\n",
      "Epoch   5 Batch 2505/2739 train_loss = 2.685\n",
      "Epoch   5 Batch 2555/2739 train_loss = 2.091\n",
      "Epoch   5 Batch 2605/2739 train_loss = 4.479\n",
      "Epoch   5 Batch 2655/2739 train_loss = 2.419\n",
      "Epoch   5 Batch 2705/2739 train_loss = 2.662\n",
      "Epoch   6 Batch   16/2739 train_loss = 1.491\n",
      "Epoch   6 Batch   66/2739 train_loss = 4.491\n",
      "Epoch   6 Batch  116/2739 train_loss = 2.710\n",
      "Epoch   6 Batch  166/2739 train_loss = 3.081\n",
      "Epoch   6 Batch  216/2739 train_loss = 1.843\n",
      "Epoch   6 Batch  266/2739 train_loss = 2.544\n",
      "Epoch   6 Batch  316/2739 train_loss = 2.981\n",
      "Epoch   6 Batch  366/2739 train_loss = 2.551\n",
      "Epoch   6 Batch  416/2739 train_loss = 2.898\n",
      "Epoch   6 Batch  466/2739 train_loss = 2.841\n",
      "Epoch   6 Batch  516/2739 train_loss = 2.582\n",
      "Epoch   6 Batch  566/2739 train_loss = 2.209\n",
      "Epoch   6 Batch  616/2739 train_loss = 4.358\n",
      "Epoch   6 Batch  666/2739 train_loss = 4.251\n",
      "Epoch   6 Batch  716/2739 train_loss = 2.378\n",
      "Epoch   6 Batch  766/2739 train_loss = 1.299\n",
      "Epoch   6 Batch  816/2739 train_loss = 3.215\n",
      "Epoch   6 Batch  866/2739 train_loss = 2.748\n",
      "Epoch   6 Batch  916/2739 train_loss = 3.483\n",
      "Epoch   6 Batch  966/2739 train_loss = 3.931\n",
      "Epoch   6 Batch 1016/2739 train_loss = 3.064\n",
      "Epoch   6 Batch 1066/2739 train_loss = 1.348\n",
      "Epoch   6 Batch 1116/2739 train_loss = 2.188\n",
      "Epoch   6 Batch 1166/2739 train_loss = 2.684\n",
      "Epoch   6 Batch 1216/2739 train_loss = 4.244\n",
      "Epoch   6 Batch 1266/2739 train_loss = 1.737\n",
      "Epoch   6 Batch 1316/2739 train_loss = 1.430\n",
      "Epoch   6 Batch 1366/2739 train_loss = 1.472\n",
      "Epoch   6 Batch 1416/2739 train_loss = 2.637\n",
      "Epoch   6 Batch 1466/2739 train_loss = 1.867\n",
      "Epoch   6 Batch 1516/2739 train_loss = 2.679\n",
      "Epoch   6 Batch 1566/2739 train_loss = 2.765\n",
      "Epoch   6 Batch 1616/2739 train_loss = 4.141\n",
      "Epoch   6 Batch 1666/2739 train_loss = 2.222\n",
      "Epoch   6 Batch 1716/2739 train_loss = 2.316\n",
      "Epoch   6 Batch 1766/2739 train_loss = 1.582\n",
      "Epoch   6 Batch 1816/2739 train_loss = 4.182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   6 Batch 1866/2739 train_loss = 1.514\n",
      "Epoch   6 Batch 1916/2739 train_loss = 2.843\n",
      "Epoch   6 Batch 1966/2739 train_loss = 4.832\n",
      "Epoch   6 Batch 2016/2739 train_loss = 1.351\n",
      "Epoch   6 Batch 2066/2739 train_loss = 4.648\n",
      "Epoch   6 Batch 2116/2739 train_loss = 2.490\n",
      "Epoch   6 Batch 2166/2739 train_loss = 4.024\n",
      "Epoch   6 Batch 2216/2739 train_loss = 3.403\n",
      "Epoch   6 Batch 2266/2739 train_loss = 4.682\n",
      "Epoch   6 Batch 2316/2739 train_loss = 4.802\n",
      "Epoch   6 Batch 2366/2739 train_loss = 2.485\n",
      "Epoch   6 Batch 2416/2739 train_loss = 4.510\n",
      "Epoch   6 Batch 2466/2739 train_loss = 1.106\n",
      "Epoch   6 Batch 2516/2739 train_loss = 1.988\n",
      "Epoch   6 Batch 2566/2739 train_loss = 4.448\n",
      "Epoch   6 Batch 2616/2739 train_loss = 3.295\n",
      "Epoch   6 Batch 2666/2739 train_loss = 3.968\n",
      "Epoch   6 Batch 2716/2739 train_loss = 2.140\n",
      "Epoch   7 Batch   27/2739 train_loss = 1.801\n",
      "Epoch   7 Batch   77/2739 train_loss = 2.027\n",
      "Epoch   7 Batch  127/2739 train_loss = 3.285\n",
      "Epoch   7 Batch  177/2739 train_loss = 2.456\n",
      "Epoch   7 Batch  227/2739 train_loss = 2.025\n",
      "Epoch   7 Batch  277/2739 train_loss = 2.561\n",
      "Epoch   7 Batch  327/2739 train_loss = 4.183\n",
      "Epoch   7 Batch  377/2739 train_loss = 2.381\n",
      "Epoch   7 Batch  427/2739 train_loss = 3.043\n",
      "Epoch   7 Batch  477/2739 train_loss = 2.988\n",
      "Epoch   7 Batch  527/2739 train_loss = 2.329\n",
      "Epoch   7 Batch  577/2739 train_loss = 4.054\n",
      "Epoch   7 Batch  627/2739 train_loss = 3.937\n",
      "Epoch   7 Batch  677/2739 train_loss = 2.323\n",
      "Epoch   7 Batch  727/2739 train_loss = 2.920\n",
      "Epoch   7 Batch  777/2739 train_loss = 3.726\n",
      "Epoch   7 Batch  827/2739 train_loss = 2.513\n",
      "Epoch   7 Batch  877/2739 train_loss = 1.677\n",
      "Epoch   7 Batch  927/2739 train_loss = 1.932\n",
      "Epoch   7 Batch  977/2739 train_loss = 2.769\n",
      "Epoch   7 Batch 1027/2739 train_loss = 1.754\n",
      "Epoch   7 Batch 1077/2739 train_loss = 2.575\n",
      "Epoch   7 Batch 1127/2739 train_loss = 1.424\n",
      "Epoch   7 Batch 1177/2739 train_loss = 2.085\n",
      "Epoch   7 Batch 1227/2739 train_loss = 2.873\n",
      "Epoch   7 Batch 1277/2739 train_loss = 2.535\n",
      "Epoch   7 Batch 1327/2739 train_loss = 2.288\n",
      "Epoch   7 Batch 1377/2739 train_loss = 2.233\n",
      "Epoch   7 Batch 1427/2739 train_loss = 3.021\n",
      "Epoch   7 Batch 1477/2739 train_loss = 2.236\n",
      "Epoch   7 Batch 1527/2739 train_loss = 1.569\n",
      "Epoch   7 Batch 1577/2739 train_loss = 2.501\n",
      "Epoch   7 Batch 1627/2739 train_loss = 3.773\n",
      "Epoch   7 Batch 1677/2739 train_loss = 3.745\n",
      "Epoch   7 Batch 1727/2739 train_loss = 3.144\n",
      "Epoch   7 Batch 1777/2739 train_loss = 4.128\n",
      "Epoch   7 Batch 1827/2739 train_loss = 2.459\n",
      "Epoch   7 Batch 1877/2739 train_loss = 2.754\n",
      "Epoch   7 Batch 1927/2739 train_loss = 1.299\n",
      "Epoch   7 Batch 1977/2739 train_loss = 1.707\n",
      "Epoch   7 Batch 2027/2739 train_loss = 4.140\n",
      "Epoch   7 Batch 2077/2739 train_loss = 3.905\n",
      "Epoch   7 Batch 2127/2739 train_loss = 2.091\n",
      "Epoch   7 Batch 2177/2739 train_loss = 2.520\n",
      "Epoch   7 Batch 2227/2739 train_loss = 1.535\n",
      "Epoch   7 Batch 2277/2739 train_loss = 3.729\n",
      "Epoch   7 Batch 2327/2739 train_loss = 2.169\n",
      "Epoch   7 Batch 2377/2739 train_loss = 1.592\n",
      "Epoch   7 Batch 2427/2739 train_loss = 2.391\n",
      "Epoch   7 Batch 2477/2739 train_loss = 2.069\n",
      "Epoch   7 Batch 2527/2739 train_loss = 2.653\n",
      "Epoch   7 Batch 2577/2739 train_loss = 2.518\n",
      "Epoch   7 Batch 2627/2739 train_loss = 2.620\n",
      "Epoch   7 Batch 2677/2739 train_loss = 0.927\n",
      "Epoch   7 Batch 2727/2739 train_loss = 2.073\n",
      "Epoch   8 Batch   38/2739 train_loss = 1.845\n",
      "Epoch   8 Batch   88/2739 train_loss = 2.454\n",
      "Epoch   8 Batch  138/2739 train_loss = 1.641\n",
      "Epoch   8 Batch  188/2739 train_loss = 2.792\n",
      "Epoch   8 Batch  238/2739 train_loss = 1.896\n",
      "Epoch   8 Batch  288/2739 train_loss = 2.224\n",
      "Epoch   8 Batch  338/2739 train_loss = 2.059\n",
      "Epoch   8 Batch  388/2739 train_loss = 1.906\n",
      "Epoch   8 Batch  438/2739 train_loss = 3.233\n",
      "Epoch   8 Batch  488/2739 train_loss = 3.709\n",
      "Epoch   8 Batch  538/2739 train_loss = 2.977\n",
      "Epoch   8 Batch  588/2739 train_loss = 4.202\n",
      "Epoch   8 Batch  638/2739 train_loss = 2.811\n",
      "Epoch   8 Batch  688/2739 train_loss = 2.135\n",
      "Epoch   8 Batch  738/2739 train_loss = 1.905\n",
      "Epoch   8 Batch  788/2739 train_loss = 2.993\n",
      "Epoch   8 Batch  838/2739 train_loss = 2.740\n",
      "Epoch   8 Batch  888/2739 train_loss = 2.309\n",
      "Epoch   8 Batch  938/2739 train_loss = 2.781\n",
      "Epoch   8 Batch  988/2739 train_loss = 3.928\n",
      "Epoch   8 Batch 1038/2739 train_loss = 3.095\n",
      "Epoch   8 Batch 1088/2739 train_loss = 2.272\n",
      "Epoch   8 Batch 1138/2739 train_loss = 2.769\n",
      "Epoch   8 Batch 1188/2739 train_loss = 1.689\n",
      "Epoch   8 Batch 1238/2739 train_loss = 1.546\n",
      "Epoch   8 Batch 1288/2739 train_loss = 3.477\n",
      "Epoch   8 Batch 1338/2739 train_loss = 4.496\n",
      "Epoch   8 Batch 1388/2739 train_loss = 2.799\n",
      "Epoch   8 Batch 1438/2739 train_loss = 3.023\n",
      "Epoch   8 Batch 1488/2739 train_loss = 4.330\n",
      "Epoch   8 Batch 1538/2739 train_loss = 2.134\n",
      "Epoch   8 Batch 1588/2739 train_loss = 2.465\n",
      "Epoch   8 Batch 1638/2739 train_loss = 2.325\n",
      "Epoch   8 Batch 1688/2739 train_loss = 3.577\n",
      "Epoch   8 Batch 1738/2739 train_loss = 1.944\n",
      "Epoch   8 Batch 1788/2739 train_loss = 2.724\n",
      "Epoch   8 Batch 1838/2739 train_loss = 2.963\n",
      "Epoch   8 Batch 1888/2739 train_loss = 4.091\n",
      "Epoch   8 Batch 1938/2739 train_loss = 2.481\n",
      "Epoch   8 Batch 1988/2739 train_loss = 1.766\n",
      "Epoch   8 Batch 2038/2739 train_loss = 3.699\n",
      "Epoch   8 Batch 2088/2739 train_loss = 4.236\n",
      "Epoch   8 Batch 2138/2739 train_loss = 3.869\n",
      "Epoch   8 Batch 2188/2739 train_loss = 4.262\n",
      "Epoch   8 Batch 2238/2739 train_loss = 4.348\n",
      "Epoch   8 Batch 2288/2739 train_loss = 4.616\n",
      "Epoch   8 Batch 2338/2739 train_loss = 1.302\n",
      "Epoch   8 Batch 2388/2739 train_loss = 1.593\n",
      "Epoch   8 Batch 2438/2739 train_loss = 4.760\n",
      "Epoch   8 Batch 2488/2739 train_loss = 2.214\n",
      "Epoch   8 Batch 2538/2739 train_loss = 2.558\n",
      "Epoch   8 Batch 2588/2739 train_loss = 4.052\n",
      "Epoch   8 Batch 2638/2739 train_loss = 2.148\n",
      "Epoch   8 Batch 2688/2739 train_loss = 2.149\n",
      "Epoch   8 Batch 2738/2739 train_loss = 4.362\n",
      "Epoch   9 Batch   49/2739 train_loss = 1.801\n",
      "Epoch   9 Batch   99/2739 train_loss = 2.480\n",
      "Epoch   9 Batch  149/2739 train_loss = 3.363\n",
      "Epoch   9 Batch  199/2739 train_loss = 1.278\n",
      "Epoch   9 Batch  249/2739 train_loss = 1.755\n",
      "Epoch   9 Batch  299/2739 train_loss = 2.388\n",
      "Epoch   9 Batch  349/2739 train_loss = 3.621\n",
      "Epoch   9 Batch  399/2739 train_loss = 2.233\n",
      "Epoch   9 Batch  449/2739 train_loss = 3.006\n",
      "Epoch   9 Batch  499/2739 train_loss = 3.827\n",
      "Epoch   9 Batch  549/2739 train_loss = 1.185\n",
      "Epoch   9 Batch  599/2739 train_loss = 1.691\n",
      "Epoch   9 Batch  649/2739 train_loss = 2.131\n",
      "Epoch   9 Batch  699/2739 train_loss = 1.752\n",
      "Epoch   9 Batch  749/2739 train_loss = 2.336\n",
      "Epoch   9 Batch  799/2739 train_loss = 2.392\n",
      "Epoch   9 Batch  849/2739 train_loss = 1.947\n",
      "Epoch   9 Batch  899/2739 train_loss = 2.430\n",
      "Epoch   9 Batch  949/2739 train_loss = 2.744\n",
      "Epoch   9 Batch  999/2739 train_loss = 1.868\n",
      "Epoch   9 Batch 1049/2739 train_loss = 2.784\n",
      "Epoch   9 Batch 1099/2739 train_loss = 3.000\n",
      "Epoch   9 Batch 1149/2739 train_loss = 3.877\n",
      "Epoch   9 Batch 1199/2739 train_loss = 2.030\n",
      "Epoch   9 Batch 1249/2739 train_loss = 2.034\n",
      "Epoch   9 Batch 1299/2739 train_loss = 1.711\n",
      "Epoch   9 Batch 1349/2739 train_loss = 2.844\n",
      "Epoch   9 Batch 1399/2739 train_loss = 1.632\n",
      "Epoch   9 Batch 1449/2739 train_loss = 2.473\n",
      "Epoch   9 Batch 1499/2739 train_loss = 1.641\n",
      "Epoch   9 Batch 1549/2739 train_loss = 4.574\n",
      "Epoch   9 Batch 1599/2739 train_loss = 2.966\n",
      "Epoch   9 Batch 1649/2739 train_loss = 3.399\n",
      "Epoch   9 Batch 1699/2739 train_loss = 2.536\n",
      "Epoch   9 Batch 1749/2739 train_loss = 3.461\n",
      "Epoch   9 Batch 1799/2739 train_loss = 2.108\n",
      "Epoch   9 Batch 1849/2739 train_loss = 2.232\n",
      "Epoch   9 Batch 1899/2739 train_loss = 2.943\n",
      "Epoch   9 Batch 1949/2739 train_loss = 2.248\n",
      "Epoch   9 Batch 1999/2739 train_loss = 2.912\n",
      "Epoch   9 Batch 2049/2739 train_loss = 2.469\n",
      "Epoch   9 Batch 2099/2739 train_loss = 2.878\n",
      "Epoch   9 Batch 2149/2739 train_loss = 2.914\n",
      "Epoch   9 Batch 2199/2739 train_loss = 4.605\n",
      "Epoch   9 Batch 2249/2739 train_loss = 1.585\n",
      "Epoch   9 Batch 2299/2739 train_loss = 3.249\n",
      "Epoch   9 Batch 2349/2739 train_loss = 1.470\n",
      "Epoch   9 Batch 2399/2739 train_loss = 4.109\n",
      "Epoch   9 Batch 2449/2739 train_loss = 1.671\n",
      "Epoch   9 Batch 2499/2739 train_loss = 3.082\n",
      "Epoch   9 Batch 2549/2739 train_loss = 2.458\n",
      "Epoch   9 Batch 2599/2739 train_loss = 3.619\n",
      "Epoch   9 Batch 2649/2739 train_loss = 1.188\n",
      "Epoch   9 Batch 2699/2739 train_loss = 1.478\n",
      "Epoch  10 Batch   10/2739 train_loss = 2.298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10 Batch   60/2739 train_loss = 2.455\n",
      "Epoch  10 Batch  110/2739 train_loss = 2.255\n",
      "Epoch  10 Batch  160/2739 train_loss = 2.224\n",
      "Epoch  10 Batch  210/2739 train_loss = 1.773\n",
      "Epoch  10 Batch  260/2739 train_loss = 2.408\n",
      "Epoch  10 Batch  310/2739 train_loss = 2.869\n",
      "Epoch  10 Batch  360/2739 train_loss = 2.655\n",
      "Epoch  10 Batch  410/2739 train_loss = 2.208\n",
      "Epoch  10 Batch  460/2739 train_loss = 1.663\n",
      "Epoch  10 Batch  510/2739 train_loss = 3.234\n",
      "Epoch  10 Batch  560/2739 train_loss = 2.512\n",
      "Epoch  10 Batch  610/2739 train_loss = 1.572\n",
      "Epoch  10 Batch  660/2739 train_loss = 2.423\n",
      "Epoch  10 Batch  710/2739 train_loss = 2.032\n",
      "Epoch  10 Batch  760/2739 train_loss = 3.674\n",
      "Epoch  10 Batch  810/2739 train_loss = 2.724\n",
      "Epoch  10 Batch  860/2739 train_loss = 3.819\n",
      "Epoch  10 Batch  910/2739 train_loss = 2.176\n",
      "Epoch  10 Batch  960/2739 train_loss = 4.506\n",
      "Epoch  10 Batch 1010/2739 train_loss = 1.402\n",
      "Epoch  10 Batch 1060/2739 train_loss = 3.246\n",
      "Epoch  10 Batch 1110/2739 train_loss = 1.680\n",
      "Epoch  10 Batch 1160/2739 train_loss = 1.395\n",
      "Epoch  10 Batch 1210/2739 train_loss = 2.138\n",
      "Epoch  10 Batch 1260/2739 train_loss = 2.038\n",
      "Epoch  10 Batch 1310/2739 train_loss = 2.835\n",
      "Epoch  10 Batch 1360/2739 train_loss = 1.851\n",
      "Epoch  10 Batch 1410/2739 train_loss = 4.275\n",
      "Epoch  10 Batch 1460/2739 train_loss = 2.906\n",
      "Epoch  10 Batch 1510/2739 train_loss = 1.332\n",
      "Epoch  10 Batch 1560/2739 train_loss = 2.544\n",
      "Epoch  10 Batch 1610/2739 train_loss = 3.800\n",
      "Epoch  10 Batch 1660/2739 train_loss = 2.458\n",
      "Epoch  10 Batch 1710/2739 train_loss = 2.253\n",
      "Epoch  10 Batch 1760/2739 train_loss = 2.404\n",
      "Epoch  10 Batch 1810/2739 train_loss = 1.718\n",
      "Epoch  10 Batch 1860/2739 train_loss = 1.941\n",
      "Epoch  10 Batch 1910/2739 train_loss = 2.080\n",
      "Epoch  10 Batch 1960/2739 train_loss = 1.949\n",
      "Epoch  10 Batch 2010/2739 train_loss = 2.118\n",
      "Epoch  10 Batch 2060/2739 train_loss = 4.487\n",
      "Epoch  10 Batch 2110/2739 train_loss = 2.398\n",
      "Epoch  10 Batch 2160/2739 train_loss = 2.006\n",
      "Epoch  10 Batch 2210/2739 train_loss = 2.996\n",
      "Epoch  10 Batch 2260/2739 train_loss = 4.658\n",
      "Epoch  10 Batch 2310/2739 train_loss = 3.266\n",
      "Epoch  10 Batch 2360/2739 train_loss = 2.674\n",
      "Epoch  10 Batch 2410/2739 train_loss = 2.743\n",
      "Epoch  10 Batch 2460/2739 train_loss = 2.068\n",
      "Epoch  10 Batch 2510/2739 train_loss = 1.909\n",
      "Epoch  10 Batch 2560/2739 train_loss = 1.767\n",
      "Epoch  10 Batch 2610/2739 train_loss = 4.490\n",
      "Epoch  10 Batch 2660/2739 train_loss = 2.168\n",
      "Epoch  10 Batch 2710/2739 train_loss = 2.602\n",
      "Epoch  11 Batch   21/2739 train_loss = 1.597\n",
      "Epoch  11 Batch   71/2739 train_loss = 2.464\n",
      "Epoch  11 Batch  121/2739 train_loss = 2.205\n",
      "Epoch  11 Batch  171/2739 train_loss = 2.837\n",
      "Epoch  11 Batch  221/2739 train_loss = 3.140\n",
      "Epoch  11 Batch  271/2739 train_loss = 1.979\n",
      "Epoch  11 Batch  321/2739 train_loss = 2.668\n",
      "Epoch  11 Batch  371/2739 train_loss = 2.217\n",
      "Epoch  11 Batch  421/2739 train_loss = 4.390\n",
      "Epoch  11 Batch  471/2739 train_loss = 2.617\n",
      "Epoch  11 Batch  521/2739 train_loss = 2.380\n",
      "Epoch  11 Batch  571/2739 train_loss = 1.897\n",
      "Epoch  11 Batch  621/2739 train_loss = 2.049\n",
      "Epoch  11 Batch  671/2739 train_loss = 2.454\n",
      "Epoch  11 Batch  721/2739 train_loss = 4.012\n",
      "Epoch  11 Batch  771/2739 train_loss = 2.110\n",
      "Epoch  11 Batch  821/2739 train_loss = 2.445\n",
      "Epoch  11 Batch  871/2739 train_loss = 2.255\n",
      "Epoch  11 Batch  921/2739 train_loss = 1.353\n",
      "Epoch  11 Batch  971/2739 train_loss = 2.141\n",
      "Epoch  11 Batch 1021/2739 train_loss = 1.032\n",
      "Epoch  11 Batch 1071/2739 train_loss = 2.963\n",
      "Epoch  11 Batch 1121/2739 train_loss = 3.664\n",
      "Epoch  11 Batch 1171/2739 train_loss = 2.993\n",
      "Epoch  11 Batch 1221/2739 train_loss = 3.412\n",
      "Epoch  11 Batch 1271/2739 train_loss = 2.846\n",
      "Epoch  11 Batch 1321/2739 train_loss = 2.401\n",
      "Epoch  11 Batch 1371/2739 train_loss = 2.482\n",
      "Epoch  11 Batch 1421/2739 train_loss = 1.533\n",
      "Epoch  11 Batch 1471/2739 train_loss = 1.463\n",
      "Epoch  11 Batch 1521/2739 train_loss = 1.647\n",
      "Epoch  11 Batch 1571/2739 train_loss = 3.946\n",
      "Epoch  11 Batch 1621/2739 train_loss = 1.891\n",
      "Epoch  11 Batch 1671/2739 train_loss = 2.516\n",
      "Epoch  11 Batch 1721/2739 train_loss = 2.238\n",
      "Epoch  11 Batch 1771/2739 train_loss = 2.677\n",
      "Epoch  11 Batch 1821/2739 train_loss = 2.359\n",
      "Epoch  11 Batch 1871/2739 train_loss = 4.038\n",
      "Epoch  11 Batch 1921/2739 train_loss = 1.703\n",
      "Epoch  11 Batch 1971/2739 train_loss = 4.226\n",
      "Epoch  11 Batch 2021/2739 train_loss = 2.705\n",
      "Epoch  11 Batch 2071/2739 train_loss = 3.597\n",
      "Epoch  11 Batch 2121/2739 train_loss = 1.787\n",
      "Epoch  11 Batch 2171/2739 train_loss = 2.580\n",
      "Epoch  11 Batch 2221/2739 train_loss = 2.951\n",
      "Epoch  11 Batch 2271/2739 train_loss = 2.500\n",
      "Epoch  11 Batch 2321/2739 train_loss = 4.653\n",
      "Epoch  11 Batch 2371/2739 train_loss = 2.956\n",
      "Epoch  11 Batch 2421/2739 train_loss = 3.874\n",
      "Epoch  11 Batch 2471/2739 train_loss = 1.982\n",
      "Epoch  11 Batch 2521/2739 train_loss = 2.586\n",
      "Epoch  11 Batch 2571/2739 train_loss = 2.257\n",
      "Epoch  11 Batch 2621/2739 train_loss = 2.480\n",
      "Epoch  11 Batch 2671/2739 train_loss = 2.349\n",
      "Epoch  11 Batch 2721/2739 train_loss = 2.597\n",
      "Epoch  12 Batch   32/2739 train_loss = 4.333\n",
      "Epoch  12 Batch   82/2739 train_loss = 1.830\n",
      "Epoch  12 Batch  132/2739 train_loss = 2.517\n",
      "Epoch  12 Batch  182/2739 train_loss = 2.206\n",
      "Epoch  12 Batch  232/2739 train_loss = 3.347\n",
      "Epoch  12 Batch  282/2739 train_loss = 2.721\n",
      "Epoch  12 Batch  332/2739 train_loss = 4.075\n",
      "Epoch  12 Batch  382/2739 train_loss = 1.887\n",
      "Epoch  12 Batch  432/2739 train_loss = 1.368\n",
      "Epoch  12 Batch  482/2739 train_loss = 2.324\n",
      "Epoch  12 Batch  532/2739 train_loss = 2.209\n",
      "Epoch  12 Batch  582/2739 train_loss = 2.176\n",
      "Epoch  12 Batch  632/2739 train_loss = 1.384\n",
      "Epoch  12 Batch  682/2739 train_loss = 1.416\n",
      "Epoch  12 Batch  732/2739 train_loss = 2.007\n",
      "Epoch  12 Batch  782/2739 train_loss = 1.730\n",
      "Epoch  12 Batch  832/2739 train_loss = 2.838\n",
      "Epoch  12 Batch  882/2739 train_loss = 2.752\n",
      "Epoch  12 Batch  932/2739 train_loss = 2.230\n",
      "Epoch  12 Batch  982/2739 train_loss = 3.368\n",
      "Epoch  12 Batch 1032/2739 train_loss = 1.948\n",
      "Epoch  12 Batch 1082/2739 train_loss = 3.779\n",
      "Epoch  12 Batch 1132/2739 train_loss = 2.979\n",
      "Epoch  12 Batch 1182/2739 train_loss = 1.398\n",
      "Epoch  12 Batch 1232/2739 train_loss = 3.553\n",
      "Epoch  12 Batch 1282/2739 train_loss = 2.439\n",
      "Epoch  12 Batch 1332/2739 train_loss = 2.236\n",
      "Epoch  12 Batch 1382/2739 train_loss = 1.813\n",
      "Epoch  12 Batch 1432/2739 train_loss = 3.433\n",
      "Epoch  12 Batch 1482/2739 train_loss = 2.332\n",
      "Epoch  12 Batch 1532/2739 train_loss = 2.071\n",
      "Epoch  12 Batch 1582/2739 train_loss = 1.878\n",
      "Epoch  12 Batch 1632/2739 train_loss = 2.590\n",
      "Epoch  12 Batch 1682/2739 train_loss = 2.363\n",
      "Epoch  12 Batch 1732/2739 train_loss = 4.186\n",
      "Epoch  12 Batch 1782/2739 train_loss = 1.985\n",
      "Epoch  12 Batch 1832/2739 train_loss = 1.633\n",
      "Epoch  12 Batch 1882/2739 train_loss = 4.076\n",
      "Epoch  12 Batch 1932/2739 train_loss = 4.598\n",
      "Epoch  12 Batch 1982/2739 train_loss = 1.668\n",
      "Epoch  12 Batch 2032/2739 train_loss = 3.858\n",
      "Epoch  12 Batch 2082/2739 train_loss = 1.696\n",
      "Epoch  12 Batch 2132/2739 train_loss = 2.522\n",
      "Epoch  12 Batch 2182/2739 train_loss = 2.750\n",
      "Epoch  12 Batch 2232/2739 train_loss = 2.003\n",
      "Epoch  12 Batch 2282/2739 train_loss = 4.293\n",
      "Epoch  12 Batch 2332/2739 train_loss = 1.533\n",
      "Epoch  12 Batch 2382/2739 train_loss = 1.532\n",
      "Epoch  12 Batch 2432/2739 train_loss = 4.146\n",
      "Epoch  12 Batch 2482/2739 train_loss = 2.044\n",
      "Epoch  12 Batch 2532/2739 train_loss = 2.672\n",
      "Epoch  12 Batch 2582/2739 train_loss = 2.582\n",
      "Epoch  12 Batch 2632/2739 train_loss = 1.503\n",
      "Epoch  12 Batch 2682/2739 train_loss = 2.350\n",
      "Epoch  12 Batch 2732/2739 train_loss = 1.575\n",
      "Epoch  13 Batch   43/2739 train_loss = 1.572\n",
      "Epoch  13 Batch   93/2739 train_loss = 1.518\n",
      "Epoch  13 Batch  143/2739 train_loss = 2.657\n",
      "Epoch  13 Batch  193/2739 train_loss = 2.198\n",
      "Epoch  13 Batch  243/2739 train_loss = 2.800\n",
      "Epoch  13 Batch  293/2739 train_loss = 2.859\n",
      "Epoch  13 Batch  343/2739 train_loss = 2.049\n",
      "Epoch  13 Batch  393/2739 train_loss = 1.691\n",
      "Epoch  13 Batch  443/2739 train_loss = 3.094\n",
      "Epoch  13 Batch  493/2739 train_loss = 2.666\n",
      "Epoch  13 Batch  543/2739 train_loss = 2.081\n",
      "Epoch  13 Batch  593/2739 train_loss = 2.024\n",
      "Epoch  13 Batch  643/2739 train_loss = 2.469\n",
      "Epoch  13 Batch  693/2739 train_loss = 2.054\n",
      "Epoch  13 Batch  743/2739 train_loss = 2.814\n",
      "Epoch  13 Batch  793/2739 train_loss = 3.966\n",
      "Epoch  13 Batch  843/2739 train_loss = 2.761\n",
      "Epoch  13 Batch  893/2739 train_loss = 1.793\n",
      "Epoch  13 Batch  943/2739 train_loss = 1.410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  13 Batch  993/2739 train_loss = 3.197\n",
      "Epoch  13 Batch 1043/2739 train_loss = 2.686\n",
      "Epoch  13 Batch 1093/2739 train_loss = 1.621\n",
      "Epoch  13 Batch 1143/2739 train_loss = 1.941\n",
      "Epoch  13 Batch 1193/2739 train_loss = 1.912\n",
      "Epoch  13 Batch 1243/2739 train_loss = 3.122\n",
      "Epoch  13 Batch 1293/2739 train_loss = 1.457\n",
      "Epoch  13 Batch 1343/2739 train_loss = 2.176\n",
      "Epoch  13 Batch 1393/2739 train_loss = 2.278\n",
      "Epoch  13 Batch 1443/2739 train_loss = 3.480\n",
      "Epoch  13 Batch 1493/2739 train_loss = 2.607\n",
      "Epoch  13 Batch 1543/2739 train_loss = 1.807\n",
      "Epoch  13 Batch 1593/2739 train_loss = 2.589\n",
      "Epoch  13 Batch 1643/2739 train_loss = 3.244\n",
      "Epoch  13 Batch 1693/2739 train_loss = 3.800\n",
      "Epoch  13 Batch 1743/2739 train_loss = 4.209\n",
      "Epoch  13 Batch 1793/2739 train_loss = 2.866\n",
      "Epoch  13 Batch 1843/2739 train_loss = 2.286\n",
      "Epoch  13 Batch 1893/2739 train_loss = 3.564\n",
      "Epoch  13 Batch 1943/2739 train_loss = 1.786\n",
      "Epoch  13 Batch 1993/2739 train_loss = 1.738\n",
      "Epoch  13 Batch 2043/2739 train_loss = 2.126\n",
      "Epoch  13 Batch 2093/2739 train_loss = 2.040\n",
      "Epoch  13 Batch 2143/2739 train_loss = 2.035\n",
      "Epoch  13 Batch 2193/2739 train_loss = 2.416\n",
      "Epoch  13 Batch 2243/2739 train_loss = 4.255\n",
      "Epoch  13 Batch 2293/2739 train_loss = 4.431\n",
      "Epoch  13 Batch 2343/2739 train_loss = 2.732\n",
      "Epoch  13 Batch 2393/2739 train_loss = 3.482\n",
      "Epoch  13 Batch 2443/2739 train_loss = 2.141\n",
      "Epoch  13 Batch 2493/2739 train_loss = 1.694\n",
      "Epoch  13 Batch 2543/2739 train_loss = 1.292\n",
      "Epoch  13 Batch 2593/2739 train_loss = 4.195\n",
      "Epoch  13 Batch 2643/2739 train_loss = 1.173\n",
      "Epoch  13 Batch 2693/2739 train_loss = 2.979\n",
      "Epoch  14 Batch    4/2739 train_loss = 3.448\n",
      "Epoch  14 Batch   54/2739 train_loss = 2.603\n",
      "Epoch  14 Batch  104/2739 train_loss = 1.919\n",
      "Epoch  14 Batch  154/2739 train_loss = 2.225\n",
      "Epoch  14 Batch  204/2739 train_loss = 1.686\n",
      "Epoch  14 Batch  254/2739 train_loss = 2.199\n",
      "Epoch  14 Batch  304/2739 train_loss = 1.525\n",
      "Epoch  14 Batch  354/2739 train_loss = 3.697\n",
      "Epoch  14 Batch  404/2739 train_loss = 2.231\n",
      "Epoch  14 Batch  454/2739 train_loss = 3.816\n",
      "Epoch  14 Batch  504/2739 train_loss = 1.783\n",
      "Epoch  14 Batch  554/2739 train_loss = 2.777\n",
      "Epoch  14 Batch  604/2739 train_loss = 1.256\n",
      "Epoch  14 Batch  654/2739 train_loss = 1.526\n",
      "Epoch  14 Batch  704/2739 train_loss = 2.096\n",
      "Epoch  14 Batch  754/2739 train_loss = 1.845\n",
      "Epoch  14 Batch  804/2739 train_loss = 2.610\n",
      "Epoch  14 Batch  854/2739 train_loss = 2.352\n",
      "Epoch  14 Batch  904/2739 train_loss = 2.922\n",
      "Epoch  14 Batch  954/2739 train_loss = 1.943\n",
      "Epoch  14 Batch 1004/2739 train_loss = 1.353\n",
      "Epoch  14 Batch 1054/2739 train_loss = 3.864\n",
      "Epoch  14 Batch 1104/2739 train_loss = 2.360\n",
      "Epoch  14 Batch 1154/2739 train_loss = 2.450\n",
      "Epoch  14 Batch 1204/2739 train_loss = 1.611\n",
      "Epoch  14 Batch 1254/2739 train_loss = 3.510\n",
      "Epoch  14 Batch 1304/2739 train_loss = 1.779\n",
      "Epoch  14 Batch 1354/2739 train_loss = 2.451\n",
      "Epoch  14 Batch 1404/2739 train_loss = 2.254\n",
      "Epoch  14 Batch 1454/2739 train_loss = 1.667\n",
      "Epoch  14 Batch 1504/2739 train_loss = 1.907\n",
      "Epoch  14 Batch 1554/2739 train_loss = 1.699\n",
      "Epoch  14 Batch 1604/2739 train_loss = 2.742\n",
      "Epoch  14 Batch 1654/2739 train_loss = 1.889\n",
      "Epoch  14 Batch 1704/2739 train_loss = 1.644\n",
      "Epoch  14 Batch 1754/2739 train_loss = 2.021\n",
      "Epoch  14 Batch 1804/2739 train_loss = 1.475\n",
      "Epoch  14 Batch 1854/2739 train_loss = 2.007\n",
      "Epoch  14 Batch 1904/2739 train_loss = 3.016\n",
      "Epoch  14 Batch 1954/2739 train_loss = 3.289\n",
      "Epoch  14 Batch 2004/2739 train_loss = 1.706\n",
      "Epoch  14 Batch 2054/2739 train_loss = 2.436\n",
      "Epoch  14 Batch 2104/2739 train_loss = 4.086\n",
      "Epoch  14 Batch 2154/2739 train_loss = 3.453\n",
      "Epoch  14 Batch 2204/2739 train_loss = 3.578\n",
      "Epoch  14 Batch 2254/2739 train_loss = 2.473\n",
      "Epoch  14 Batch 2304/2739 train_loss = 2.762\n",
      "Epoch  14 Batch 2354/2739 train_loss = 1.855\n",
      "Epoch  14 Batch 2404/2739 train_loss = 3.310\n",
      "Epoch  14 Batch 2454/2739 train_loss = 2.854\n",
      "Epoch  14 Batch 2504/2739 train_loss = 3.161\n",
      "Epoch  14 Batch 2554/2739 train_loss = 2.335\n",
      "Epoch  14 Batch 2604/2739 train_loss = 4.383\n",
      "Epoch  14 Batch 2654/2739 train_loss = 1.687\n",
      "Epoch  14 Batch 2704/2739 train_loss = 2.883\n",
      "Epoch  15 Batch   15/2739 train_loss = 2.495\n",
      "Epoch  15 Batch   65/2739 train_loss = 2.673\n",
      "Epoch  15 Batch  115/2739 train_loss = 1.586\n",
      "Epoch  15 Batch  165/2739 train_loss = 1.713\n",
      "Epoch  15 Batch  215/2739 train_loss = 2.743\n",
      "Epoch  15 Batch  265/2739 train_loss = 3.218\n",
      "Epoch  15 Batch  315/2739 train_loss = 3.024\n",
      "Epoch  15 Batch  365/2739 train_loss = 1.765\n",
      "Epoch  15 Batch  415/2739 train_loss = 4.283\n",
      "Epoch  15 Batch  465/2739 train_loss = 3.699\n",
      "Epoch  15 Batch  515/2739 train_loss = 1.229\n",
      "Epoch  15 Batch  565/2739 train_loss = 1.930\n",
      "Epoch  15 Batch  615/2739 train_loss = 3.931\n",
      "Epoch  15 Batch  665/2739 train_loss = 2.645\n",
      "Epoch  15 Batch  715/2739 train_loss = 1.998\n",
      "Epoch  15 Batch  765/2739 train_loss = 1.854\n",
      "Epoch  15 Batch  815/2739 train_loss = 1.787\n",
      "Epoch  15 Batch  865/2739 train_loss = 2.629\n",
      "Epoch  15 Batch  915/2739 train_loss = 2.303\n",
      "Epoch  15 Batch  965/2739 train_loss = 2.719\n",
      "Epoch  15 Batch 1015/2739 train_loss = 1.943\n",
      "Epoch  15 Batch 1065/2739 train_loss = 3.348\n",
      "Epoch  15 Batch 1115/2739 train_loss = 2.972\n",
      "Epoch  15 Batch 1165/2739 train_loss = 1.971\n",
      "Epoch  15 Batch 1215/2739 train_loss = 3.724\n",
      "Epoch  15 Batch 1265/2739 train_loss = 1.239\n",
      "Epoch  15 Batch 1315/2739 train_loss = 2.299\n",
      "Epoch  15 Batch 1365/2739 train_loss = 1.959\n",
      "Epoch  15 Batch 1415/2739 train_loss = 2.664\n",
      "Epoch  15 Batch 1465/2739 train_loss = 2.303\n",
      "Epoch  15 Batch 1515/2739 train_loss = 1.379\n",
      "Epoch  15 Batch 1565/2739 train_loss = 3.906\n",
      "Epoch  15 Batch 1615/2739 train_loss = 2.991\n",
      "Epoch  15 Batch 1665/2739 train_loss = 2.267\n",
      "Epoch  15 Batch 1715/2739 train_loss = 1.826\n",
      "Epoch  15 Batch 1765/2739 train_loss = 1.391\n",
      "Epoch  15 Batch 1815/2739 train_loss = 2.408\n",
      "Epoch  15 Batch 1865/2739 train_loss = 2.956\n",
      "Epoch  15 Batch 1915/2739 train_loss = 4.041\n",
      "Epoch  15 Batch 1965/2739 train_loss = 4.638\n",
      "Epoch  15 Batch 2015/2739 train_loss = 3.892\n",
      "Epoch  15 Batch 2065/2739 train_loss = 4.243\n",
      "Epoch  15 Batch 2115/2739 train_loss = 2.497\n",
      "Epoch  15 Batch 2165/2739 train_loss = 2.715\n",
      "Epoch  15 Batch 2215/2739 train_loss = 3.427\n",
      "Epoch  15 Batch 2265/2739 train_loss = 1.913\n",
      "Epoch  15 Batch 2315/2739 train_loss = 2.574\n",
      "Epoch  15 Batch 2365/2739 train_loss = 4.556\n",
      "Epoch  15 Batch 2415/2739 train_loss = 3.674\n",
      "Epoch  15 Batch 2465/2739 train_loss = 2.533\n",
      "Epoch  15 Batch 2515/2739 train_loss = 3.073\n",
      "Epoch  15 Batch 2565/2739 train_loss = 2.021\n",
      "Epoch  15 Batch 2615/2739 train_loss = 2.199\n",
      "Epoch  15 Batch 2665/2739 train_loss = 2.499\n",
      "Epoch  15 Batch 2715/2739 train_loss = 2.148\n",
      "Epoch  16 Batch   26/2739 train_loss = 1.948\n",
      "Epoch  16 Batch   76/2739 train_loss = 3.137\n",
      "Epoch  16 Batch  126/2739 train_loss = 2.581\n",
      "Epoch  16 Batch  176/2739 train_loss = 1.371\n",
      "Epoch  16 Batch  226/2739 train_loss = 2.867\n",
      "Epoch  16 Batch  276/2739 train_loss = 2.961\n",
      "Epoch  16 Batch  326/2739 train_loss = 3.859\n",
      "Epoch  16 Batch  376/2739 train_loss = 2.028\n",
      "Epoch  16 Batch  426/2739 train_loss = 2.212\n",
      "Epoch  16 Batch  476/2739 train_loss = 2.617\n",
      "Epoch  16 Batch  526/2739 train_loss = 2.552\n",
      "Epoch  16 Batch  576/2739 train_loss = 2.279\n",
      "Epoch  16 Batch  626/2739 train_loss = 2.182\n",
      "Epoch  16 Batch  676/2739 train_loss = 1.615\n",
      "Epoch  16 Batch  726/2739 train_loss = 2.806\n",
      "Epoch  16 Batch  776/2739 train_loss = 2.013\n",
      "Epoch  16 Batch  826/2739 train_loss = 1.683\n",
      "Epoch  16 Batch  876/2739 train_loss = 1.469\n",
      "Epoch  16 Batch  926/2739 train_loss = 1.213\n",
      "Epoch  16 Batch  976/2739 train_loss = 3.108\n",
      "Epoch  16 Batch 1026/2739 train_loss = 1.604\n",
      "Epoch  16 Batch 1076/2739 train_loss = 3.578\n",
      "Epoch  16 Batch 1126/2739 train_loss = 2.615\n",
      "Epoch  16 Batch 1176/2739 train_loss = 4.248\n",
      "Epoch  16 Batch 1226/2739 train_loss = 3.095\n",
      "Epoch  16 Batch 1276/2739 train_loss = 2.246\n",
      "Epoch  16 Batch 1326/2739 train_loss = 2.874\n",
      "Epoch  16 Batch 1376/2739 train_loss = 1.976\n",
      "Epoch  16 Batch 1426/2739 train_loss = 1.665\n",
      "Epoch  16 Batch 1476/2739 train_loss = 3.349\n",
      "Epoch  16 Batch 1526/2739 train_loss = 2.926\n",
      "Epoch  16 Batch 1576/2739 train_loss = 1.965\n",
      "Epoch  16 Batch 1626/2739 train_loss = 2.528\n",
      "Epoch  16 Batch 1676/2739 train_loss = 3.877\n",
      "Epoch  16 Batch 1726/2739 train_loss = 1.359\n",
      "Epoch  16 Batch 1776/2739 train_loss = 3.877\n",
      "Epoch  16 Batch 1826/2739 train_loss = 2.856\n",
      "Epoch  16 Batch 1876/2739 train_loss = 2.988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  16 Batch 1926/2739 train_loss = 4.006\n",
      "Epoch  16 Batch 1976/2739 train_loss = 2.699\n",
      "Epoch  16 Batch 2026/2739 train_loss = 2.378\n",
      "Epoch  16 Batch 2076/2739 train_loss = 4.063\n",
      "Epoch  16 Batch 2126/2739 train_loss = 0.814\n",
      "Epoch  16 Batch 2176/2739 train_loss = 2.304\n",
      "Epoch  16 Batch 2226/2739 train_loss = 3.318\n",
      "Epoch  16 Batch 2276/2739 train_loss = 1.281\n",
      "Epoch  16 Batch 2326/2739 train_loss = 1.442\n",
      "Epoch  16 Batch 2376/2739 train_loss = 3.279\n",
      "Epoch  16 Batch 2426/2739 train_loss = 2.305\n",
      "Epoch  16 Batch 2476/2739 train_loss = 2.539\n",
      "Epoch  16 Batch 2526/2739 train_loss = 3.112\n",
      "Epoch  16 Batch 2576/2739 train_loss = 3.947\n",
      "Epoch  16 Batch 2626/2739 train_loss = 1.871\n",
      "Epoch  16 Batch 2676/2739 train_loss = 1.770\n",
      "Epoch  16 Batch 2726/2739 train_loss = 3.005\n",
      "Epoch  17 Batch   37/2739 train_loss = 1.365\n",
      "Epoch  17 Batch   87/2739 train_loss = 3.946\n",
      "Epoch  17 Batch  137/2739 train_loss = 2.445\n",
      "Epoch  17 Batch  187/2739 train_loss = 2.804\n",
      "Epoch  17 Batch  237/2739 train_loss = 2.444\n",
      "Epoch  17 Batch  287/2739 train_loss = 1.240\n",
      "Epoch  17 Batch  337/2739 train_loss = 2.022\n",
      "Epoch  17 Batch  387/2739 train_loss = 2.110\n",
      "Epoch  17 Batch  437/2739 train_loss = 2.676\n",
      "Epoch  17 Batch  487/2739 train_loss = 3.924\n",
      "Epoch  17 Batch  537/2739 train_loss = 1.850\n",
      "Epoch  17 Batch  587/2739 train_loss = 2.066\n",
      "Epoch  17 Batch  637/2739 train_loss = 4.087\n",
      "Epoch  17 Batch  687/2739 train_loss = 2.060\n",
      "Epoch  17 Batch  737/2739 train_loss = 1.723\n",
      "Epoch  17 Batch  787/2739 train_loss = 2.512\n",
      "Epoch  17 Batch  837/2739 train_loss = 2.853\n",
      "Epoch  17 Batch  887/2739 train_loss = 2.124\n",
      "Epoch  17 Batch  937/2739 train_loss = 1.566\n",
      "Epoch  17 Batch  987/2739 train_loss = 4.088\n",
      "Epoch  17 Batch 1037/2739 train_loss = 3.148\n",
      "Epoch  17 Batch 1087/2739 train_loss = 2.524\n",
      "Epoch  17 Batch 1137/2739 train_loss = 3.304\n",
      "Epoch  17 Batch 1187/2739 train_loss = 4.499\n",
      "Epoch  17 Batch 1237/2739 train_loss = 1.378\n",
      "Epoch  17 Batch 1287/2739 train_loss = 2.117\n",
      "Epoch  17 Batch 1337/2739 train_loss = 4.237\n",
      "Epoch  17 Batch 1387/2739 train_loss = 1.985\n",
      "Epoch  17 Batch 1437/2739 train_loss = 3.812\n",
      "Epoch  17 Batch 1487/2739 train_loss = 1.028\n",
      "Epoch  17 Batch 1537/2739 train_loss = 2.086\n",
      "Epoch  17 Batch 1587/2739 train_loss = 2.998\n",
      "Epoch  17 Batch 1637/2739 train_loss = 3.916\n",
      "Epoch  17 Batch 1687/2739 train_loss = 4.088\n",
      "Epoch  17 Batch 1737/2739 train_loss = 2.368\n",
      "Epoch  17 Batch 1787/2739 train_loss = 2.376\n",
      "Epoch  17 Batch 1837/2739 train_loss = 2.237\n",
      "Epoch  17 Batch 1887/2739 train_loss = 3.798\n",
      "Epoch  17 Batch 1937/2739 train_loss = 1.936\n",
      "Epoch  17 Batch 1987/2739 train_loss = 2.698\n",
      "Epoch  17 Batch 2037/2739 train_loss = 4.274\n",
      "Epoch  17 Batch 2087/2739 train_loss = 4.153\n",
      "Epoch  17 Batch 2137/2739 train_loss = 1.833\n",
      "Epoch  17 Batch 2187/2739 train_loss = 2.145\n",
      "Epoch  17 Batch 2237/2739 train_loss = 4.034\n",
      "Epoch  17 Batch 2287/2739 train_loss = 4.283\n",
      "Epoch  17 Batch 2337/2739 train_loss = 3.621\n",
      "Epoch  17 Batch 2387/2739 train_loss = 1.768\n",
      "Epoch  17 Batch 2437/2739 train_loss = 1.724\n",
      "Epoch  17 Batch 2487/2739 train_loss = 1.858\n",
      "Epoch  17 Batch 2537/2739 train_loss = 1.718\n",
      "Epoch  17 Batch 2587/2739 train_loss = 4.078\n",
      "Epoch  17 Batch 2637/2739 train_loss = 4.496\n",
      "Epoch  17 Batch 2687/2739 train_loss = 1.457\n",
      "Epoch  17 Batch 2737/2739 train_loss = 3.353\n",
      "Epoch  18 Batch   48/2739 train_loss = 1.246\n",
      "Epoch  18 Batch   98/2739 train_loss = 1.775\n",
      "Epoch  18 Batch  148/2739 train_loss = 4.085\n",
      "Epoch  18 Batch  198/2739 train_loss = 2.074\n",
      "Epoch  18 Batch  248/2739 train_loss = 2.962\n",
      "Epoch  18 Batch  298/2739 train_loss = 2.634\n",
      "Epoch  18 Batch  348/2739 train_loss = 2.408\n",
      "Epoch  18 Batch  398/2739 train_loss = 2.311\n",
      "Epoch  18 Batch  448/2739 train_loss = 3.659\n",
      "Epoch  18 Batch  498/2739 train_loss = 2.706\n",
      "Epoch  18 Batch  548/2739 train_loss = 1.620\n",
      "Epoch  18 Batch  598/2739 train_loss = 2.191\n",
      "Epoch  18 Batch  648/2739 train_loss = 2.073\n",
      "Epoch  18 Batch  698/2739 train_loss = 3.136\n",
      "Epoch  18 Batch  748/2739 train_loss = 2.096\n",
      "Epoch  18 Batch  798/2739 train_loss = 2.434\n",
      "Epoch  18 Batch  848/2739 train_loss = 3.762\n",
      "Epoch  18 Batch  898/2739 train_loss = 3.551\n",
      "Epoch  18 Batch  948/2739 train_loss = 2.307\n",
      "Epoch  18 Batch  998/2739 train_loss = 2.356\n",
      "Epoch  18 Batch 1048/2739 train_loss = 2.191\n",
      "Epoch  18 Batch 1098/2739 train_loss = 2.206\n",
      "Epoch  18 Batch 1148/2739 train_loss = 3.408\n",
      "Epoch  18 Batch 1198/2739 train_loss = 2.071\n",
      "Epoch  18 Batch 1248/2739 train_loss = 1.400\n",
      "Epoch  18 Batch 1298/2739 train_loss = 1.999\n",
      "Epoch  18 Batch 1348/2739 train_loss = 1.282\n",
      "Epoch  18 Batch 1398/2739 train_loss = 2.765\n",
      "Epoch  18 Batch 1448/2739 train_loss = 3.205\n",
      "Epoch  18 Batch 1498/2739 train_loss = 2.307\n",
      "Epoch  18 Batch 1548/2739 train_loss = 1.619\n",
      "Epoch  18 Batch 1598/2739 train_loss = 1.602\n",
      "Epoch  18 Batch 1648/2739 train_loss = 1.560\n",
      "Epoch  18 Batch 1698/2739 train_loss = 2.339\n",
      "Epoch  18 Batch 1748/2739 train_loss = 2.101\n",
      "Epoch  18 Batch 1798/2739 train_loss = 2.048\n",
      "Epoch  18 Batch 1848/2739 train_loss = 2.246\n",
      "Epoch  18 Batch 1898/2739 train_loss = 2.218\n",
      "Epoch  18 Batch 1948/2739 train_loss = 3.665\n",
      "Epoch  18 Batch 1998/2739 train_loss = 2.272\n",
      "Epoch  18 Batch 2048/2739 train_loss = 3.825\n",
      "Epoch  18 Batch 2098/2739 train_loss = 4.144\n",
      "Epoch  18 Batch 2148/2739 train_loss = 2.795\n",
      "Epoch  18 Batch 2198/2739 train_loss = 4.219\n",
      "Epoch  18 Batch 2248/2739 train_loss = 3.220\n",
      "Epoch  18 Batch 2298/2739 train_loss = 1.764\n",
      "Epoch  18 Batch 2348/2739 train_loss = 2.895\n",
      "Epoch  18 Batch 2398/2739 train_loss = 3.647\n",
      "Epoch  18 Batch 2448/2739 train_loss = 1.294\n",
      "Epoch  18 Batch 2498/2739 train_loss = 2.177\n",
      "Epoch  18 Batch 2548/2739 train_loss = 1.618\n",
      "Epoch  18 Batch 2598/2739 train_loss = 4.523\n",
      "Epoch  18 Batch 2648/2739 train_loss = 3.065\n",
      "Epoch  18 Batch 2698/2739 train_loss = 1.239\n",
      "Epoch  19 Batch    9/2739 train_loss = 0.911\n",
      "Epoch  19 Batch   59/2739 train_loss = 2.147\n",
      "Epoch  19 Batch  109/2739 train_loss = 2.811\n",
      "Epoch  19 Batch  159/2739 train_loss = 2.239\n",
      "Epoch  19 Batch  209/2739 train_loss = 1.982\n",
      "Epoch  19 Batch  259/2739 train_loss = 1.549\n",
      "Epoch  19 Batch  309/2739 train_loss = 2.870\n",
      "Epoch  19 Batch  359/2739 train_loss = 1.602\n",
      "Epoch  19 Batch  409/2739 train_loss = 1.910\n",
      "Epoch  19 Batch  459/2739 train_loss = 3.488\n",
      "Epoch  19 Batch  509/2739 train_loss = 2.257\n",
      "Epoch  19 Batch  559/2739 train_loss = 2.218\n",
      "Epoch  19 Batch  609/2739 train_loss = 2.171\n",
      "Epoch  19 Batch  659/2739 train_loss = 2.195\n",
      "Epoch  19 Batch  709/2739 train_loss = 2.192\n",
      "Epoch  19 Batch  759/2739 train_loss = 3.385\n",
      "Epoch  19 Batch  809/2739 train_loss = 2.083\n",
      "Epoch  19 Batch  859/2739 train_loss = 3.727\n",
      "Epoch  19 Batch  909/2739 train_loss = 1.340\n",
      "Epoch  19 Batch  959/2739 train_loss = 4.000\n",
      "Epoch  19 Batch 1009/2739 train_loss = 1.750\n",
      "Epoch  19 Batch 1059/2739 train_loss = 1.736\n",
      "Epoch  19 Batch 1109/2739 train_loss = 2.861\n",
      "Epoch  19 Batch 1159/2739 train_loss = 3.544\n",
      "Epoch  19 Batch 1209/2739 train_loss = 1.853\n",
      "Epoch  19 Batch 1259/2739 train_loss = 1.736\n",
      "Epoch  19 Batch 1309/2739 train_loss = 2.521\n",
      "Epoch  19 Batch 1359/2739 train_loss = 2.603\n",
      "Epoch  19 Batch 1409/2739 train_loss = 3.228\n",
      "Epoch  19 Batch 1459/2739 train_loss = 3.846\n",
      "Epoch  19 Batch 1509/2739 train_loss = 2.121\n",
      "Epoch  19 Batch 1559/2739 train_loss = 2.087\n",
      "Epoch  19 Batch 1609/2739 train_loss = 2.059\n",
      "Epoch  19 Batch 1659/2739 train_loss = 3.202\n",
      "Epoch  19 Batch 1709/2739 train_loss = 3.483\n",
      "Epoch  19 Batch 1759/2739 train_loss = 1.817\n",
      "Epoch  19 Batch 1809/2739 train_loss = 2.417\n",
      "Epoch  19 Batch 1859/2739 train_loss = 2.749\n",
      "Epoch  19 Batch 1909/2739 train_loss = 3.111\n",
      "Epoch  19 Batch 1959/2739 train_loss = 4.609\n",
      "Epoch  19 Batch 2009/2739 train_loss = 2.482\n",
      "Epoch  19 Batch 2059/2739 train_loss = 2.603\n",
      "Epoch  19 Batch 2109/2739 train_loss = 1.745\n",
      "Epoch  19 Batch 2159/2739 train_loss = 2.615\n",
      "Epoch  19 Batch 2209/2739 train_loss = 3.850\n",
      "Epoch  19 Batch 2259/2739 train_loss = 4.361\n",
      "Epoch  19 Batch 2309/2739 train_loss = 3.471\n",
      "Epoch  19 Batch 2359/2739 train_loss = 2.113\n",
      "Epoch  19 Batch 2409/2739 train_loss = 2.217\n",
      "Epoch  19 Batch 2459/2739 train_loss = 2.235\n",
      "Epoch  19 Batch 2509/2739 train_loss = 1.767\n",
      "Epoch  19 Batch 2559/2739 train_loss = 2.220\n",
      "Epoch  19 Batch 2609/2739 train_loss = 4.216\n",
      "Epoch  19 Batch 2659/2739 train_loss = 1.190\n",
      "Epoch  19 Batch 2709/2739 train_loss = 2.494\n",
      "Epoch  20 Batch   20/2739 train_loss = 1.907\n",
      "Epoch  20 Batch   70/2739 train_loss = 2.911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  20 Batch  120/2739 train_loss = 4.085\n",
      "Epoch  20 Batch  170/2739 train_loss = 2.308\n",
      "Epoch  20 Batch  220/2739 train_loss = 1.881\n",
      "Epoch  20 Batch  270/2739 train_loss = 1.497\n",
      "Epoch  20 Batch  320/2739 train_loss = 2.750\n",
      "Epoch  20 Batch  370/2739 train_loss = 1.958\n",
      "Epoch  20 Batch  420/2739 train_loss = 3.637\n",
      "Epoch  20 Batch  470/2739 train_loss = 3.235\n",
      "Epoch  20 Batch  520/2739 train_loss = 2.332\n",
      "Epoch  20 Batch  570/2739 train_loss = 2.302\n",
      "Epoch  20 Batch  620/2739 train_loss = 2.532\n",
      "Epoch  20 Batch  670/2739 train_loss = 2.435\n",
      "Epoch  20 Batch  720/2739 train_loss = 2.454\n",
      "Epoch  20 Batch  770/2739 train_loss = 1.046\n",
      "Epoch  20 Batch  820/2739 train_loss = 2.607\n",
      "Epoch  20 Batch  870/2739 train_loss = 3.594\n",
      "Epoch  20 Batch  920/2739 train_loss = 1.887\n",
      "Epoch  20 Batch  970/2739 train_loss = 3.691\n",
      "Epoch  20 Batch 1020/2739 train_loss = 2.443\n",
      "Epoch  20 Batch 1070/2739 train_loss = 3.851\n",
      "Epoch  20 Batch 1120/2739 train_loss = 4.029\n",
      "Epoch  20 Batch 1170/2739 train_loss = 2.383\n",
      "Epoch  20 Batch 1220/2739 train_loss = 1.459\n",
      "Epoch  20 Batch 1270/2739 train_loss = 3.237\n",
      "Epoch  20 Batch 1320/2739 train_loss = 4.053\n",
      "Epoch  20 Batch 1370/2739 train_loss = 3.202\n",
      "Epoch  20 Batch 1420/2739 train_loss = 1.806\n",
      "Epoch  20 Batch 1470/2739 train_loss = 2.203\n",
      "Epoch  20 Batch 1520/2739 train_loss = 3.767\n",
      "Epoch  20 Batch 1570/2739 train_loss = 2.222\n",
      "Epoch  20 Batch 1620/2739 train_loss = 1.473\n",
      "Epoch  20 Batch 1670/2739 train_loss = 2.918\n",
      "Epoch  20 Batch 1720/2739 train_loss = 2.274\n",
      "Epoch  20 Batch 1770/2739 train_loss = 2.017\n",
      "Epoch  20 Batch 1820/2739 train_loss = 3.970\n",
      "Epoch  20 Batch 1870/2739 train_loss = 3.835\n",
      "Epoch  20 Batch 1920/2739 train_loss = 3.119\n",
      "Epoch  20 Batch 1970/2739 train_loss = 2.308\n",
      "Epoch  20 Batch 2020/2739 train_loss = 1.684\n",
      "Epoch  20 Batch 2070/2739 train_loss = 2.836\n",
      "Epoch  20 Batch 2120/2739 train_loss = 2.207\n",
      "Epoch  20 Batch 2170/2739 train_loss = 2.071\n",
      "Epoch  20 Batch 2220/2739 train_loss = 1.524\n",
      "Epoch  20 Batch 2270/2739 train_loss = 2.427\n",
      "Epoch  20 Batch 2320/2739 train_loss = 4.562\n",
      "Epoch  20 Batch 2370/2739 train_loss = 1.449\n",
      "Epoch  20 Batch 2420/2739 train_loss = 2.993\n",
      "Epoch  20 Batch 2470/2739 train_loss = 1.786\n",
      "Epoch  20 Batch 2520/2739 train_loss = 2.576\n",
      "Epoch  20 Batch 2570/2739 train_loss = 1.590\n",
      "Epoch  20 Batch 2620/2739 train_loss = 3.108\n",
      "Epoch  20 Batch 2670/2739 train_loss = 2.284\n",
      "Epoch  20 Batch 2720/2739 train_loss = 1.802\n",
      "Epoch  21 Batch   31/2739 train_loss = 3.413\n",
      "Epoch  21 Batch   81/2739 train_loss = 1.383\n",
      "Epoch  21 Batch  131/2739 train_loss = 2.470\n",
      "Epoch  21 Batch  181/2739 train_loss = 2.571\n",
      "Epoch  21 Batch  231/2739 train_loss = 1.632\n",
      "Epoch  21 Batch  281/2739 train_loss = 2.149\n",
      "Epoch  21 Batch  331/2739 train_loss = 2.082\n",
      "Epoch  21 Batch  381/2739 train_loss = 1.772\n",
      "Epoch  21 Batch  431/2739 train_loss = 1.786\n",
      "Epoch  21 Batch  481/2739 train_loss = 2.987\n",
      "Epoch  21 Batch  531/2739 train_loss = 2.024\n",
      "Epoch  21 Batch  581/2739 train_loss = 2.875\n",
      "Epoch  21 Batch  631/2739 train_loss = 3.235\n",
      "Epoch  21 Batch  681/2739 train_loss = 1.583\n",
      "Epoch  21 Batch  731/2739 train_loss = 3.662\n",
      "Epoch  21 Batch  781/2739 train_loss = 2.134\n",
      "Epoch  21 Batch  831/2739 train_loss = 2.349\n",
      "Epoch  21 Batch  881/2739 train_loss = 1.955\n",
      "Epoch  21 Batch  931/2739 train_loss = 3.168\n",
      "Epoch  21 Batch  981/2739 train_loss = 1.857\n",
      "Epoch  21 Batch 1031/2739 train_loss = 1.920\n",
      "Epoch  21 Batch 1081/2739 train_loss = 2.071\n",
      "Epoch  21 Batch 1131/2739 train_loss = 1.629\n",
      "Epoch  21 Batch 1181/2739 train_loss = 3.800\n",
      "Epoch  21 Batch 1231/2739 train_loss = 3.364\n",
      "Epoch  21 Batch 1281/2739 train_loss = 1.732\n",
      "Epoch  21 Batch 1331/2739 train_loss = 4.099\n",
      "Epoch  21 Batch 1381/2739 train_loss = 3.067\n",
      "Epoch  21 Batch 1431/2739 train_loss = 2.933\n",
      "Epoch  21 Batch 1481/2739 train_loss = 1.251\n",
      "Epoch  21 Batch 1531/2739 train_loss = 1.815\n",
      "Epoch  21 Batch 1581/2739 train_loss = 2.280\n",
      "Epoch  21 Batch 1631/2739 train_loss = 3.972\n",
      "Epoch  21 Batch 1681/2739 train_loss = 2.232\n",
      "Epoch  21 Batch 1731/2739 train_loss = 4.198\n",
      "Epoch  21 Batch 1781/2739 train_loss = 2.653\n",
      "Epoch  21 Batch 1831/2739 train_loss = 2.421\n",
      "Epoch  21 Batch 1881/2739 train_loss = 3.814\n",
      "Epoch  21 Batch 1931/2739 train_loss = 4.588\n",
      "Epoch  21 Batch 1981/2739 train_loss = 2.142\n",
      "Epoch  21 Batch 2031/2739 train_loss = 2.749\n",
      "Epoch  21 Batch 2081/2739 train_loss = 3.856\n",
      "Epoch  21 Batch 2131/2739 train_loss = 2.882\n",
      "Epoch  21 Batch 2181/2739 train_loss = 2.583\n",
      "Epoch  21 Batch 2231/2739 train_loss = 1.773\n",
      "Epoch  21 Batch 2281/2739 train_loss = 4.526\n",
      "Epoch  21 Batch 2331/2739 train_loss = 1.709\n",
      "Epoch  21 Batch 2381/2739 train_loss = 1.661\n",
      "Epoch  21 Batch 2431/2739 train_loss = 1.702\n",
      "Epoch  21 Batch 2481/2739 train_loss = 3.183\n",
      "Epoch  21 Batch 2531/2739 train_loss = 3.336\n",
      "Epoch  21 Batch 2581/2739 train_loss = 2.259\n",
      "Epoch  21 Batch 2631/2739 train_loss = 1.966\n",
      "Epoch  21 Batch 2681/2739 train_loss = 2.313\n",
      "Epoch  21 Batch 2731/2739 train_loss = 1.413\n",
      "Epoch  22 Batch   42/2739 train_loss = 2.996\n",
      "Epoch  22 Batch   92/2739 train_loss = 1.435\n",
      "Epoch  22 Batch  142/2739 train_loss = 2.362\n",
      "Epoch  22 Batch  192/2739 train_loss = 1.094\n",
      "Epoch  22 Batch  242/2739 train_loss = 2.660\n",
      "Epoch  22 Batch  292/2739 train_loss = 2.632\n",
      "Epoch  22 Batch  342/2739 train_loss = 2.247\n",
      "Epoch  22 Batch  392/2739 train_loss = 1.324\n",
      "Epoch  22 Batch  442/2739 train_loss = 1.068\n",
      "Epoch  22 Batch  492/2739 train_loss = 2.318\n",
      "Epoch  22 Batch  542/2739 train_loss = 2.315\n",
      "Epoch  22 Batch  592/2739 train_loss = 3.324\n",
      "Epoch  22 Batch  642/2739 train_loss = 2.834\n",
      "Epoch  22 Batch  692/2739 train_loss = 1.532\n",
      "Epoch  22 Batch  742/2739 train_loss = 2.069\n",
      "Epoch  22 Batch  792/2739 train_loss = 3.491\n",
      "Epoch  22 Batch  842/2739 train_loss = 2.266\n",
      "Epoch  22 Batch  892/2739 train_loss = 1.682\n",
      "Epoch  22 Batch  942/2739 train_loss = 3.087\n",
      "Epoch  22 Batch  992/2739 train_loss = 1.869\n",
      "Epoch  22 Batch 1042/2739 train_loss = 4.017\n",
      "Epoch  22 Batch 1092/2739 train_loss = 3.019\n",
      "Epoch  22 Batch 1142/2739 train_loss = 4.061\n",
      "Epoch  22 Batch 1192/2739 train_loss = 1.798\n",
      "Epoch  22 Batch 1242/2739 train_loss = 1.948\n",
      "Epoch  22 Batch 1292/2739 train_loss = 2.143\n",
      "Epoch  22 Batch 1342/2739 train_loss = 1.805\n",
      "Epoch  22 Batch 1392/2739 train_loss = 3.794\n",
      "Epoch  22 Batch 1442/2739 train_loss = 1.952\n",
      "Epoch  22 Batch 1492/2739 train_loss = 1.874\n",
      "Epoch  22 Batch 1542/2739 train_loss = 2.916\n",
      "Epoch  22 Batch 1592/2739 train_loss = 2.798\n",
      "Epoch  22 Batch 1642/2739 train_loss = 3.815\n",
      "Epoch  22 Batch 1692/2739 train_loss = 3.707\n",
      "Epoch  22 Batch 1742/2739 train_loss = 3.888\n",
      "Epoch  22 Batch 1792/2739 train_loss = 2.574\n",
      "Epoch  22 Batch 1842/2739 train_loss = 2.102\n",
      "Epoch  22 Batch 1892/2739 train_loss = 3.161\n",
      "Epoch  22 Batch 1942/2739 train_loss = 2.373\n",
      "Epoch  22 Batch 1992/2739 train_loss = 2.007\n",
      "Epoch  22 Batch 2042/2739 train_loss = 3.623\n",
      "Epoch  22 Batch 2092/2739 train_loss = 3.546\n",
      "Epoch  22 Batch 2142/2739 train_loss = 3.738\n",
      "Epoch  22 Batch 2192/2739 train_loss = 2.653\n",
      "Epoch  22 Batch 2242/2739 train_loss = 1.904\n",
      "Epoch  22 Batch 2292/2739 train_loss = 1.811\n",
      "Epoch  22 Batch 2342/2739 train_loss = 1.737\n",
      "Epoch  22 Batch 2392/2739 train_loss = 2.929\n",
      "Epoch  22 Batch 2442/2739 train_loss = 1.824\n",
      "Epoch  22 Batch 2492/2739 train_loss = 1.791\n",
      "Epoch  22 Batch 2542/2739 train_loss = 3.940\n",
      "Epoch  22 Batch 2592/2739 train_loss = 3.942\n",
      "Epoch  22 Batch 2642/2739 train_loss = 1.122\n",
      "Epoch  22 Batch 2692/2739 train_loss = 2.167\n",
      "Epoch  23 Batch    3/2739 train_loss = 2.694\n",
      "Epoch  23 Batch   53/2739 train_loss = 1.793\n",
      "Epoch  23 Batch  103/2739 train_loss = 1.828\n",
      "Epoch  23 Batch  153/2739 train_loss = 2.232\n",
      "Epoch  23 Batch  203/2739 train_loss = 1.555\n",
      "Epoch  23 Batch  253/2739 train_loss = 2.370\n",
      "Epoch  23 Batch  303/2739 train_loss = 2.434\n",
      "Epoch  23 Batch  353/2739 train_loss = 2.530\n",
      "Epoch  23 Batch  403/2739 train_loss = 2.156\n",
      "Epoch  23 Batch  453/2739 train_loss = 3.417\n",
      "Epoch  23 Batch  503/2739 train_loss = 2.205\n",
      "Epoch  23 Batch  553/2739 train_loss = 1.734\n",
      "Epoch  23 Batch  603/2739 train_loss = 2.391\n",
      "Epoch  23 Batch  653/2739 train_loss = 1.716\n",
      "Epoch  23 Batch  703/2739 train_loss = 2.006\n",
      "Epoch  23 Batch  753/2739 train_loss = 1.935\n",
      "Epoch  23 Batch  803/2739 train_loss = 1.683\n",
      "Epoch  23 Batch  853/2739 train_loss = 1.885\n",
      "Epoch  23 Batch  903/2739 train_loss = 3.363\n",
      "Epoch  23 Batch  953/2739 train_loss = 2.923\n",
      "Epoch  23 Batch 1003/2739 train_loss = 3.366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  23 Batch 1053/2739 train_loss = 3.997\n",
      "Epoch  23 Batch 1103/2739 train_loss = 2.363\n",
      "Epoch  23 Batch 1153/2739 train_loss = 2.300\n",
      "Epoch  23 Batch 1203/2739 train_loss = 1.625\n",
      "Epoch  23 Batch 1253/2739 train_loss = 3.565\n",
      "Epoch  23 Batch 1303/2739 train_loss = 1.951\n",
      "Epoch  23 Batch 1353/2739 train_loss = 3.791\n",
      "Epoch  23 Batch 1403/2739 train_loss = 2.486\n",
      "Epoch  23 Batch 1453/2739 train_loss = 0.988\n",
      "Epoch  23 Batch 1503/2739 train_loss = 1.590\n",
      "Epoch  23 Batch 1553/2739 train_loss = 3.624\n",
      "Epoch  23 Batch 1603/2739 train_loss = 1.639\n",
      "Epoch  23 Batch 1653/2739 train_loss = 3.082\n",
      "Epoch  23 Batch 1703/2739 train_loss = 3.021\n",
      "Epoch  23 Batch 1753/2739 train_loss = 3.817\n",
      "Epoch  23 Batch 1803/2739 train_loss = 3.235\n",
      "Epoch  23 Batch 1853/2739 train_loss = 1.712\n",
      "Epoch  23 Batch 1903/2739 train_loss = 2.200\n",
      "Epoch  23 Batch 1953/2739 train_loss = 1.390\n",
      "Epoch  23 Batch 2003/2739 train_loss = 2.229\n",
      "Epoch  23 Batch 2053/2739 train_loss = 2.401\n",
      "Epoch  23 Batch 2103/2739 train_loss = 2.612\n",
      "Epoch  23 Batch 2153/2739 train_loss = 2.975\n",
      "Epoch  23 Batch 2203/2739 train_loss = 3.049\n",
      "Epoch  23 Batch 2253/2739 train_loss = 1.999\n",
      "Epoch  23 Batch 2303/2739 train_loss = 4.711\n",
      "Epoch  23 Batch 2353/2739 train_loss = 4.224\n",
      "Epoch  23 Batch 2403/2739 train_loss = 3.209\n",
      "Epoch  23 Batch 2453/2739 train_loss = 2.418\n",
      "Epoch  23 Batch 2503/2739 train_loss = 1.873\n",
      "Epoch  23 Batch 2553/2739 train_loss = 1.877\n",
      "Epoch  23 Batch 2603/2739 train_loss = 2.375\n",
      "Epoch  23 Batch 2653/2739 train_loss = 1.287\n",
      "Epoch  23 Batch 2703/2739 train_loss = 2.618\n",
      "Epoch  24 Batch   14/2739 train_loss = 1.710\n",
      "Epoch  24 Batch   64/2739 train_loss = 2.495\n",
      "Epoch  24 Batch  114/2739 train_loss = 1.778\n",
      "Epoch  24 Batch  164/2739 train_loss = 2.454\n",
      "Epoch  24 Batch  214/2739 train_loss = 2.170\n",
      "Epoch  24 Batch  264/2739 train_loss = 2.143\n",
      "Epoch  24 Batch  314/2739 train_loss = 2.549\n",
      "Epoch  24 Batch  364/2739 train_loss = 2.570\n",
      "Epoch  24 Batch  414/2739 train_loss = 2.489\n",
      "Epoch  24 Batch  464/2739 train_loss = 2.334\n",
      "Epoch  24 Batch  514/2739 train_loss = 2.254\n",
      "Epoch  24 Batch  564/2739 train_loss = 2.455\n",
      "Epoch  24 Batch  614/2739 train_loss = 4.195\n",
      "Epoch  24 Batch  664/2739 train_loss = 3.299\n",
      "Epoch  24 Batch  714/2739 train_loss = 2.331\n",
      "Epoch  24 Batch  764/2739 train_loss = 1.762\n",
      "Epoch  24 Batch  814/2739 train_loss = 2.451\n",
      "Epoch  24 Batch  864/2739 train_loss = 3.668\n",
      "Epoch  24 Batch  914/2739 train_loss = 2.044\n",
      "Epoch  24 Batch  964/2739 train_loss = 4.283\n",
      "Epoch  24 Batch 1014/2739 train_loss = 2.138\n",
      "Epoch  24 Batch 1064/2739 train_loss = 1.960\n",
      "Epoch  24 Batch 1114/2739 train_loss = 1.742\n",
      "Epoch  24 Batch 1164/2739 train_loss = 1.813\n",
      "Epoch  24 Batch 1214/2739 train_loss = 1.813\n",
      "Epoch  24 Batch 1264/2739 train_loss = 2.844\n",
      "Epoch  24 Batch 1314/2739 train_loss = 2.468\n",
      "Epoch  24 Batch 1364/2739 train_loss = 3.107\n",
      "Epoch  24 Batch 1414/2739 train_loss = 2.281\n",
      "Epoch  24 Batch 1464/2739 train_loss = 1.267\n",
      "Epoch  24 Batch 1514/2739 train_loss = 1.664\n",
      "Epoch  24 Batch 1564/2739 train_loss = 2.171\n",
      "Epoch  24 Batch 1614/2739 train_loss = 1.329\n",
      "Epoch  24 Batch 1664/2739 train_loss = 2.067\n",
      "Epoch  24 Batch 1714/2739 train_loss = 2.568\n",
      "Epoch  24 Batch 1764/2739 train_loss = 2.307\n",
      "Epoch  24 Batch 1814/2739 train_loss = 2.105\n",
      "Epoch  24 Batch 1864/2739 train_loss = 2.324\n",
      "Epoch  24 Batch 1914/2739 train_loss = 4.019\n",
      "Epoch  24 Batch 1964/2739 train_loss = 2.718\n",
      "Epoch  24 Batch 2014/2739 train_loss = 1.834\n",
      "Epoch  24 Batch 2064/2739 train_loss = 4.190\n",
      "Epoch  24 Batch 2114/2739 train_loss = 3.958\n",
      "Epoch  24 Batch 2164/2739 train_loss = 2.077\n",
      "Epoch  24 Batch 2214/2739 train_loss = 3.180\n",
      "Epoch  24 Batch 2264/2739 train_loss = 4.115\n",
      "Epoch  24 Batch 2314/2739 train_loss = 3.945\n",
      "Epoch  24 Batch 2364/2739 train_loss = 1.567\n",
      "Epoch  24 Batch 2414/2739 train_loss = 2.470\n",
      "Epoch  24 Batch 2464/2739 train_loss = 2.710\n",
      "Epoch  24 Batch 2514/2739 train_loss = 1.824\n",
      "Epoch  24 Batch 2564/2739 train_loss = 1.394\n",
      "Epoch  24 Batch 2614/2739 train_loss = 2.438\n",
      "Epoch  24 Batch 2664/2739 train_loss = 2.494\n",
      "Epoch  24 Batch 2714/2739 train_loss = 2.571\n",
      "Epoch  25 Batch   25/2739 train_loss = 1.893\n",
      "Epoch  25 Batch   75/2739 train_loss = 3.621\n",
      "Epoch  25 Batch  125/2739 train_loss = 2.909\n",
      "Epoch  25 Batch  175/2739 train_loss = 2.356\n",
      "Epoch  25 Batch  225/2739 train_loss = 2.558\n",
      "Epoch  25 Batch  275/2739 train_loss = 2.962\n",
      "Epoch  25 Batch  325/2739 train_loss = 3.343\n",
      "Epoch  25 Batch  375/2739 train_loss = 2.973\n",
      "Epoch  25 Batch  425/2739 train_loss = 1.447\n",
      "Epoch  25 Batch  475/2739 train_loss = 1.759\n",
      "Epoch  25 Batch  525/2739 train_loss = 1.869\n",
      "Epoch  25 Batch  575/2739 train_loss = 3.783\n",
      "Epoch  25 Batch  625/2739 train_loss = 2.034\n",
      "Epoch  25 Batch  675/2739 train_loss = 2.764\n",
      "Epoch  25 Batch  725/2739 train_loss = 2.197\n",
      "Epoch  25 Batch  775/2739 train_loss = 3.615\n",
      "Epoch  25 Batch  825/2739 train_loss = 2.957\n",
      "Epoch  25 Batch  875/2739 train_loss = 1.630\n",
      "Epoch  25 Batch  925/2739 train_loss = 1.856\n",
      "Epoch  25 Batch  975/2739 train_loss = 1.925\n",
      "Epoch  25 Batch 1025/2739 train_loss = 1.795\n",
      "Epoch  25 Batch 1075/2739 train_loss = 1.595\n",
      "Epoch  25 Batch 1125/2739 train_loss = 3.389\n",
      "Epoch  25 Batch 1175/2739 train_loss = 4.106\n",
      "Epoch  25 Batch 1225/2739 train_loss = 2.192\n",
      "Epoch  25 Batch 1275/2739 train_loss = 2.135\n",
      "Epoch  25 Batch 1325/2739 train_loss = 2.940\n",
      "Epoch  25 Batch 1375/2739 train_loss = 2.072\n",
      "Epoch  25 Batch 1425/2739 train_loss = 1.378\n",
      "Epoch  25 Batch 1475/2739 train_loss = 1.827\n",
      "Epoch  25 Batch 1525/2739 train_loss = 2.892\n",
      "Epoch  25 Batch 1575/2739 train_loss = 3.539\n",
      "Epoch  25 Batch 1625/2739 train_loss = 1.883\n",
      "Epoch  25 Batch 1675/2739 train_loss = 1.939\n",
      "Epoch  25 Batch 1725/2739 train_loss = 2.098\n",
      "Epoch  25 Batch 1775/2739 train_loss = 1.316\n",
      "Epoch  25 Batch 1825/2739 train_loss = 3.939\n",
      "Epoch  25 Batch 1875/2739 train_loss = 1.293\n",
      "Epoch  25 Batch 1925/2739 train_loss = 2.138\n",
      "Epoch  25 Batch 1975/2739 train_loss = 2.016\n",
      "Epoch  25 Batch 2025/2739 train_loss = 1.690\n",
      "Epoch  25 Batch 2075/2739 train_loss = 3.000\n",
      "Epoch  25 Batch 2125/2739 train_loss = 3.861\n",
      "Epoch  25 Batch 2175/2739 train_loss = 2.318\n",
      "Epoch  25 Batch 2225/2739 train_loss = 3.483\n",
      "Epoch  25 Batch 2275/2739 train_loss = 2.348\n",
      "Epoch  25 Batch 2325/2739 train_loss = 2.569\n",
      "Epoch  25 Batch 2375/2739 train_loss = 1.514\n",
      "Epoch  25 Batch 2425/2739 train_loss = 2.273\n",
      "Epoch  25 Batch 2475/2739 train_loss = 1.629\n",
      "Epoch  25 Batch 2525/2739 train_loss = 1.027\n",
      "Epoch  25 Batch 2575/2739 train_loss = 3.989\n",
      "Epoch  25 Batch 2625/2739 train_loss = 2.307\n",
      "Epoch  25 Batch 2675/2739 train_loss = 1.781\n",
      "Epoch  25 Batch 2725/2739 train_loss = 4.030\n",
      "Epoch  26 Batch   36/2739 train_loss = 3.251\n",
      "Epoch  26 Batch   86/2739 train_loss = 2.321\n",
      "Epoch  26 Batch  136/2739 train_loss = 1.928\n",
      "Epoch  26 Batch  186/2739 train_loss = 1.671\n",
      "Epoch  26 Batch  236/2739 train_loss = 1.648\n",
      "Epoch  26 Batch  286/2739 train_loss = 2.023\n",
      "Epoch  26 Batch  336/2739 train_loss = 2.776\n",
      "Epoch  26 Batch  386/2739 train_loss = 3.440\n",
      "Epoch  26 Batch  436/2739 train_loss = 2.707\n",
      "Epoch  26 Batch  486/2739 train_loss = 3.553\n",
      "Epoch  26 Batch  536/2739 train_loss = 1.981\n",
      "Epoch  26 Batch  586/2739 train_loss = 2.822\n",
      "Epoch  26 Batch  636/2739 train_loss = 3.998\n",
      "Epoch  26 Batch  686/2739 train_loss = 2.751\n",
      "Epoch  26 Batch  736/2739 train_loss = 3.579\n",
      "Epoch  26 Batch  786/2739 train_loss = 3.744\n",
      "Epoch  26 Batch  836/2739 train_loss = 2.536\n",
      "Epoch  26 Batch  886/2739 train_loss = 3.007\n",
      "Epoch  26 Batch  936/2739 train_loss = 2.461\n",
      "Epoch  26 Batch  986/2739 train_loss = 1.445\n",
      "Epoch  26 Batch 1036/2739 train_loss = 2.337\n",
      "Epoch  26 Batch 1086/2739 train_loss = 2.447\n",
      "Epoch  26 Batch 1136/2739 train_loss = 2.839\n",
      "Epoch  26 Batch 1186/2739 train_loss = 4.356\n",
      "Epoch  26 Batch 1236/2739 train_loss = 2.714\n",
      "Epoch  26 Batch 1286/2739 train_loss = 1.986\n",
      "Epoch  26 Batch 1336/2739 train_loss = 4.283\n",
      "Epoch  26 Batch 1386/2739 train_loss = 2.458\n",
      "Epoch  26 Batch 1436/2739 train_loss = 2.094\n",
      "Epoch  26 Batch 1486/2739 train_loss = 3.397\n",
      "Epoch  26 Batch 1536/2739 train_loss = 2.172\n",
      "Epoch  26 Batch 1586/2739 train_loss = 1.955\n",
      "Epoch  26 Batch 1636/2739 train_loss = 1.364\n",
      "Epoch  26 Batch 1686/2739 train_loss = 3.867\n",
      "Epoch  26 Batch 1736/2739 train_loss = 2.080\n",
      "Epoch  26 Batch 1786/2739 train_loss = 2.186\n",
      "Epoch  26 Batch 1836/2739 train_loss = 3.866\n",
      "Epoch  26 Batch 1886/2739 train_loss = 3.777\n",
      "Epoch  26 Batch 1936/2739 train_loss = 1.882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  26 Batch 1986/2739 train_loss = 2.571\n",
      "Epoch  26 Batch 2036/2739 train_loss = 3.919\n",
      "Epoch  26 Batch 2086/2739 train_loss = 2.645\n",
      "Epoch  26 Batch 2136/2739 train_loss = 4.260\n",
      "Epoch  26 Batch 2186/2739 train_loss = 1.688\n",
      "Epoch  26 Batch 2236/2739 train_loss = 2.239\n",
      "Epoch  26 Batch 2286/2739 train_loss = 2.380\n",
      "Epoch  26 Batch 2336/2739 train_loss = 4.036\n",
      "Epoch  26 Batch 2386/2739 train_loss = 2.282\n",
      "Epoch  26 Batch 2436/2739 train_loss = 2.356\n",
      "Epoch  26 Batch 2486/2739 train_loss = 2.459\n",
      "Epoch  26 Batch 2536/2739 train_loss = 2.532\n",
      "Epoch  26 Batch 2586/2739 train_loss = 4.026\n",
      "Epoch  26 Batch 2636/2739 train_loss = 2.829\n",
      "Epoch  26 Batch 2686/2739 train_loss = 2.260\n",
      "Epoch  26 Batch 2736/2739 train_loss = 2.103\n",
      "Epoch  27 Batch   47/2739 train_loss = 1.029\n",
      "Epoch  27 Batch   97/2739 train_loss = 2.051\n",
      "Epoch  27 Batch  147/2739 train_loss = 3.022\n",
      "Epoch  27 Batch  197/2739 train_loss = 2.603\n",
      "Epoch  27 Batch  247/2739 train_loss = 3.045\n",
      "Epoch  27 Batch  297/2739 train_loss = 1.386\n",
      "Epoch  27 Batch  347/2739 train_loss = 1.738\n",
      "Epoch  27 Batch  397/2739 train_loss = 1.341\n",
      "Epoch  27 Batch  447/2739 train_loss = 1.981\n",
      "Epoch  27 Batch  497/2739 train_loss = 1.335\n",
      "Epoch  27 Batch  547/2739 train_loss = 1.903\n",
      "Epoch  27 Batch  597/2739 train_loss = 2.579\n",
      "Epoch  27 Batch  647/2739 train_loss = 2.441\n",
      "Epoch  27 Batch  697/2739 train_loss = 1.580\n",
      "Epoch  27 Batch  747/2739 train_loss = 2.471\n",
      "Epoch  27 Batch  797/2739 train_loss = 2.246\n",
      "Epoch  27 Batch  847/2739 train_loss = 3.771\n",
      "Epoch  27 Batch  897/2739 train_loss = 3.293\n",
      "Epoch  27 Batch  947/2739 train_loss = 2.117\n",
      "Epoch  27 Batch  997/2739 train_loss = 2.657\n",
      "Epoch  27 Batch 1047/2739 train_loss = 2.492\n",
      "Epoch  27 Batch 1097/2739 train_loss = 1.557\n",
      "Epoch  27 Batch 1147/2739 train_loss = 2.351\n",
      "Epoch  27 Batch 1197/2739 train_loss = 2.414\n",
      "Epoch  27 Batch 1247/2739 train_loss = 2.263\n",
      "Epoch  27 Batch 1297/2739 train_loss = 1.557\n",
      "Epoch  27 Batch 1347/2739 train_loss = 2.876\n",
      "Epoch  27 Batch 1397/2739 train_loss = 4.269\n",
      "Epoch  27 Batch 1447/2739 train_loss = 2.740\n",
      "Epoch  27 Batch 1497/2739 train_loss = 1.740\n",
      "Epoch  27 Batch 1547/2739 train_loss = 2.801\n",
      "Epoch  27 Batch 1597/2739 train_loss = 1.361\n",
      "Epoch  27 Batch 1647/2739 train_loss = 3.952\n",
      "Epoch  27 Batch 1697/2739 train_loss = 3.419\n",
      "Epoch  27 Batch 1747/2739 train_loss = 3.920\n",
      "Epoch  27 Batch 1797/2739 train_loss = 1.783\n",
      "Epoch  27 Batch 1847/2739 train_loss = 2.273\n",
      "Epoch  27 Batch 1897/2739 train_loss = 1.527\n",
      "Epoch  27 Batch 1947/2739 train_loss = 2.696\n",
      "Epoch  27 Batch 1997/2739 train_loss = 2.433\n",
      "Epoch  27 Batch 2047/2739 train_loss = 3.726\n",
      "Epoch  27 Batch 2097/2739 train_loss = 3.109\n",
      "Epoch  27 Batch 2147/2739 train_loss = 2.244\n",
      "Epoch  27 Batch 2197/2739 train_loss = 2.368\n",
      "Epoch  27 Batch 2247/2739 train_loss = 3.873\n",
      "Epoch  27 Batch 2297/2739 train_loss = 3.924\n",
      "Epoch  27 Batch 2347/2739 train_loss = 3.919\n",
      "Epoch  27 Batch 2397/2739 train_loss = 1.649\n",
      "Epoch  27 Batch 2447/2739 train_loss = 2.813\n",
      "Epoch  27 Batch 2497/2739 train_loss = 3.003\n",
      "Epoch  27 Batch 2547/2739 train_loss = 1.872\n",
      "Epoch  27 Batch 2597/2739 train_loss = 4.336\n",
      "Epoch  27 Batch 2647/2739 train_loss = 2.103\n",
      "Epoch  27 Batch 2697/2739 train_loss = 1.697\n",
      "Epoch  28 Batch    8/2739 train_loss = 2.653\n",
      "Epoch  28 Batch   58/2739 train_loss = 1.459\n",
      "Epoch  28 Batch  108/2739 train_loss = 2.661\n",
      "Epoch  28 Batch  158/2739 train_loss = 2.740\n",
      "Epoch  28 Batch  208/2739 train_loss = 2.058\n",
      "Epoch  28 Batch  258/2739 train_loss = 3.269\n",
      "Epoch  28 Batch  308/2739 train_loss = 2.567\n",
      "Epoch  28 Batch  358/2739 train_loss = 2.025\n",
      "Epoch  28 Batch  408/2739 train_loss = 2.163\n",
      "Epoch  28 Batch  458/2739 train_loss = 3.133\n",
      "Epoch  28 Batch  508/2739 train_loss = 2.298\n",
      "Epoch  28 Batch  558/2739 train_loss = 1.882\n",
      "Epoch  28 Batch  608/2739 train_loss = 2.281\n",
      "Epoch  28 Batch  658/2739 train_loss = 2.707\n",
      "Epoch  28 Batch  708/2739 train_loss = 1.472\n",
      "Epoch  28 Batch  758/2739 train_loss = 3.275\n",
      "Epoch  28 Batch  808/2739 train_loss = 2.359\n",
      "Epoch  28 Batch  858/2739 train_loss = 1.818\n",
      "Epoch  28 Batch  908/2739 train_loss = 2.617\n",
      "Epoch  28 Batch  958/2739 train_loss = 3.489\n",
      "Epoch  28 Batch 1008/2739 train_loss = 1.459\n",
      "Epoch  28 Batch 1058/2739 train_loss = 3.633\n",
      "Epoch  28 Batch 1108/2739 train_loss = 2.253\n",
      "Epoch  28 Batch 1158/2739 train_loss = 2.562\n",
      "Epoch  28 Batch 1208/2739 train_loss = 1.551\n",
      "Epoch  28 Batch 1258/2739 train_loss = 2.444\n",
      "Epoch  28 Batch 1308/2739 train_loss = 2.248\n",
      "Epoch  28 Batch 1358/2739 train_loss = 1.310\n",
      "Epoch  28 Batch 1408/2739 train_loss = 3.377\n",
      "Epoch  28 Batch 1458/2739 train_loss = 2.084\n",
      "Epoch  28 Batch 1508/2739 train_loss = 2.260\n",
      "Epoch  28 Batch 1558/2739 train_loss = 4.414\n",
      "Epoch  28 Batch 1608/2739 train_loss = 1.454\n",
      "Epoch  28 Batch 1658/2739 train_loss = 2.807\n",
      "Epoch  28 Batch 1708/2739 train_loss = 2.112\n",
      "Epoch  28 Batch 1758/2739 train_loss = 3.188\n",
      "Epoch  28 Batch 1808/2739 train_loss = 2.838\n",
      "Epoch  28 Batch 1858/2739 train_loss = 3.404\n",
      "Epoch  28 Batch 1908/2739 train_loss = 3.406\n",
      "Epoch  28 Batch 1958/2739 train_loss = 1.801\n",
      "Epoch  28 Batch 2008/2739 train_loss = 2.638\n",
      "Epoch  28 Batch 2058/2739 train_loss = 3.863\n",
      "Epoch  28 Batch 2108/2739 train_loss = 1.064\n",
      "Epoch  28 Batch 2158/2739 train_loss = 1.821\n",
      "Epoch  28 Batch 2208/2739 train_loss = 3.687\n",
      "Epoch  28 Batch 2258/2739 train_loss = 4.289\n",
      "Epoch  28 Batch 2308/2739 train_loss = 4.174\n",
      "Epoch  28 Batch 2358/2739 train_loss = 1.253\n",
      "Epoch  28 Batch 2408/2739 train_loss = 3.439\n",
      "Epoch  28 Batch 2458/2739 train_loss = 2.690\n",
      "Epoch  28 Batch 2508/2739 train_loss = 1.861\n",
      "Epoch  28 Batch 2558/2739 train_loss = 2.873\n",
      "Epoch  28 Batch 2608/2739 train_loss = 4.199\n",
      "Epoch  28 Batch 2658/2739 train_loss = 1.326\n",
      "Epoch  28 Batch 2708/2739 train_loss = 3.124\n",
      "Epoch  29 Batch   19/2739 train_loss = 1.549\n",
      "Epoch  29 Batch   69/2739 train_loss = 2.610\n",
      "Epoch  29 Batch  119/2739 train_loss = 1.115\n",
      "Epoch  29 Batch  169/2739 train_loss = 2.792\n",
      "Epoch  29 Batch  219/2739 train_loss = 1.195\n",
      "Epoch  29 Batch  269/2739 train_loss = 2.561\n",
      "Epoch  29 Batch  319/2739 train_loss = 2.766\n",
      "Epoch  29 Batch  369/2739 train_loss = 1.851\n",
      "Epoch  29 Batch  419/2739 train_loss = 2.684\n",
      "Epoch  29 Batch  469/2739 train_loss = 2.895\n",
      "Epoch  29 Batch  519/2739 train_loss = 1.452\n",
      "Epoch  29 Batch  569/2739 train_loss = 2.408\n",
      "Epoch  29 Batch  619/2739 train_loss = 1.448\n",
      "Epoch  29 Batch  669/2739 train_loss = 1.581\n",
      "Epoch  29 Batch  719/2739 train_loss = 2.392\n",
      "Epoch  29 Batch  769/2739 train_loss = 3.603\n",
      "Epoch  29 Batch  819/2739 train_loss = 2.533\n",
      "Epoch  29 Batch  869/2739 train_loss = 2.990\n",
      "Epoch  29 Batch  919/2739 train_loss = 2.213\n",
      "Epoch  29 Batch  969/2739 train_loss = 3.752\n",
      "Epoch  29 Batch 1019/2739 train_loss = 1.993\n",
      "Epoch  29 Batch 1069/2739 train_loss = 2.396\n",
      "Epoch  29 Batch 1119/2739 train_loss = 2.501\n",
      "Epoch  29 Batch 1169/2739 train_loss = 2.627\n",
      "Epoch  29 Batch 1219/2739 train_loss = 1.529\n",
      "Epoch  29 Batch 1269/2739 train_loss = 3.291\n",
      "Epoch  29 Batch 1319/2739 train_loss = 1.698\n",
      "Epoch  29 Batch 1369/2739 train_loss = 2.644\n",
      "Epoch  29 Batch 1419/2739 train_loss = 3.599\n",
      "Epoch  29 Batch 1469/2739 train_loss = 1.934\n",
      "Epoch  29 Batch 1519/2739 train_loss = 2.062\n",
      "Epoch  29 Batch 1569/2739 train_loss = 2.144\n",
      "Epoch  29 Batch 1619/2739 train_loss = 2.464\n",
      "Epoch  29 Batch 1669/2739 train_loss = 1.431\n",
      "Epoch  29 Batch 1719/2739 train_loss = 2.298\n",
      "Epoch  29 Batch 1769/2739 train_loss = 3.783\n",
      "Epoch  29 Batch 1819/2739 train_loss = 1.542\n",
      "Epoch  29 Batch 1869/2739 train_loss = 3.751\n",
      "Epoch  29 Batch 1919/2739 train_loss = 1.929\n",
      "Epoch  29 Batch 1969/2739 train_loss = 4.086\n",
      "Epoch  29 Batch 2019/2739 train_loss = 3.686\n",
      "Epoch  29 Batch 2069/2739 train_loss = 2.155\n",
      "Epoch  29 Batch 2119/2739 train_loss = 2.545\n",
      "Epoch  29 Batch 2169/2739 train_loss = 1.610\n",
      "Epoch  29 Batch 2219/2739 train_loss = 3.188\n",
      "Epoch  29 Batch 2269/2739 train_loss = 3.023\n",
      "Epoch  29 Batch 2319/2739 train_loss = 4.260\n",
      "Epoch  29 Batch 2369/2739 train_loss = 3.349\n",
      "Epoch  29 Batch 2419/2739 train_loss = 1.880\n",
      "Epoch  29 Batch 2469/2739 train_loss = 1.419\n",
      "Epoch  29 Batch 2519/2739 train_loss = 2.163\n",
      "Epoch  29 Batch 2569/2739 train_loss = 3.922\n",
      "Epoch  29 Batch 2619/2739 train_loss = 2.461\n",
      "Epoch  29 Batch 2669/2739 train_loss = 2.487\n",
      "Epoch  29 Batch 2719/2739 train_loss = 1.863\n",
      "Epoch  30 Batch   30/2739 train_loss = 3.353\n",
      "Epoch  30 Batch   80/2739 train_loss = 1.320\n",
      "Epoch  30 Batch  130/2739 train_loss = 2.265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  30 Batch  180/2739 train_loss = 3.227\n",
      "Epoch  30 Batch  230/2739 train_loss = 1.957\n",
      "Epoch  30 Batch  280/2739 train_loss = 1.782\n",
      "Epoch  30 Batch  330/2739 train_loss = 2.431\n",
      "Epoch  30 Batch  380/2739 train_loss = 2.262\n",
      "Epoch  30 Batch  430/2739 train_loss = 1.780\n",
      "Epoch  30 Batch  480/2739 train_loss = 3.413\n",
      "Epoch  30 Batch  530/2739 train_loss = 2.424\n",
      "Epoch  30 Batch  580/2739 train_loss = 3.702\n",
      "Epoch  30 Batch  630/2739 train_loss = 2.242\n",
      "Epoch  30 Batch  680/2739 train_loss = 3.818\n",
      "Epoch  30 Batch  730/2739 train_loss = 3.701\n",
      "Epoch  30 Batch  780/2739 train_loss = 2.936\n",
      "Epoch  30 Batch  830/2739 train_loss = 1.923\n",
      "Epoch  30 Batch  880/2739 train_loss = 1.618\n",
      "Epoch  30 Batch  930/2739 train_loss = 1.892\n",
      "Epoch  30 Batch  980/2739 train_loss = 2.567\n",
      "Epoch  30 Batch 1030/2739 train_loss = 2.583\n",
      "Epoch  30 Batch 1080/2739 train_loss = 2.954\n",
      "Epoch  30 Batch 1130/2739 train_loss = 3.826\n",
      "Epoch  30 Batch 1180/2739 train_loss = 3.934\n",
      "Epoch  30 Batch 1230/2739 train_loss = 3.495\n",
      "Epoch  30 Batch 1280/2739 train_loss = 1.890\n",
      "Epoch  30 Batch 1330/2739 train_loss = 4.166\n",
      "Epoch  30 Batch 1380/2739 train_loss = 2.360\n",
      "Epoch  30 Batch 1430/2739 train_loss = 2.504\n",
      "Epoch  30 Batch 1480/2739 train_loss = 2.742\n",
      "Epoch  30 Batch 1530/2739 train_loss = 2.598\n",
      "Epoch  30 Batch 1580/2739 train_loss = 2.047\n",
      "Epoch  30 Batch 1630/2739 train_loss = 2.108\n",
      "Epoch  30 Batch 1680/2739 train_loss = 4.038\n",
      "Epoch  30 Batch 1730/2739 train_loss = 2.250\n",
      "Epoch  30 Batch 1780/2739 train_loss = 1.883\n",
      "Epoch  30 Batch 1830/2739 train_loss = 2.186\n",
      "Epoch  30 Batch 1880/2739 train_loss = 2.394\n",
      "Epoch  30 Batch 1930/2739 train_loss = 4.636\n",
      "Epoch  30 Batch 1980/2739 train_loss = 2.164\n",
      "Epoch  30 Batch 2030/2739 train_loss = 3.679\n",
      "Epoch  30 Batch 2080/2739 train_loss = 3.114\n",
      "Epoch  30 Batch 2130/2739 train_loss = 3.965\n",
      "Epoch  30 Batch 2180/2739 train_loss = 2.218\n",
      "Epoch  30 Batch 2230/2739 train_loss = 1.007\n",
      "Epoch  30 Batch 2280/2739 train_loss = 1.747\n",
      "Epoch  30 Batch 2330/2739 train_loss = 1.200\n",
      "Epoch  30 Batch 2380/2739 train_loss = 2.590\n",
      "Epoch  30 Batch 2430/2739 train_loss = 1.647\n",
      "Epoch  30 Batch 2480/2739 train_loss = 2.538\n",
      "Epoch  30 Batch 2530/2739 train_loss = 1.831\n",
      "Epoch  30 Batch 2580/2739 train_loss = 2.469\n",
      "Epoch  30 Batch 2630/2739 train_loss = 1.665\n",
      "Epoch  30 Batch 2680/2739 train_loss = 1.836\n",
      "Epoch  30 Batch 2730/2739 train_loss = 3.661\n",
      "Epoch  31 Batch   41/2739 train_loss = 4.062\n",
      "Epoch  31 Batch   91/2739 train_loss = 1.192\n",
      "Epoch  31 Batch  141/2739 train_loss = 2.556\n",
      "Epoch  31 Batch  191/2739 train_loss = 2.681\n",
      "Epoch  31 Batch  241/2739 train_loss = 2.540\n",
      "Epoch  31 Batch  291/2739 train_loss = 2.917\n",
      "Epoch  31 Batch  341/2739 train_loss = 4.028\n",
      "Epoch  31 Batch  391/2739 train_loss = 2.374\n",
      "Epoch  31 Batch  441/2739 train_loss = 2.313\n",
      "Epoch  31 Batch  491/2739 train_loss = 3.229\n",
      "Epoch  31 Batch  541/2739 train_loss = 2.044\n",
      "Epoch  31 Batch  591/2739 train_loss = 2.263\n",
      "Epoch  31 Batch  641/2739 train_loss = 3.583\n",
      "Epoch  31 Batch  691/2739 train_loss = 1.951\n",
      "Epoch  31 Batch  741/2739 train_loss = 1.074\n",
      "Epoch  31 Batch  791/2739 train_loss = 3.905\n",
      "Epoch  31 Batch  841/2739 train_loss = 1.699\n",
      "Epoch  31 Batch  891/2739 train_loss = 1.793\n",
      "Epoch  31 Batch  941/2739 train_loss = 2.681\n",
      "Epoch  31 Batch  991/2739 train_loss = 3.388\n",
      "Epoch  31 Batch 1041/2739 train_loss = 2.224\n",
      "Epoch  31 Batch 1091/2739 train_loss = 2.225\n",
      "Epoch  31 Batch 1141/2739 train_loss = 2.474\n",
      "Epoch  31 Batch 1191/2739 train_loss = 2.267\n",
      "Epoch  31 Batch 1241/2739 train_loss = 2.222\n",
      "Epoch  31 Batch 1291/2739 train_loss = 1.776\n",
      "Epoch  31 Batch 1341/2739 train_loss = 4.210\n",
      "Epoch  31 Batch 1391/2739 train_loss = 3.737\n",
      "Epoch  31 Batch 1441/2739 train_loss = 1.735\n",
      "Epoch  31 Batch 1491/2739 train_loss = 1.476\n",
      "Epoch  31 Batch 1541/2739 train_loss = 2.577\n",
      "Epoch  31 Batch 1591/2739 train_loss = 2.855\n",
      "Epoch  31 Batch 1641/2739 train_loss = 1.971\n",
      "Epoch  31 Batch 1691/2739 train_loss = 3.514\n",
      "Epoch  31 Batch 1741/2739 train_loss = 2.489\n",
      "Epoch  31 Batch 1791/2739 train_loss = 2.812\n",
      "Epoch  31 Batch 1841/2739 train_loss = 2.904\n",
      "Epoch  31 Batch 1891/2739 train_loss = 2.214\n",
      "Epoch  31 Batch 1941/2739 train_loss = 1.816\n",
      "Epoch  31 Batch 1991/2739 train_loss = 1.399\n",
      "Epoch  31 Batch 2041/2739 train_loss = 3.860\n",
      "Epoch  31 Batch 2091/2739 train_loss = 3.318\n",
      "Epoch  31 Batch 2141/2739 train_loss = 3.568\n",
      "Epoch  31 Batch 2191/2739 train_loss = 3.205\n",
      "Epoch  31 Batch 2241/2739 train_loss = 2.440\n",
      "Epoch  31 Batch 2291/2739 train_loss = 1.446\n",
      "Epoch  31 Batch 2341/2739 train_loss = 3.956\n",
      "Epoch  31 Batch 2391/2739 train_loss = 2.246\n",
      "Epoch  31 Batch 2441/2739 train_loss = 2.796\n",
      "Epoch  31 Batch 2491/2739 train_loss = 2.672\n",
      "Epoch  31 Batch 2541/2739 train_loss = 2.195\n",
      "Epoch  31 Batch 2591/2739 train_loss = 3.774\n",
      "Epoch  31 Batch 2641/2739 train_loss = 2.052\n",
      "Epoch  31 Batch 2691/2739 train_loss = 1.729\n",
      "Epoch  32 Batch    2/2739 train_loss = 2.599\n",
      "Epoch  32 Batch   52/2739 train_loss = 1.968\n",
      "Epoch  32 Batch  102/2739 train_loss = 2.052\n",
      "Epoch  32 Batch  152/2739 train_loss = 2.787\n",
      "Epoch  32 Batch  202/2739 train_loss = 2.613\n",
      "Epoch  32 Batch  252/2739 train_loss = 2.053\n",
      "Epoch  32 Batch  302/2739 train_loss = 2.584\n",
      "Epoch  32 Batch  352/2739 train_loss = 2.069\n",
      "Epoch  32 Batch  402/2739 train_loss = 1.959\n",
      "Epoch  32 Batch  452/2739 train_loss = 2.342\n",
      "Epoch  32 Batch  502/2739 train_loss = 2.424\n",
      "Epoch  32 Batch  552/2739 train_loss = 1.773\n",
      "Epoch  32 Batch  602/2739 train_loss = 1.473\n",
      "Epoch  32 Batch  652/2739 train_loss = 2.590\n",
      "Epoch  32 Batch  702/2739 train_loss = 3.025\n",
      "Epoch  32 Batch  752/2739 train_loss = 1.871\n",
      "Epoch  32 Batch  802/2739 train_loss = 2.596\n",
      "Epoch  32 Batch  852/2739 train_loss = 2.179\n",
      "Epoch  32 Batch  902/2739 train_loss = 3.841\n",
      "Epoch  32 Batch  952/2739 train_loss = 1.335\n",
      "Epoch  32 Batch 1002/2739 train_loss = 2.528\n",
      "Epoch  32 Batch 1052/2739 train_loss = 2.423\n",
      "Epoch  32 Batch 1102/2739 train_loss = 2.143\n",
      "Epoch  32 Batch 1152/2739 train_loss = 2.385\n",
      "Epoch  32 Batch 1202/2739 train_loss = 1.899\n",
      "Epoch  32 Batch 1252/2739 train_loss = 2.839\n",
      "Epoch  32 Batch 1302/2739 train_loss = 2.369\n",
      "Epoch  32 Batch 1352/2739 train_loss = 2.645\n",
      "Epoch  32 Batch 1402/2739 train_loss = 3.643\n",
      "Epoch  32 Batch 1452/2739 train_loss = 1.518\n",
      "Epoch  32 Batch 1502/2739 train_loss = 1.316\n",
      "Epoch  32 Batch 1552/2739 train_loss = 1.537\n",
      "Epoch  32 Batch 1602/2739 train_loss = 3.936\n",
      "Epoch  32 Batch 1652/2739 train_loss = 3.627\n",
      "Epoch  32 Batch 1702/2739 train_loss = 2.177\n",
      "Epoch  32 Batch 1752/2739 train_loss = 2.131\n",
      "Epoch  32 Batch 1802/2739 train_loss = 4.125\n",
      "Epoch  32 Batch 1852/2739 train_loss = 3.153\n",
      "Epoch  32 Batch 1902/2739 train_loss = 2.627\n",
      "Epoch  32 Batch 1952/2739 train_loss = 1.368\n",
      "Epoch  32 Batch 2002/2739 train_loss = 2.116\n",
      "Epoch  32 Batch 2052/2739 train_loss = 4.048\n",
      "Epoch  32 Batch 2102/2739 train_loss = 2.131\n",
      "Epoch  32 Batch 2152/2739 train_loss = 4.098\n",
      "Epoch  32 Batch 2202/2739 train_loss = 3.314\n",
      "Epoch  32 Batch 2252/2739 train_loss = 3.636\n",
      "Epoch  32 Batch 2302/2739 train_loss = 3.190\n",
      "Epoch  32 Batch 2352/2739 train_loss = 2.457\n",
      "Epoch  32 Batch 2402/2739 train_loss = 1.997\n",
      "Epoch  32 Batch 2452/2739 train_loss = 1.490\n",
      "Epoch  32 Batch 2502/2739 train_loss = 2.295\n",
      "Epoch  32 Batch 2552/2739 train_loss = 4.031\n",
      "Epoch  32 Batch 2602/2739 train_loss = 2.374\n",
      "Epoch  32 Batch 2652/2739 train_loss = 1.199\n",
      "Epoch  32 Batch 2702/2739 train_loss = 1.806\n",
      "Epoch  33 Batch   13/2739 train_loss = 2.320\n",
      "Epoch  33 Batch   63/2739 train_loss = 1.276\n",
      "Epoch  33 Batch  113/2739 train_loss = 2.934\n",
      "Epoch  33 Batch  163/2739 train_loss = 2.064\n",
      "Epoch  33 Batch  213/2739 train_loss = 2.712\n",
      "Epoch  33 Batch  263/2739 train_loss = 2.129\n",
      "Epoch  33 Batch  313/2739 train_loss = 2.246\n",
      "Epoch  33 Batch  363/2739 train_loss = 2.661\n",
      "Epoch  33 Batch  413/2739 train_loss = 2.876\n",
      "Epoch  33 Batch  463/2739 train_loss = 2.548\n",
      "Epoch  33 Batch  513/2739 train_loss = 2.509\n",
      "Epoch  33 Batch  563/2739 train_loss = 1.443\n",
      "Epoch  33 Batch  613/2739 train_loss = 2.669\n",
      "Epoch  33 Batch  663/2739 train_loss = 2.266\n",
      "Epoch  33 Batch  713/2739 train_loss = 1.400\n",
      "Epoch  33 Batch  763/2739 train_loss = 2.337\n",
      "Epoch  33 Batch  813/2739 train_loss = 1.945\n",
      "Epoch  33 Batch  863/2739 train_loss = 3.662\n",
      "Epoch  33 Batch  913/2739 train_loss = 2.159\n",
      "Epoch  33 Batch  963/2739 train_loss = 3.987\n",
      "Epoch  33 Batch 1013/2739 train_loss = 2.104\n",
      "Epoch  33 Batch 1063/2739 train_loss = 3.147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  33 Batch 1113/2739 train_loss = 2.315\n",
      "Epoch  33 Batch 1163/2739 train_loss = 3.751\n",
      "Epoch  33 Batch 1213/2739 train_loss = 3.686\n",
      "Epoch  33 Batch 1263/2739 train_loss = 1.963\n",
      "Epoch  33 Batch 1313/2739 train_loss = 1.984\n",
      "Epoch  33 Batch 1363/2739 train_loss = 1.524\n",
      "Epoch  33 Batch 1413/2739 train_loss = 2.303\n",
      "Epoch  33 Batch 1463/2739 train_loss = 2.010\n",
      "Epoch  33 Batch 1513/2739 train_loss = 2.268\n",
      "Epoch  33 Batch 1563/2739 train_loss = 2.365\n",
      "Epoch  33 Batch 1613/2739 train_loss = 1.851\n",
      "Epoch  33 Batch 1663/2739 train_loss = 1.984\n",
      "Epoch  33 Batch 1713/2739 train_loss = 1.514\n",
      "Epoch  33 Batch 1763/2739 train_loss = 1.352\n",
      "Epoch  33 Batch 1813/2739 train_loss = 2.031\n",
      "Epoch  33 Batch 1863/2739 train_loss = 2.786\n",
      "Epoch  33 Batch 1913/2739 train_loss = 3.770\n",
      "Epoch  33 Batch 1963/2739 train_loss = 1.929\n",
      "Epoch  33 Batch 2013/2739 train_loss = 2.757\n",
      "Epoch  33 Batch 2063/2739 train_loss = 2.375\n",
      "Epoch  33 Batch 2113/2739 train_loss = 3.868\n",
      "Epoch  33 Batch 2163/2739 train_loss = 1.627\n",
      "Epoch  33 Batch 2213/2739 train_loss = 1.975\n",
      "Epoch  33 Batch 2263/2739 train_loss = 2.371\n",
      "Epoch  33 Batch 2313/2739 train_loss = 1.350\n",
      "Epoch  33 Batch 2363/2739 train_loss = 1.482\n",
      "Epoch  33 Batch 2413/2739 train_loss = 1.643\n",
      "Epoch  33 Batch 2463/2739 train_loss = 2.278\n",
      "Epoch  33 Batch 2513/2739 train_loss = 2.932\n",
      "Epoch  33 Batch 2563/2739 train_loss = 1.961\n",
      "Epoch  33 Batch 2613/2739 train_loss = 3.911\n",
      "Epoch  33 Batch 2663/2739 train_loss = 1.492\n",
      "Epoch  33 Batch 2713/2739 train_loss = 2.281\n",
      "Epoch  34 Batch   24/2739 train_loss = 1.675\n",
      "Epoch  34 Batch   74/2739 train_loss = 3.076\n",
      "Epoch  34 Batch  124/2739 train_loss = 2.287\n",
      "Epoch  34 Batch  174/2739 train_loss = 2.641\n",
      "Epoch  34 Batch  224/2739 train_loss = 2.841\n",
      "Epoch  34 Batch  274/2739 train_loss = 2.256\n",
      "Epoch  34 Batch  324/2739 train_loss = 3.322\n",
      "Epoch  34 Batch  374/2739 train_loss = 1.851\n",
      "Epoch  34 Batch  424/2739 train_loss = 2.802\n",
      "Epoch  34 Batch  474/2739 train_loss = 2.006\n",
      "Epoch  34 Batch  524/2739 train_loss = 1.401\n",
      "Epoch  34 Batch  574/2739 train_loss = 1.758\n",
      "Epoch  34 Batch  624/2739 train_loss = 2.346\n",
      "Epoch  34 Batch  674/2739 train_loss = 2.450\n",
      "Epoch  34 Batch  724/2739 train_loss = 3.158\n",
      "Epoch  34 Batch  774/2739 train_loss = 1.017\n",
      "Epoch  34 Batch  824/2739 train_loss = 1.731\n",
      "Epoch  34 Batch  874/2739 train_loss = 1.633\n",
      "Epoch  34 Batch  924/2739 train_loss = 2.141\n",
      "Epoch  34 Batch  974/2739 train_loss = 4.082\n",
      "Epoch  34 Batch 1024/2739 train_loss = 1.594\n",
      "Epoch  34 Batch 1074/2739 train_loss = 3.601\n",
      "Epoch  34 Batch 1124/2739 train_loss = 3.655\n",
      "Epoch  34 Batch 1174/2739 train_loss = 3.279\n",
      "Epoch  34 Batch 1224/2739 train_loss = 2.022\n",
      "Epoch  34 Batch 1274/2739 train_loss = 2.797\n",
      "Epoch  34 Batch 1324/2739 train_loss = 2.291\n",
      "Epoch  34 Batch 1374/2739 train_loss = 2.427\n",
      "Epoch  34 Batch 1424/2739 train_loss = 2.753\n",
      "Epoch  34 Batch 1474/2739 train_loss = 2.256\n",
      "Epoch  34 Batch 1524/2739 train_loss = 3.712\n",
      "Epoch  34 Batch 1574/2739 train_loss = 3.463\n",
      "Epoch  34 Batch 1624/2739 train_loss = 3.555\n",
      "Epoch  34 Batch 1674/2739 train_loss = 4.068\n",
      "Epoch  34 Batch 1724/2739 train_loss = 2.440\n",
      "Epoch  34 Batch 1774/2739 train_loss = 2.683\n",
      "Epoch  34 Batch 1824/2739 train_loss = 4.117\n",
      "Epoch  34 Batch 1874/2739 train_loss = 1.736\n",
      "Epoch  34 Batch 1924/2739 train_loss = 2.026\n",
      "Epoch  34 Batch 1974/2739 train_loss = 2.242\n",
      "Epoch  34 Batch 2024/2739 train_loss = 1.508\n",
      "Epoch  34 Batch 2074/2739 train_loss = 2.764\n",
      "Epoch  34 Batch 2124/2739 train_loss = 1.838\n",
      "Epoch  34 Batch 2174/2739 train_loss = 2.079\n",
      "Epoch  34 Batch 2224/2739 train_loss = 1.384\n",
      "Epoch  34 Batch 2274/2739 train_loss = 2.376\n",
      "Epoch  34 Batch 2324/2739 train_loss = 1.367\n",
      "Epoch  34 Batch 2374/2739 train_loss = 2.532\n",
      "Epoch  34 Batch 2424/2739 train_loss = 1.977\n",
      "Epoch  34 Batch 2474/2739 train_loss = 1.173\n",
      "Epoch  34 Batch 2524/2739 train_loss = 2.377\n",
      "Epoch  34 Batch 2574/2739 train_loss = 2.165\n",
      "Epoch  34 Batch 2624/2739 train_loss = 3.553\n",
      "Epoch  34 Batch 2674/2739 train_loss = 2.080\n",
      "Epoch  34 Batch 2724/2739 train_loss = 3.739\n",
      "Epoch  35 Batch   35/2739 train_loss = 2.453\n",
      "Epoch  35 Batch   85/2739 train_loss = 3.116\n",
      "Epoch  35 Batch  135/2739 train_loss = 1.752\n",
      "Epoch  35 Batch  185/2739 train_loss = 2.975\n",
      "Epoch  35 Batch  235/2739 train_loss = 2.356\n",
      "Epoch  35 Batch  285/2739 train_loss = 1.891\n",
      "Epoch  35 Batch  335/2739 train_loss = 2.092\n",
      "Epoch  35 Batch  385/2739 train_loss = 2.224\n",
      "Epoch  35 Batch  435/2739 train_loss = 3.683\n",
      "Epoch  35 Batch  485/2739 train_loss = 3.438\n",
      "Epoch  35 Batch  535/2739 train_loss = 2.140\n",
      "Epoch  35 Batch  585/2739 train_loss = 2.408\n",
      "Epoch  35 Batch  635/2739 train_loss = 1.630\n",
      "Epoch  35 Batch  685/2739 train_loss = 2.263\n",
      "Epoch  35 Batch  735/2739 train_loss = 2.665\n",
      "Epoch  35 Batch  785/2739 train_loss = 3.334\n",
      "Epoch  35 Batch  835/2739 train_loss = 1.924\n",
      "Epoch  35 Batch  885/2739 train_loss = 2.316\n",
      "Epoch  35 Batch  935/2739 train_loss = 2.971\n",
      "Epoch  35 Batch  985/2739 train_loss = 2.081\n",
      "Epoch  35 Batch 1035/2739 train_loss = 3.213\n",
      "Epoch  35 Batch 1085/2739 train_loss = 1.272\n",
      "Epoch  35 Batch 1135/2739 train_loss = 2.682\n",
      "Epoch  35 Batch 1185/2739 train_loss = 2.644\n",
      "Epoch  35 Batch 1235/2739 train_loss = 2.188\n",
      "Epoch  35 Batch 1285/2739 train_loss = 1.800\n",
      "Epoch  35 Batch 1335/2739 train_loss = 3.767\n",
      "Epoch  35 Batch 1385/2739 train_loss = 1.805\n",
      "Epoch  35 Batch 1435/2739 train_loss = 2.763\n",
      "Epoch  35 Batch 1485/2739 train_loss = 1.765\n",
      "Epoch  35 Batch 1535/2739 train_loss = 1.921\n",
      "Epoch  35 Batch 1585/2739 train_loss = 1.135\n",
      "Epoch  35 Batch 1635/2739 train_loss = 2.053\n",
      "Epoch  35 Batch 1685/2739 train_loss = 2.256\n",
      "Epoch  35 Batch 1735/2739 train_loss = 2.747\n",
      "Epoch  35 Batch 1785/2739 train_loss = 2.327\n",
      "Epoch  35 Batch 1835/2739 train_loss = 3.972\n",
      "Epoch  35 Batch 1885/2739 train_loss = 3.530\n",
      "Epoch  35 Batch 1935/2739 train_loss = 2.526\n",
      "Epoch  35 Batch 1985/2739 train_loss = 2.111\n",
      "Epoch  35 Batch 2035/2739 train_loss = 2.024\n",
      "Epoch  35 Batch 2085/2739 train_loss = 3.916\n",
      "Epoch  35 Batch 2135/2739 train_loss = 4.269\n",
      "Epoch  35 Batch 2185/2739 train_loss = 2.472\n",
      "Epoch  35 Batch 2235/2739 train_loss = 3.970\n",
      "Epoch  35 Batch 2285/2739 train_loss = 3.217\n",
      "Epoch  35 Batch 2335/2739 train_loss = 2.962\n",
      "Epoch  35 Batch 2385/2739 train_loss = 1.371\n",
      "Epoch  35 Batch 2435/2739 train_loss = 2.229\n",
      "Epoch  35 Batch 2485/2739 train_loss = 1.278\n",
      "Epoch  35 Batch 2535/2739 train_loss = 3.003\n",
      "Epoch  35 Batch 2585/2739 train_loss = 3.861\n",
      "Epoch  35 Batch 2635/2739 train_loss = 1.497\n",
      "Epoch  35 Batch 2685/2739 train_loss = 1.508\n",
      "Epoch  35 Batch 2735/2739 train_loss = 2.235\n",
      "Epoch  36 Batch   46/2739 train_loss = 2.536\n",
      "Epoch  36 Batch   96/2739 train_loss = 4.220\n",
      "Epoch  36 Batch  146/2739 train_loss = 1.794\n",
      "Epoch  36 Batch  196/2739 train_loss = 1.431\n",
      "Epoch  36 Batch  246/2739 train_loss = 1.695\n",
      "Epoch  36 Batch  296/2739 train_loss = 2.000\n",
      "Epoch  36 Batch  346/2739 train_loss = 2.209\n",
      "Epoch  36 Batch  396/2739 train_loss = 2.310\n",
      "Epoch  36 Batch  446/2739 train_loss = 2.207\n",
      "Epoch  36 Batch  496/2739 train_loss = 2.463\n",
      "Epoch  36 Batch  546/2739 train_loss = 2.092\n",
      "Epoch  36 Batch  596/2739 train_loss = 2.365\n",
      "Epoch  36 Batch  646/2739 train_loss = 1.548\n",
      "Epoch  36 Batch  696/2739 train_loss = 1.877\n",
      "Epoch  36 Batch  746/2739 train_loss = 3.278\n",
      "Epoch  36 Batch  796/2739 train_loss = 1.975\n",
      "Epoch  36 Batch  846/2739 train_loss = 1.876\n",
      "Epoch  36 Batch  896/2739 train_loss = 1.787\n",
      "Epoch  36 Batch  946/2739 train_loss = 4.483\n",
      "Epoch  36 Batch  996/2739 train_loss = 2.604\n",
      "Epoch  36 Batch 1046/2739 train_loss = 2.200\n",
      "Epoch  36 Batch 1096/2739 train_loss = 2.997\n",
      "Epoch  36 Batch 1146/2739 train_loss = 2.600\n",
      "Epoch  36 Batch 1196/2739 train_loss = 1.214\n",
      "Epoch  36 Batch 1246/2739 train_loss = 2.123\n",
      "Epoch  36 Batch 1296/2739 train_loss = 2.181\n",
      "Epoch  36 Batch 1346/2739 train_loss = 1.595\n",
      "Epoch  36 Batch 1396/2739 train_loss = 3.112\n",
      "Epoch  36 Batch 1446/2739 train_loss = 2.103\n",
      "Epoch  36 Batch 1496/2739 train_loss = 1.398\n",
      "Epoch  36 Batch 1546/2739 train_loss = 2.220\n",
      "Epoch  36 Batch 1596/2739 train_loss = 3.219\n",
      "Epoch  36 Batch 1646/2739 train_loss = 2.146\n",
      "Epoch  36 Batch 1696/2739 train_loss = 1.923\n",
      "Epoch  36 Batch 1746/2739 train_loss = 4.050\n",
      "Epoch  36 Batch 1796/2739 train_loss = 2.426\n",
      "Epoch  36 Batch 1846/2739 train_loss = 1.446\n",
      "Epoch  36 Batch 1896/2739 train_loss = 2.602\n",
      "Epoch  36 Batch 1946/2739 train_loss = 4.489\n",
      "Epoch  36 Batch 1996/2739 train_loss = 1.226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  36 Batch 2046/2739 train_loss = 3.688\n",
      "Epoch  36 Batch 2096/2739 train_loss = 2.299\n",
      "Epoch  36 Batch 2146/2739 train_loss = 2.866\n",
      "Epoch  36 Batch 2196/2739 train_loss = 4.058\n",
      "Epoch  36 Batch 2246/2739 train_loss = 2.020\n",
      "Epoch  36 Batch 2296/2739 train_loss = 3.910\n",
      "Epoch  36 Batch 2346/2739 train_loss = 1.924\n",
      "Epoch  36 Batch 2396/2739 train_loss = 2.851\n",
      "Epoch  36 Batch 2446/2739 train_loss = 2.893\n",
      "Epoch  36 Batch 2496/2739 train_loss = 2.537\n",
      "Epoch  36 Batch 2546/2739 train_loss = 1.368\n",
      "Epoch  36 Batch 2596/2739 train_loss = 4.211\n",
      "Epoch  36 Batch 2646/2739 train_loss = 1.324\n",
      "Epoch  36 Batch 2696/2739 train_loss = 2.472\n",
      "Epoch  37 Batch    7/2739 train_loss = 3.148\n",
      "Epoch  37 Batch   57/2739 train_loss = 2.362\n",
      "Epoch  37 Batch  107/2739 train_loss = 1.974\n",
      "Epoch  37 Batch  157/2739 train_loss = 1.910\n",
      "Epoch  37 Batch  207/2739 train_loss = 2.050\n",
      "Epoch  37 Batch  257/2739 train_loss = 2.375\n",
      "Epoch  37 Batch  307/2739 train_loss = 1.846\n",
      "Epoch  37 Batch  357/2739 train_loss = 1.982\n",
      "Epoch  37 Batch  407/2739 train_loss = 2.775\n",
      "Epoch  37 Batch  457/2739 train_loss = 1.770\n",
      "Epoch  37 Batch  507/2739 train_loss = 1.930\n",
      "Epoch  37 Batch  557/2739 train_loss = 1.379\n",
      "Epoch  37 Batch  607/2739 train_loss = 3.901\n",
      "Epoch  37 Batch  657/2739 train_loss = 2.013\n",
      "Epoch  37 Batch  707/2739 train_loss = 1.788\n",
      "Epoch  37 Batch  757/2739 train_loss = 3.379\n",
      "Epoch  37 Batch  807/2739 train_loss = 2.755\n",
      "Epoch  37 Batch  857/2739 train_loss = 2.574\n",
      "Epoch  37 Batch  907/2739 train_loss = 3.569\n",
      "Epoch  37 Batch  957/2739 train_loss = 4.049\n",
      "Epoch  37 Batch 1007/2739 train_loss = 2.686\n",
      "Epoch  37 Batch 1057/2739 train_loss = 3.655\n",
      "Epoch  37 Batch 1107/2739 train_loss = 1.154\n",
      "Epoch  37 Batch 1157/2739 train_loss = 2.930\n",
      "Epoch  37 Batch 1207/2739 train_loss = 2.304\n",
      "Epoch  37 Batch 1257/2739 train_loss = 1.789\n",
      "Epoch  37 Batch 1307/2739 train_loss = 2.154\n",
      "Epoch  37 Batch 1357/2739 train_loss = 1.937\n",
      "Epoch  37 Batch 1407/2739 train_loss = 3.239\n",
      "Epoch  37 Batch 1457/2739 train_loss = 1.731\n",
      "Epoch  37 Batch 1507/2739 train_loss = 2.484\n",
      "Epoch  37 Batch 1557/2739 train_loss = 2.356\n",
      "Epoch  37 Batch 1607/2739 train_loss = 2.052\n",
      "Epoch  37 Batch 1657/2739 train_loss = 2.799\n",
      "Epoch  37 Batch 1707/2739 train_loss = 1.754\n",
      "Epoch  37 Batch 1757/2739 train_loss = 1.957\n",
      "Epoch  37 Batch 1807/2739 train_loss = 4.165\n",
      "Epoch  37 Batch 1857/2739 train_loss = 1.857\n",
      "Epoch  37 Batch 1907/2739 train_loss = 2.330\n",
      "Epoch  37 Batch 1957/2739 train_loss = 2.151\n",
      "Epoch  37 Batch 2007/2739 train_loss = 3.216\n",
      "Epoch  37 Batch 2057/2739 train_loss = 3.654\n",
      "Epoch  37 Batch 2107/2739 train_loss = 3.146\n",
      "Epoch  37 Batch 2157/2739 train_loss = 2.836\n",
      "Epoch  37 Batch 2207/2739 train_loss = 3.567\n",
      "Epoch  37 Batch 2257/2739 train_loss = 2.075\n",
      "Epoch  37 Batch 2307/2739 train_loss = 4.216\n",
      "Epoch  37 Batch 2357/2739 train_loss = 1.801\n",
      "Epoch  37 Batch 2407/2739 train_loss = 1.178\n",
      "Epoch  37 Batch 2457/2739 train_loss = 2.909\n",
      "Epoch  37 Batch 2507/2739 train_loss = 2.893\n",
      "Epoch  37 Batch 2557/2739 train_loss = 1.410\n",
      "Epoch  37 Batch 2607/2739 train_loss = 4.246\n",
      "Epoch  37 Batch 2657/2739 train_loss = 1.246\n",
      "Epoch  37 Batch 2707/2739 train_loss = 3.217\n",
      "Epoch  38 Batch   18/2739 train_loss = 1.790\n",
      "Epoch  38 Batch   68/2739 train_loss = 1.758\n",
      "Epoch  38 Batch  118/2739 train_loss = 1.481\n",
      "Epoch  38 Batch  168/2739 train_loss = 1.613\n",
      "Epoch  38 Batch  218/2739 train_loss = 2.193\n",
      "Epoch  38 Batch  268/2739 train_loss = 2.419\n",
      "Epoch  38 Batch  318/2739 train_loss = 3.696\n",
      "Epoch  38 Batch  368/2739 train_loss = 2.315\n",
      "Epoch  38 Batch  418/2739 train_loss = 2.376\n",
      "Epoch  38 Batch  468/2739 train_loss = 1.123\n",
      "Epoch  38 Batch  518/2739 train_loss = 2.445\n",
      "Epoch  38 Batch  568/2739 train_loss = 2.850\n",
      "Epoch  38 Batch  618/2739 train_loss = 3.035\n",
      "Epoch  38 Batch  668/2739 train_loss = 2.214\n",
      "Epoch  38 Batch  718/2739 train_loss = 2.319\n",
      "Epoch  38 Batch  768/2739 train_loss = 2.461\n",
      "Epoch  38 Batch  818/2739 train_loss = 2.411\n",
      "Epoch  38 Batch  868/2739 train_loss = 3.592\n",
      "Epoch  38 Batch  918/2739 train_loss = 2.023\n",
      "Epoch  38 Batch  968/2739 train_loss = 3.544\n",
      "Epoch  38 Batch 1018/2739 train_loss = 2.596\n",
      "Epoch  38 Batch 1068/2739 train_loss = 1.758\n",
      "Epoch  38 Batch 1118/2739 train_loss = 2.176\n",
      "Epoch  38 Batch 1168/2739 train_loss = 2.327\n",
      "Epoch  38 Batch 1218/2739 train_loss = 1.584\n",
      "Epoch  38 Batch 1268/2739 train_loss = 3.250\n",
      "Epoch  38 Batch 1318/2739 train_loss = 2.636\n",
      "Epoch  38 Batch 1368/2739 train_loss = 1.944\n",
      "Epoch  38 Batch 1418/2739 train_loss = 3.304\n",
      "Epoch  38 Batch 1468/2739 train_loss = 2.359\n",
      "Epoch  38 Batch 1518/2739 train_loss = 2.432\n",
      "Epoch  38 Batch 1568/2739 train_loss = 1.567\n",
      "Epoch  38 Batch 1618/2739 train_loss = 3.600\n",
      "Epoch  38 Batch 1668/2739 train_loss = 2.392\n",
      "Epoch  38 Batch 1718/2739 train_loss = 1.956\n",
      "Epoch  38 Batch 1768/2739 train_loss = 1.914\n",
      "Epoch  38 Batch 1818/2739 train_loss = 2.098\n",
      "Epoch  38 Batch 1868/2739 train_loss = 3.457\n",
      "Epoch  38 Batch 1918/2739 train_loss = 2.039\n",
      "Epoch  38 Batch 1968/2739 train_loss = 1.475\n",
      "Epoch  38 Batch 2018/2739 train_loss = 2.166\n",
      "Epoch  38 Batch 2068/2739 train_loss = 3.258\n",
      "Epoch  38 Batch 2118/2739 train_loss = 2.235\n",
      "Epoch  38 Batch 2168/2739 train_loss = 1.672\n",
      "Epoch  38 Batch 2218/2739 train_loss = 3.676\n",
      "Epoch  38 Batch 2268/2739 train_loss = 4.329\n",
      "Epoch  38 Batch 2318/2739 train_loss = 4.424\n",
      "Epoch  38 Batch 2368/2739 train_loss = 1.959\n",
      "Epoch  38 Batch 2418/2739 train_loss = 3.174\n",
      "Epoch  38 Batch 2468/2739 train_loss = 1.645\n",
      "Epoch  38 Batch 2518/2739 train_loss = 3.081\n",
      "Epoch  38 Batch 2568/2739 train_loss = 2.154\n",
      "Epoch  38 Batch 2618/2739 train_loss = 1.793\n",
      "Epoch  38 Batch 2668/2739 train_loss = 2.700\n",
      "Epoch  38 Batch 2718/2739 train_loss = 1.833\n",
      "Epoch  39 Batch   29/2739 train_loss = 2.690\n",
      "Epoch  39 Batch   79/2739 train_loss = 2.692\n",
      "Epoch  39 Batch  129/2739 train_loss = 2.446\n",
      "Epoch  39 Batch  179/2739 train_loss = 2.491\n",
      "Epoch  39 Batch  229/2739 train_loss = 2.317\n",
      "Epoch  39 Batch  279/2739 train_loss = 1.203\n",
      "Epoch  39 Batch  329/2739 train_loss = 2.260\n",
      "Epoch  39 Batch  379/2739 train_loss = 1.915\n",
      "Epoch  39 Batch  429/2739 train_loss = 2.281\n",
      "Epoch  39 Batch  479/2739 train_loss = 3.603\n",
      "Epoch  39 Batch  529/2739 train_loss = 3.258\n",
      "Epoch  39 Batch  579/2739 train_loss = 2.720\n",
      "Epoch  39 Batch  629/2739 train_loss = 3.243\n",
      "Epoch  39 Batch  679/2739 train_loss = 3.533\n",
      "Epoch  39 Batch  729/2739 train_loss = 3.731\n",
      "Epoch  39 Batch  779/2739 train_loss = 3.404\n",
      "Epoch  39 Batch  829/2739 train_loss = 2.511\n",
      "Epoch  39 Batch  879/2739 train_loss = 2.104\n",
      "Epoch  39 Batch  929/2739 train_loss = 1.778\n",
      "Epoch  39 Batch  979/2739 train_loss = 4.246\n",
      "Epoch  39 Batch 1029/2739 train_loss = 2.762\n",
      "Epoch  39 Batch 1079/2739 train_loss = 1.963\n",
      "Epoch  39 Batch 1129/2739 train_loss = 1.570\n",
      "Epoch  39 Batch 1179/2739 train_loss = 2.303\n",
      "Epoch  39 Batch 1229/2739 train_loss = 3.293\n",
      "Epoch  39 Batch 1279/2739 train_loss = 2.156\n",
      "Epoch  39 Batch 1329/2739 train_loss = 4.157\n",
      "Epoch  39 Batch 1379/2739 train_loss = 2.080\n",
      "Epoch  39 Batch 1429/2739 train_loss = 3.552\n",
      "Epoch  39 Batch 1479/2739 train_loss = 1.936\n",
      "Epoch  39 Batch 1529/2739 train_loss = 4.290\n",
      "Epoch  39 Batch 1579/2739 train_loss = 1.692\n",
      "Epoch  39 Batch 1629/2739 train_loss = 3.055\n",
      "Epoch  39 Batch 1679/2739 train_loss = 2.173\n",
      "Epoch  39 Batch 1729/2739 train_loss = 1.646\n",
      "Epoch  39 Batch 1779/2739 train_loss = 1.745\n",
      "Epoch  39 Batch 1829/2739 train_loss = 2.622\n",
      "Epoch  39 Batch 1879/2739 train_loss = 2.071\n",
      "Epoch  39 Batch 1929/2739 train_loss = 1.222\n",
      "Epoch  39 Batch 1979/2739 train_loss = 2.410\n",
      "Epoch  39 Batch 2029/2739 train_loss = 3.697\n",
      "Epoch  39 Batch 2079/2739 train_loss = 1.761\n",
      "Epoch  39 Batch 2129/2739 train_loss = 1.737\n",
      "Epoch  39 Batch 2179/2739 train_loss = 3.800\n",
      "Epoch  39 Batch 2229/2739 train_loss = 2.429\n",
      "Epoch  39 Batch 2279/2739 train_loss = 1.486\n",
      "Epoch  39 Batch 2329/2739 train_loss = 2.115\n",
      "Epoch  39 Batch 2379/2739 train_loss = 2.090\n",
      "Epoch  39 Batch 2429/2739 train_loss = 4.050\n",
      "Epoch  39 Batch 2479/2739 train_loss = 2.682\n",
      "Epoch  39 Batch 2529/2739 train_loss = 2.818\n",
      "Epoch  39 Batch 2579/2739 train_loss = 1.184\n",
      "Epoch  39 Batch 2629/2739 train_loss = 1.920\n",
      "Epoch  39 Batch 2679/2739 train_loss = 2.058\n",
      "Epoch  39 Batch 2729/2739 train_loss = 3.798\n",
      "Epoch  40 Batch   40/2739 train_loss = 3.391\n",
      "Epoch  40 Batch   90/2739 train_loss = 1.718\n",
      "Epoch  40 Batch  140/2739 train_loss = 1.951\n",
      "Epoch  40 Batch  190/2739 train_loss = 2.340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  40 Batch  240/2739 train_loss = 2.910\n",
      "Epoch  40 Batch  290/2739 train_loss = 1.494\n",
      "Epoch  40 Batch  340/2739 train_loss = 3.993\n",
      "Epoch  40 Batch  390/2739 train_loss = 2.186\n",
      "Epoch  40 Batch  440/2739 train_loss = 1.319\n",
      "Epoch  40 Batch  490/2739 train_loss = 3.417\n",
      "Epoch  40 Batch  540/2739 train_loss = 2.246\n",
      "Epoch  40 Batch  590/2739 train_loss = 3.823\n",
      "Epoch  40 Batch  640/2739 train_loss = 1.905\n",
      "Epoch  40 Batch  690/2739 train_loss = 1.642\n",
      "Epoch  40 Batch  740/2739 train_loss = 2.226\n",
      "Epoch  40 Batch  790/2739 train_loss = 3.313\n",
      "Epoch  40 Batch  840/2739 train_loss = 3.422\n",
      "Epoch  40 Batch  890/2739 train_loss = 3.406\n",
      "Epoch  40 Batch  940/2739 train_loss = 2.437\n",
      "Epoch  40 Batch  990/2739 train_loss = 3.496\n",
      "Epoch  40 Batch 1040/2739 train_loss = 2.272\n",
      "Epoch  40 Batch 1090/2739 train_loss = 1.393\n",
      "Epoch  40 Batch 1140/2739 train_loss = 1.287\n",
      "Epoch  40 Batch 1190/2739 train_loss = 2.545\n",
      "Epoch  40 Batch 1240/2739 train_loss = 1.957\n",
      "Epoch  40 Batch 1290/2739 train_loss = 1.202\n",
      "Epoch  40 Batch 1340/2739 train_loss = 4.079\n",
      "Epoch  40 Batch 1390/2739 train_loss = 2.002\n",
      "Epoch  40 Batch 1440/2739 train_loss = 1.568\n",
      "Epoch  40 Batch 1490/2739 train_loss = 1.819\n",
      "Epoch  40 Batch 1540/2739 train_loss = 2.098\n",
      "Epoch  40 Batch 1590/2739 train_loss = 1.774\n",
      "Epoch  40 Batch 1640/2739 train_loss = 2.931\n",
      "Epoch  40 Batch 1690/2739 train_loss = 2.090\n",
      "Epoch  40 Batch 1740/2739 train_loss = 2.898\n",
      "Epoch  40 Batch 1790/2739 train_loss = 2.565\n",
      "Epoch  40 Batch 1840/2739 train_loss = 3.371\n",
      "Epoch  40 Batch 1890/2739 train_loss = 2.681\n",
      "Epoch  40 Batch 1940/2739 train_loss = 4.485\n",
      "Epoch  40 Batch 1990/2739 train_loss = 2.731\n",
      "Epoch  40 Batch 2040/2739 train_loss = 3.793\n",
      "Epoch  40 Batch 2090/2739 train_loss = 3.975\n",
      "Epoch  40 Batch 2140/2739 train_loss = 1.897\n",
      "Epoch  40 Batch 2190/2739 train_loss = 1.795\n",
      "Epoch  40 Batch 2240/2739 train_loss = 3.881\n",
      "Epoch  40 Batch 2290/2739 train_loss = 1.873\n",
      "Epoch  40 Batch 2340/2739 train_loss = 4.047\n",
      "Epoch  40 Batch 2390/2739 train_loss = 2.563\n",
      "Epoch  40 Batch 2440/2739 train_loss = 4.087\n",
      "Epoch  40 Batch 2490/2739 train_loss = 2.847\n",
      "Epoch  40 Batch 2540/2739 train_loss = 1.369\n",
      "Epoch  40 Batch 2590/2739 train_loss = 1.630\n",
      "Epoch  40 Batch 2640/2739 train_loss = 1.772\n",
      "Epoch  40 Batch 2690/2739 train_loss = 2.283\n",
      "Epoch  41 Batch    1/2739 train_loss = 2.186\n",
      "Epoch  41 Batch   51/2739 train_loss = 1.283\n",
      "Epoch  41 Batch  101/2739 train_loss = 2.221\n",
      "Epoch  41 Batch  151/2739 train_loss = 3.076\n",
      "Epoch  41 Batch  201/2739 train_loss = 1.605\n",
      "Epoch  41 Batch  251/2739 train_loss = 2.490\n",
      "Epoch  41 Batch  301/2739 train_loss = 3.835\n",
      "Epoch  41 Batch  351/2739 train_loss = 2.652\n",
      "Epoch  41 Batch  401/2739 train_loss = 2.811\n",
      "Epoch  41 Batch  451/2739 train_loss = 3.205\n",
      "Epoch  41 Batch  501/2739 train_loss = 1.102\n",
      "Epoch  41 Batch  551/2739 train_loss = 1.946\n",
      "Epoch  41 Batch  601/2739 train_loss = 2.252\n",
      "Epoch  41 Batch  651/2739 train_loss = 2.419\n",
      "Epoch  41 Batch  701/2739 train_loss = 3.065\n",
      "Epoch  41 Batch  751/2739 train_loss = 2.345\n",
      "Epoch  41 Batch  801/2739 train_loss = 2.758\n",
      "Epoch  41 Batch  851/2739 train_loss = 2.440\n",
      "Epoch  41 Batch  901/2739 train_loss = 4.191\n",
      "Epoch  41 Batch  951/2739 train_loss = 1.634\n",
      "Epoch  41 Batch 1001/2739 train_loss = 3.917\n",
      "Epoch  41 Batch 1051/2739 train_loss = 2.462\n",
      "Epoch  41 Batch 1101/2739 train_loss = 1.484\n",
      "Epoch  41 Batch 1151/2739 train_loss = 2.176\n",
      "Epoch  41 Batch 1201/2739 train_loss = 3.313\n",
      "Epoch  41 Batch 1251/2739 train_loss = 1.813\n",
      "Epoch  41 Batch 1301/2739 train_loss = 2.001\n",
      "Epoch  41 Batch 1351/2739 train_loss = 2.481\n",
      "Epoch  41 Batch 1401/2739 train_loss = 3.734\n",
      "Epoch  41 Batch 1451/2739 train_loss = 2.690\n",
      "Epoch  41 Batch 1501/2739 train_loss = 1.525\n",
      "Epoch  41 Batch 1551/2739 train_loss = 2.007\n",
      "Epoch  41 Batch 1601/2739 train_loss = 3.781\n",
      "Epoch  41 Batch 1651/2739 train_loss = 2.452\n",
      "Epoch  41 Batch 1701/2739 train_loss = 1.637\n",
      "Epoch  41 Batch 1751/2739 train_loss = 1.480\n",
      "Epoch  41 Batch 1801/2739 train_loss = 2.538\n",
      "Epoch  41 Batch 1851/2739 train_loss = 3.985\n",
      "Epoch  41 Batch 1901/2739 train_loss = 1.356\n",
      "Epoch  41 Batch 1951/2739 train_loss = 2.215\n",
      "Epoch  41 Batch 2001/2739 train_loss = 2.300\n",
      "Epoch  41 Batch 2051/2739 train_loss = 4.034\n",
      "Epoch  41 Batch 2101/2739 train_loss = 1.817\n",
      "Epoch  41 Batch 2151/2739 train_loss = 2.421\n",
      "Epoch  41 Batch 2201/2739 train_loss = 4.021\n",
      "Epoch  41 Batch 2251/2739 train_loss = 4.276\n",
      "Epoch  41 Batch 2301/2739 train_loss = 3.339\n",
      "Epoch  41 Batch 2351/2739 train_loss = 2.199\n",
      "Epoch  41 Batch 2401/2739 train_loss = 2.508\n",
      "Epoch  41 Batch 2451/2739 train_loss = 2.025\n",
      "Epoch  41 Batch 2501/2739 train_loss = 1.659\n",
      "Epoch  41 Batch 2551/2739 train_loss = 1.452\n",
      "Epoch  41 Batch 2601/2739 train_loss = 3.014\n",
      "Epoch  41 Batch 2651/2739 train_loss = 2.091\n",
      "Epoch  41 Batch 2701/2739 train_loss = 1.712\n",
      "Epoch  42 Batch   12/2739 train_loss = 2.890\n",
      "Epoch  42 Batch   62/2739 train_loss = 2.353\n",
      "Epoch  42 Batch  112/2739 train_loss = 2.956\n",
      "Epoch  42 Batch  162/2739 train_loss = 2.440\n",
      "Epoch  42 Batch  212/2739 train_loss = 2.397\n",
      "Epoch  42 Batch  262/2739 train_loss = 1.991\n",
      "Epoch  42 Batch  312/2739 train_loss = 1.960\n",
      "Epoch  42 Batch  362/2739 train_loss = 2.502\n",
      "Epoch  42 Batch  412/2739 train_loss = 2.932\n",
      "Epoch  42 Batch  462/2739 train_loss = 1.890\n",
      "Epoch  42 Batch  512/2739 train_loss = 2.532\n",
      "Epoch  42 Batch  562/2739 train_loss = 1.732\n",
      "Epoch  42 Batch  612/2739 train_loss = 2.103\n",
      "Epoch  42 Batch  662/2739 train_loss = 3.279\n",
      "Epoch  42 Batch  712/2739 train_loss = 3.742\n",
      "Epoch  42 Batch  762/2739 train_loss = 1.226\n",
      "Epoch  42 Batch  812/2739 train_loss = 1.824\n",
      "Epoch  42 Batch  862/2739 train_loss = 3.689\n",
      "Epoch  42 Batch  912/2739 train_loss = 1.582\n",
      "Epoch  42 Batch  962/2739 train_loss = 3.217\n",
      "Epoch  42 Batch 1012/2739 train_loss = 1.800\n",
      "Epoch  42 Batch 1062/2739 train_loss = 1.440\n",
      "Epoch  42 Batch 1112/2739 train_loss = 1.834\n",
      "Epoch  42 Batch 1162/2739 train_loss = 2.770\n",
      "Epoch  42 Batch 1212/2739 train_loss = 2.866\n",
      "Epoch  42 Batch 1262/2739 train_loss = 2.521\n",
      "Epoch  42 Batch 1312/2739 train_loss = 2.116\n",
      "Epoch  42 Batch 1362/2739 train_loss = 2.267\n",
      "Epoch  42 Batch 1412/2739 train_loss = 2.384\n",
      "Epoch  42 Batch 1462/2739 train_loss = 2.077\n",
      "Epoch  42 Batch 1512/2739 train_loss = 1.784\n",
      "Epoch  42 Batch 1562/2739 train_loss = 3.042\n",
      "Epoch  42 Batch 1612/2739 train_loss = 2.433\n",
      "Epoch  42 Batch 1662/2739 train_loss = 1.740\n",
      "Epoch  42 Batch 1712/2739 train_loss = 3.563\n",
      "Epoch  42 Batch 1762/2739 train_loss = 2.041\n",
      "Epoch  42 Batch 1812/2739 train_loss = 1.608\n",
      "Epoch  42 Batch 1862/2739 train_loss = 1.690\n",
      "Epoch  42 Batch 1912/2739 train_loss = 2.557\n",
      "Epoch  42 Batch 1962/2739 train_loss = 2.284\n",
      "Epoch  42 Batch 2012/2739 train_loss = 2.926\n",
      "Epoch  42 Batch 2062/2739 train_loss = 2.416\n",
      "Epoch  42 Batch 2112/2739 train_loss = 1.301\n",
      "Epoch  42 Batch 2162/2739 train_loss = 2.655\n",
      "Epoch  42 Batch 2212/2739 train_loss = 3.070\n",
      "Epoch  42 Batch 2262/2739 train_loss = 4.221\n",
      "Epoch  42 Batch 2312/2739 train_loss = 2.940\n",
      "Epoch  42 Batch 2362/2739 train_loss = 1.583\n",
      "Epoch  42 Batch 2412/2739 train_loss = 2.424\n",
      "Epoch  42 Batch 2462/2739 train_loss = 3.332\n",
      "Epoch  42 Batch 2512/2739 train_loss = 2.164\n",
      "Epoch  42 Batch 2562/2739 train_loss = 1.859\n",
      "Epoch  42 Batch 2612/2739 train_loss = 1.905\n",
      "Epoch  42 Batch 2662/2739 train_loss = 2.211\n",
      "Epoch  42 Batch 2712/2739 train_loss = 2.270\n",
      "Epoch  43 Batch   23/2739 train_loss = 1.590\n",
      "Epoch  43 Batch   73/2739 train_loss = 2.084\n",
      "Epoch  43 Batch  123/2739 train_loss = 2.105\n",
      "Epoch  43 Batch  173/2739 train_loss = 1.750\n",
      "Epoch  43 Batch  223/2739 train_loss = 1.829\n",
      "Epoch  43 Batch  273/2739 train_loss = 2.999\n",
      "Epoch  43 Batch  323/2739 train_loss = 3.285\n",
      "Epoch  43 Batch  373/2739 train_loss = 1.712\n",
      "Epoch  43 Batch  423/2739 train_loss = 1.543\n",
      "Epoch  43 Batch  473/2739 train_loss = 2.259\n",
      "Epoch  43 Batch  523/2739 train_loss = 1.318\n",
      "Epoch  43 Batch  573/2739 train_loss = 2.452\n",
      "Epoch  43 Batch  623/2739 train_loss = 2.103\n",
      "Epoch  43 Batch  673/2739 train_loss = 3.051\n",
      "Epoch  43 Batch  723/2739 train_loss = 3.292\n",
      "Epoch  43 Batch  773/2739 train_loss = 3.789\n",
      "Epoch  43 Batch  823/2739 train_loss = 2.209\n",
      "Epoch  43 Batch  873/2739 train_loss = 1.603\n",
      "Epoch  43 Batch  923/2739 train_loss = 3.097\n",
      "Epoch  43 Batch  973/2739 train_loss = 2.805\n",
      "Epoch  43 Batch 1023/2739 train_loss = 2.583\n",
      "Epoch  43 Batch 1073/2739 train_loss = 3.684\n",
      "Epoch  43 Batch 1123/2739 train_loss = 3.499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  43 Batch 1173/2739 train_loss = 4.482\n",
      "Epoch  43 Batch 1223/2739 train_loss = 1.449\n",
      "Epoch  43 Batch 1273/2739 train_loss = 2.882\n",
      "Epoch  43 Batch 1323/2739 train_loss = 3.969\n",
      "Epoch  43 Batch 1373/2739 train_loss = 2.180\n",
      "Epoch  43 Batch 1423/2739 train_loss = 1.830\n",
      "Epoch  43 Batch 1473/2739 train_loss = 2.291\n",
      "Epoch  43 Batch 1523/2739 train_loss = 1.069\n",
      "Epoch  43 Batch 1573/2739 train_loss = 3.829\n",
      "Epoch  43 Batch 1623/2739 train_loss = 3.852\n",
      "Epoch  43 Batch 1673/2739 train_loss = 1.647\n",
      "Epoch  43 Batch 1723/2739 train_loss = 3.956\n",
      "Epoch  43 Batch 1773/2739 train_loss = 2.547\n",
      "Epoch  43 Batch 1823/2739 train_loss = 2.089\n",
      "Epoch  43 Batch 1873/2739 train_loss = 3.648\n",
      "Epoch  43 Batch 1923/2739 train_loss = 1.574\n",
      "Epoch  43 Batch 1973/2739 train_loss = 1.812\n",
      "Epoch  43 Batch 2023/2739 train_loss = 2.353\n",
      "Epoch  43 Batch 2073/2739 train_loss = 3.428\n",
      "Epoch  43 Batch 2123/2739 train_loss = 1.959\n",
      "Epoch  43 Batch 2173/2739 train_loss = 2.958\n",
      "Epoch  43 Batch 2223/2739 train_loss = 1.615\n",
      "Epoch  43 Batch 2273/2739 train_loss = 2.209\n",
      "Epoch  43 Batch 2323/2739 train_loss = 4.149\n",
      "Epoch  43 Batch 2373/2739 train_loss = 2.719\n",
      "Epoch  43 Batch 2423/2739 train_loss = 1.502\n",
      "Epoch  43 Batch 2473/2739 train_loss = 2.522\n",
      "Epoch  43 Batch 2523/2739 train_loss = 2.550\n",
      "Epoch  43 Batch 2573/2739 train_loss = 1.736\n",
      "Epoch  43 Batch 2623/2739 train_loss = 2.148\n",
      "Epoch  43 Batch 2673/2739 train_loss = 2.224\n",
      "Epoch  43 Batch 2723/2739 train_loss = 1.468\n",
      "Epoch  44 Batch   34/2739 train_loss = 1.310\n",
      "Epoch  44 Batch   84/2739 train_loss = 2.396\n",
      "Epoch  44 Batch  134/2739 train_loss = 1.953\n",
      "Epoch  44 Batch  184/2739 train_loss = 2.204\n",
      "Epoch  44 Batch  234/2739 train_loss = 1.410\n",
      "Epoch  44 Batch  284/2739 train_loss = 1.939\n",
      "Epoch  44 Batch  334/2739 train_loss = 2.635\n",
      "Epoch  44 Batch  384/2739 train_loss = 2.018\n",
      "Epoch  44 Batch  434/2739 train_loss = 1.962\n",
      "Epoch  44 Batch  484/2739 train_loss = 2.265\n",
      "Epoch  44 Batch  534/2739 train_loss = 2.183\n",
      "Epoch  44 Batch  584/2739 train_loss = 1.795\n",
      "Epoch  44 Batch  634/2739 train_loss = 2.747\n",
      "Epoch  44 Batch  684/2739 train_loss = 2.999\n",
      "Epoch  44 Batch  734/2739 train_loss = 2.518\n",
      "Epoch  44 Batch  784/2739 train_loss = 3.780\n",
      "Epoch  44 Batch  834/2739 train_loss = 2.046\n",
      "Epoch  44 Batch  884/2739 train_loss = 2.403\n",
      "Epoch  44 Batch  934/2739 train_loss = 2.020\n",
      "Epoch  44 Batch  984/2739 train_loss = 1.798\n",
      "Epoch  44 Batch 1034/2739 train_loss = 1.774\n",
      "Epoch  44 Batch 1084/2739 train_loss = 1.268\n",
      "Epoch  44 Batch 1134/2739 train_loss = 2.798\n",
      "Epoch  44 Batch 1184/2739 train_loss = 1.695\n",
      "Epoch  44 Batch 1234/2739 train_loss = 3.035\n",
      "Epoch  44 Batch 1284/2739 train_loss = 1.606\n",
      "Epoch  44 Batch 1334/2739 train_loss = 3.794\n",
      "Epoch  44 Batch 1384/2739 train_loss = 2.538\n",
      "Epoch  44 Batch 1434/2739 train_loss = 1.598\n",
      "Epoch  44 Batch 1484/2739 train_loss = 1.462\n",
      "Epoch  44 Batch 1534/2739 train_loss = 1.992\n",
      "Epoch  44 Batch 1584/2739 train_loss = 1.480\n",
      "Epoch  44 Batch 1634/2739 train_loss = 1.717\n",
      "Epoch  44 Batch 1684/2739 train_loss = 3.867\n",
      "Epoch  44 Batch 1734/2739 train_loss = 2.211\n",
      "Epoch  44 Batch 1784/2739 train_loss = 2.139\n",
      "Epoch  44 Batch 1834/2739 train_loss = 3.839\n",
      "Epoch  44 Batch 1884/2739 train_loss = 3.573\n",
      "Epoch  44 Batch 1934/2739 train_loss = 2.384\n",
      "Epoch  44 Batch 1984/2739 train_loss = 2.996\n",
      "Epoch  44 Batch 2034/2739 train_loss = 4.042\n",
      "Epoch  44 Batch 2084/2739 train_loss = 2.779\n",
      "Epoch  44 Batch 2134/2739 train_loss = 3.483\n",
      "Epoch  44 Batch 2184/2739 train_loss = 2.080\n",
      "Epoch  44 Batch 2234/2739 train_loss = 3.820\n",
      "Epoch  44 Batch 2284/2739 train_loss = 1.664\n",
      "Epoch  44 Batch 2334/2739 train_loss = 2.253\n",
      "Epoch  44 Batch 2384/2739 train_loss = 1.308\n",
      "Epoch  44 Batch 2434/2739 train_loss = 4.235\n",
      "Epoch  44 Batch 2484/2739 train_loss = 1.220\n",
      "Epoch  44 Batch 2534/2739 train_loss = 3.450\n",
      "Epoch  44 Batch 2584/2739 train_loss = 3.809\n",
      "Epoch  44 Batch 2634/2739 train_loss = 2.819\n",
      "Epoch  44 Batch 2684/2739 train_loss = 2.218\n",
      "Epoch  44 Batch 2734/2739 train_loss = 2.076\n",
      "Epoch  45 Batch   45/2739 train_loss = 1.971\n",
      "Epoch  45 Batch   95/2739 train_loss = 1.593\n",
      "Epoch  45 Batch  145/2739 train_loss = 1.614\n",
      "Epoch  45 Batch  195/2739 train_loss = 2.177\n",
      "Epoch  45 Batch  245/2739 train_loss = 2.243\n",
      "Epoch  45 Batch  295/2739 train_loss = 2.362\n",
      "Epoch  45 Batch  345/2739 train_loss = 1.878\n",
      "Epoch  45 Batch  395/2739 train_loss = 2.236\n",
      "Epoch  45 Batch  445/2739 train_loss = 1.486\n",
      "Epoch  45 Batch  495/2739 train_loss = 2.504\n",
      "Epoch  45 Batch  545/2739 train_loss = 2.210\n",
      "Epoch  45 Batch  595/2739 train_loss = 2.474\n",
      "Epoch  45 Batch  645/2739 train_loss = 2.364\n",
      "Epoch  45 Batch  695/2739 train_loss = 1.711\n",
      "Epoch  45 Batch  745/2739 train_loss = 2.606\n",
      "Epoch  45 Batch  795/2739 train_loss = 2.258\n",
      "Epoch  45 Batch  845/2739 train_loss = 2.294\n",
      "Epoch  45 Batch  895/2739 train_loss = 1.747\n",
      "Epoch  45 Batch  945/2739 train_loss = 3.866\n",
      "Epoch  45 Batch  995/2739 train_loss = 2.451\n",
      "Epoch  45 Batch 1045/2739 train_loss = 2.415\n",
      "Epoch  45 Batch 1095/2739 train_loss = 1.109\n",
      "Epoch  45 Batch 1145/2739 train_loss = 3.181\n",
      "Epoch  45 Batch 1195/2739 train_loss = 2.253\n",
      "Epoch  45 Batch 1245/2739 train_loss = 1.751\n",
      "Epoch  45 Batch 1295/2739 train_loss = 1.188\n",
      "Epoch  45 Batch 1345/2739 train_loss = 2.069\n",
      "Epoch  45 Batch 1395/2739 train_loss = 2.156\n",
      "Epoch  45 Batch 1445/2739 train_loss = 1.126\n",
      "Epoch  45 Batch 1495/2739 train_loss = 2.287\n",
      "Epoch  45 Batch 1545/2739 train_loss = 3.044\n",
      "Epoch  45 Batch 1595/2739 train_loss = 1.926\n",
      "Epoch  45 Batch 1645/2739 train_loss = 3.782\n",
      "Epoch  45 Batch 1695/2739 train_loss = 2.309\n",
      "Epoch  45 Batch 1745/2739 train_loss = 3.917\n",
      "Epoch  45 Batch 1795/2739 train_loss = 2.520\n",
      "Epoch  45 Batch 1845/2739 train_loss = 1.427\n",
      "Epoch  45 Batch 1895/2739 train_loss = 1.998\n",
      "Epoch  45 Batch 1945/2739 train_loss = 1.323\n",
      "Epoch  45 Batch 1995/2739 train_loss = 0.910\n",
      "Epoch  45 Batch 2045/2739 train_loss = 3.670\n",
      "Epoch  45 Batch 2095/2739 train_loss = 2.223\n",
      "Epoch  45 Batch 2145/2739 train_loss = 2.601\n",
      "Epoch  45 Batch 2195/2739 train_loss = 4.225\n",
      "Epoch  45 Batch 2245/2739 train_loss = 3.789\n",
      "Epoch  45 Batch 2295/2739 train_loss = 3.361\n",
      "Epoch  45 Batch 2345/2739 train_loss = 2.601\n",
      "Epoch  45 Batch 2395/2739 train_loss = 2.438\n",
      "Epoch  45 Batch 2445/2739 train_loss = 2.600\n",
      "Epoch  45 Batch 2495/2739 train_loss = 1.856\n",
      "Epoch  45 Batch 2545/2739 train_loss = 1.707\n",
      "Epoch  45 Batch 2595/2739 train_loss = 3.717\n",
      "Epoch  45 Batch 2645/2739 train_loss = 1.582\n",
      "Epoch  45 Batch 2695/2739 train_loss = 2.101\n",
      "Epoch  46 Batch    6/2739 train_loss = 2.291\n",
      "Epoch  46 Batch   56/2739 train_loss = 2.333\n",
      "Epoch  46 Batch  106/2739 train_loss = 1.944\n",
      "Epoch  46 Batch  156/2739 train_loss = 2.488\n",
      "Epoch  46 Batch  206/2739 train_loss = 2.544\n",
      "Epoch  46 Batch  256/2739 train_loss = 2.129\n",
      "Epoch  46 Batch  306/2739 train_loss = 1.638\n",
      "Epoch  46 Batch  356/2739 train_loss = 0.952\n",
      "Epoch  46 Batch  406/2739 train_loss = 2.424\n",
      "Epoch  46 Batch  456/2739 train_loss = 3.063\n",
      "Epoch  46 Batch  506/2739 train_loss = 1.390\n",
      "Epoch  46 Batch  556/2739 train_loss = 1.969\n",
      "Epoch  46 Batch  606/2739 train_loss = 3.993\n",
      "Epoch  46 Batch  656/2739 train_loss = 3.359\n",
      "Epoch  46 Batch  706/2739 train_loss = 2.558\n",
      "Epoch  46 Batch  756/2739 train_loss = 3.515\n",
      "Epoch  46 Batch  806/2739 train_loss = 1.841\n",
      "Epoch  46 Batch  856/2739 train_loss = 2.004\n",
      "Epoch  46 Batch  906/2739 train_loss = 2.971\n",
      "Epoch  46 Batch  956/2739 train_loss = 3.987\n",
      "Epoch  46 Batch 1006/2739 train_loss = 2.385\n",
      "Epoch  46 Batch 1056/2739 train_loss = 2.040\n",
      "Epoch  46 Batch 1106/2739 train_loss = 1.568\n",
      "Epoch  46 Batch 1156/2739 train_loss = 2.483\n",
      "Epoch  46 Batch 1206/2739 train_loss = 2.653\n",
      "Epoch  46 Batch 1256/2739 train_loss = 2.144\n",
      "Epoch  46 Batch 1306/2739 train_loss = 2.149\n",
      "Epoch  46 Batch 1356/2739 train_loss = 1.584\n",
      "Epoch  46 Batch 1406/2739 train_loss = 3.432\n",
      "Epoch  46 Batch 1456/2739 train_loss = 2.180\n",
      "Epoch  46 Batch 1506/2739 train_loss = 2.158\n",
      "Epoch  46 Batch 1556/2739 train_loss = 2.024\n",
      "Epoch  46 Batch 1606/2739 train_loss = 1.678\n",
      "Epoch  46 Batch 1656/2739 train_loss = 3.529\n",
      "Epoch  46 Batch 1706/2739 train_loss = 1.968\n",
      "Epoch  46 Batch 1756/2739 train_loss = 2.048\n",
      "Epoch  46 Batch 1806/2739 train_loss = 4.048\n",
      "Epoch  46 Batch 1856/2739 train_loss = 3.843\n",
      "Epoch  46 Batch 1906/2739 train_loss = 2.725\n",
      "Epoch  46 Batch 1956/2739 train_loss = 2.492\n",
      "Epoch  46 Batch 2006/2739 train_loss = 3.107\n",
      "Epoch  46 Batch 2056/2739 train_loss = 2.722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  46 Batch 2106/2739 train_loss = 1.841\n",
      "Epoch  46 Batch 2156/2739 train_loss = 2.524\n",
      "Epoch  46 Batch 2206/2739 train_loss = 3.796\n",
      "Epoch  46 Batch 2256/2739 train_loss = 1.825\n",
      "Epoch  46 Batch 2306/2739 train_loss = 3.925\n",
      "Epoch  46 Batch 2356/2739 train_loss = 2.278\n",
      "Epoch  46 Batch 2406/2739 train_loss = 2.149\n",
      "Epoch  46 Batch 2456/2739 train_loss = 3.429\n",
      "Epoch  46 Batch 2506/2739 train_loss = 3.999\n",
      "Epoch  46 Batch 2556/2739 train_loss = 2.140\n",
      "Epoch  46 Batch 2606/2739 train_loss = 2.431\n",
      "Epoch  46 Batch 2656/2739 train_loss = 2.937\n",
      "Epoch  46 Batch 2706/2739 train_loss = 2.144\n",
      "Epoch  47 Batch   17/2739 train_loss = 2.215\n",
      "Epoch  47 Batch   67/2739 train_loss = 4.275\n",
      "Epoch  47 Batch  117/2739 train_loss = 2.777\n",
      "Epoch  47 Batch  167/2739 train_loss = 2.191\n",
      "Epoch  47 Batch  217/2739 train_loss = 2.384\n",
      "Epoch  47 Batch  267/2739 train_loss = 1.970\n",
      "Epoch  47 Batch  317/2739 train_loss = 2.476\n",
      "Epoch  47 Batch  367/2739 train_loss = 2.258\n",
      "Epoch  47 Batch  417/2739 train_loss = 2.052\n",
      "Epoch  47 Batch  467/2739 train_loss = 2.357\n",
      "Epoch  47 Batch  517/2739 train_loss = 2.258\n",
      "Epoch  47 Batch  567/2739 train_loss = 2.266\n",
      "Epoch  47 Batch  617/2739 train_loss = 3.766\n",
      "Epoch  47 Batch  667/2739 train_loss = 2.230\n",
      "Epoch  47 Batch  717/2739 train_loss = 2.138\n",
      "Epoch  47 Batch  767/2739 train_loss = 1.356\n",
      "Epoch  47 Batch  817/2739 train_loss = 2.719\n",
      "Epoch  47 Batch  867/2739 train_loss = 3.573\n",
      "Epoch  47 Batch  917/2739 train_loss = 1.779\n",
      "Epoch  47 Batch  967/2739 train_loss = 3.540\n",
      "Epoch  47 Batch 1017/2739 train_loss = 2.406\n",
      "Epoch  47 Batch 1067/2739 train_loss = 1.233\n",
      "Epoch  47 Batch 1117/2739 train_loss = 3.206\n",
      "Epoch  47 Batch 1167/2739 train_loss = 2.844\n",
      "Epoch  47 Batch 1217/2739 train_loss = 1.782\n",
      "Epoch  47 Batch 1267/2739 train_loss = 2.395\n",
      "Epoch  47 Batch 1317/2739 train_loss = 3.936\n",
      "Epoch  47 Batch 1367/2739 train_loss = 2.413\n",
      "Epoch  47 Batch 1417/2739 train_loss = 2.559\n",
      "Epoch  47 Batch 1467/2739 train_loss = 1.372\n",
      "Epoch  47 Batch 1517/2739 train_loss = 1.018\n",
      "Epoch  47 Batch 1567/2739 train_loss = 1.940\n",
      "Epoch  47 Batch 1617/2739 train_loss = 2.659\n",
      "Epoch  47 Batch 1667/2739 train_loss = 1.910\n",
      "Epoch  47 Batch 1717/2739 train_loss = 2.341\n",
      "Epoch  47 Batch 1767/2739 train_loss = 2.137\n",
      "Epoch  47 Batch 1817/2739 train_loss = 0.916\n",
      "Epoch  47 Batch 1867/2739 train_loss = 1.718\n",
      "Epoch  47 Batch 1917/2739 train_loss = 2.074\n",
      "Epoch  47 Batch 1967/2739 train_loss = 2.827\n",
      "Epoch  47 Batch 2017/2739 train_loss = 3.938\n",
      "Epoch  47 Batch 2067/2739 train_loss = 3.941\n",
      "Epoch  47 Batch 2117/2739 train_loss = 3.877\n",
      "Epoch  47 Batch 2167/2739 train_loss = 1.749\n",
      "Epoch  47 Batch 2217/2739 train_loss = 2.825\n",
      "Epoch  47 Batch 2267/2739 train_loss = 2.602\n",
      "Epoch  47 Batch 2317/2739 train_loss = 4.319\n",
      "Epoch  47 Batch 2367/2739 train_loss = 2.839\n",
      "Epoch  47 Batch 2417/2739 train_loss = 2.302\n",
      "Epoch  47 Batch 2467/2739 train_loss = 2.202\n",
      "Epoch  47 Batch 2517/2739 train_loss = 3.025\n",
      "Epoch  47 Batch 2567/2739 train_loss = 1.717\n",
      "Epoch  47 Batch 2617/2739 train_loss = 2.555\n",
      "Epoch  47 Batch 2667/2739 train_loss = 2.579\n",
      "Epoch  47 Batch 2717/2739 train_loss = 2.043\n",
      "Epoch  48 Batch   28/2739 train_loss = 2.289\n",
      "Epoch  48 Batch   78/2739 train_loss = 2.424\n",
      "Epoch  48 Batch  128/2739 train_loss = 2.156\n",
      "Epoch  48 Batch  178/2739 train_loss = 1.980\n",
      "Epoch  48 Batch  228/2739 train_loss = 1.580\n",
      "Epoch  48 Batch  278/2739 train_loss = 2.181\n",
      "Epoch  48 Batch  328/2739 train_loss = 3.796\n",
      "Epoch  48 Batch  378/2739 train_loss = 1.886\n",
      "Epoch  48 Batch  428/2739 train_loss = 1.571\n",
      "Epoch  48 Batch  478/2739 train_loss = 1.103\n",
      "Epoch  48 Batch  528/2739 train_loss = 1.883\n",
      "Epoch  48 Batch  578/2739 train_loss = 1.637\n",
      "Epoch  48 Batch  628/2739 train_loss = 2.649\n",
      "Epoch  48 Batch  678/2739 train_loss = 2.735\n",
      "Epoch  48 Batch  728/2739 train_loss = 1.551\n",
      "Epoch  48 Batch  778/2739 train_loss = 3.248\n",
      "Epoch  48 Batch  828/2739 train_loss = 2.713\n",
      "Epoch  48 Batch  878/2739 train_loss = 2.578\n",
      "Epoch  48 Batch  928/2739 train_loss = 1.794\n",
      "Epoch  48 Batch  978/2739 train_loss = 2.310\n",
      "Epoch  48 Batch 1028/2739 train_loss = 2.220\n",
      "Epoch  48 Batch 1078/2739 train_loss = 2.692\n",
      "Epoch  48 Batch 1128/2739 train_loss = 2.248\n",
      "Epoch  48 Batch 1178/2739 train_loss = 2.665\n",
      "Epoch  48 Batch 1228/2739 train_loss = 2.825\n",
      "Epoch  48 Batch 1278/2739 train_loss = 2.690\n",
      "Epoch  48 Batch 1328/2739 train_loss = 2.642\n",
      "Epoch  48 Batch 1378/2739 train_loss = 2.063\n",
      "Epoch  48 Batch 1428/2739 train_loss = 1.559\n",
      "Epoch  48 Batch 1478/2739 train_loss = 2.430\n",
      "Epoch  48 Batch 1528/2739 train_loss = 1.947\n",
      "Epoch  48 Batch 1578/2739 train_loss = 2.647\n",
      "Epoch  48 Batch 1628/2739 train_loss = 3.584\n",
      "Epoch  48 Batch 1678/2739 train_loss = 3.501\n",
      "Epoch  48 Batch 1728/2739 train_loss = 4.017\n",
      "Epoch  48 Batch 1778/2739 train_loss = 2.351\n",
      "Epoch  48 Batch 1828/2739 train_loss = 2.180\n",
      "Epoch  48 Batch 1878/2739 train_loss = 2.168\n",
      "Epoch  48 Batch 1928/2739 train_loss = 2.158\n",
      "Epoch  48 Batch 1978/2739 train_loss = 2.470\n",
      "Epoch  48 Batch 2028/2739 train_loss = 3.538\n",
      "Epoch  48 Batch 2078/2739 train_loss = 4.194\n",
      "Epoch  48 Batch 2128/2739 train_loss = 3.416\n",
      "Epoch  48 Batch 2178/2739 train_loss = 2.233\n",
      "Epoch  48 Batch 2228/2739 train_loss = 2.726\n",
      "Epoch  48 Batch 2278/2739 train_loss = 1.730\n",
      "Epoch  48 Batch 2328/2739 train_loss = 2.634\n",
      "Epoch  48 Batch 2378/2739 train_loss = 2.471\n",
      "Epoch  48 Batch 2428/2739 train_loss = 3.823\n",
      "Epoch  48 Batch 2478/2739 train_loss = 1.276\n",
      "Epoch  48 Batch 2528/2739 train_loss = 3.647\n",
      "Epoch  48 Batch 2578/2739 train_loss = 2.308\n",
      "Epoch  48 Batch 2628/2739 train_loss = 2.218\n",
      "Epoch  48 Batch 2678/2739 train_loss = 2.580\n",
      "Epoch  48 Batch 2728/2739 train_loss = 3.881\n",
      "Epoch  49 Batch   39/2739 train_loss = 3.770\n",
      "Epoch  49 Batch   89/2739 train_loss = 2.396\n",
      "Epoch  49 Batch  139/2739 train_loss = 2.065\n",
      "Epoch  49 Batch  189/2739 train_loss = 2.516\n",
      "Epoch  49 Batch  239/2739 train_loss = 2.134\n",
      "Epoch  49 Batch  289/2739 train_loss = 2.011\n",
      "Epoch  49 Batch  339/2739 train_loss = 4.049\n",
      "Epoch  49 Batch  389/2739 train_loss = 2.230\n",
      "Epoch  49 Batch  439/2739 train_loss = 2.579\n",
      "Epoch  49 Batch  489/2739 train_loss = 3.361\n",
      "Epoch  49 Batch  539/2739 train_loss = 1.621\n",
      "Epoch  49 Batch  589/2739 train_loss = 3.451\n",
      "Epoch  49 Batch  639/2739 train_loss = 2.050\n",
      "Epoch  49 Batch  689/2739 train_loss = 1.811\n",
      "Epoch  49 Batch  739/2739 train_loss = 3.404\n",
      "Epoch  49 Batch  789/2739 train_loss = 2.080\n",
      "Epoch  49 Batch  839/2739 train_loss = 2.338\n",
      "Epoch  49 Batch  889/2739 train_loss = 1.388\n",
      "Epoch  49 Batch  939/2739 train_loss = 1.333\n",
      "Epoch  49 Batch  989/2739 train_loss = 3.588\n",
      "Epoch  49 Batch 1039/2739 train_loss = 1.735\n",
      "Epoch  49 Batch 1089/2739 train_loss = 2.138\n",
      "Epoch  49 Batch 1139/2739 train_loss = 2.670\n",
      "Epoch  49 Batch 1189/2739 train_loss = 1.821\n",
      "Epoch  49 Batch 1239/2739 train_loss = 1.817\n",
      "Epoch  49 Batch 1289/2739 train_loss = 2.698\n",
      "Epoch  49 Batch 1339/2739 train_loss = 4.043\n",
      "Epoch  49 Batch 1389/2739 train_loss = 1.986\n",
      "Epoch  49 Batch 1439/2739 train_loss = 3.429\n",
      "Epoch  49 Batch 1489/2739 train_loss = 2.765\n",
      "Epoch  49 Batch 1539/2739 train_loss = 1.559\n",
      "Epoch  49 Batch 1589/2739 train_loss = 2.034\n",
      "Epoch  49 Batch 1639/2739 train_loss = 3.112\n",
      "Epoch  49 Batch 1689/2739 train_loss = 3.585\n",
      "Epoch  49 Batch 1739/2739 train_loss = 2.946\n",
      "Epoch  49 Batch 1789/2739 train_loss = 3.214\n",
      "Epoch  49 Batch 1839/2739 train_loss = 3.598\n",
      "Epoch  49 Batch 1889/2739 train_loss = 2.644\n",
      "Epoch  49 Batch 1939/2739 train_loss = 4.361\n",
      "Epoch  49 Batch 1989/2739 train_loss = 2.117\n",
      "Epoch  49 Batch 2039/2739 train_loss = 3.871\n",
      "Epoch  49 Batch 2089/2739 train_loss = 3.019\n",
      "Epoch  49 Batch 2139/2739 train_loss = 2.277\n",
      "Epoch  49 Batch 2189/2739 train_loss = 4.046\n",
      "Epoch  49 Batch 2239/2739 train_loss = 2.393\n",
      "Epoch  49 Batch 2289/2739 train_loss = 4.137\n",
      "Epoch  49 Batch 2339/2739 train_loss = 4.013\n",
      "Epoch  49 Batch 2389/2739 train_loss = 3.211\n",
      "Epoch  49 Batch 2439/2739 train_loss = 4.263\n",
      "Epoch  49 Batch 2489/2739 train_loss = 2.558\n",
      "Epoch  49 Batch 2539/2739 train_loss = 1.402\n",
      "Epoch  49 Batch 2589/2739 train_loss = 1.249\n",
      "Epoch  49 Batch 2639/2739 train_loss = 4.739\n",
      "Epoch  49 Batch 2689/2739 train_loss = 1.660\n",
      "Epoch  50 Batch    0/2739 train_loss = 1.922\n",
      "Epoch  50 Batch   50/2739 train_loss = 2.576\n",
      "Epoch  50 Batch  100/2739 train_loss = 2.566\n",
      "Epoch  50 Batch  150/2739 train_loss = 3.300\n",
      "Epoch  50 Batch  200/2739 train_loss = 2.417\n",
      "Epoch  50 Batch  250/2739 train_loss = 1.990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  50 Batch  300/2739 train_loss = 2.553\n",
      "Epoch  50 Batch  350/2739 train_loss = 2.477\n",
      "Epoch  50 Batch  400/2739 train_loss = 1.653\n",
      "Epoch  50 Batch  450/2739 train_loss = 2.355\n",
      "Epoch  50 Batch  500/2739 train_loss = 2.776\n",
      "Epoch  50 Batch  550/2739 train_loss = 2.365\n",
      "Epoch  50 Batch  600/2739 train_loss = 1.400\n",
      "Epoch  50 Batch  650/2739 train_loss = 2.056\n",
      "Epoch  50 Batch  700/2739 train_loss = 2.619\n",
      "Epoch  50 Batch  750/2739 train_loss = 3.191\n",
      "Epoch  50 Batch  800/2739 train_loss = 1.609\n",
      "Epoch  50 Batch  850/2739 train_loss = 2.211\n",
      "Epoch  50 Batch  900/2739 train_loss = 2.596\n",
      "Epoch  50 Batch  950/2739 train_loss = 1.496\n",
      "Epoch  50 Batch 1000/2739 train_loss = 2.113\n",
      "Epoch  50 Batch 1050/2739 train_loss = 3.484\n",
      "Epoch  50 Batch 1100/2739 train_loss = 3.283\n",
      "Epoch  50 Batch 1150/2739 train_loss = 2.368\n",
      "Epoch  50 Batch 1200/2739 train_loss = 1.511\n",
      "Epoch  50 Batch 1250/2739 train_loss = 1.628\n",
      "Epoch  50 Batch 1300/2739 train_loss = 1.966\n",
      "Epoch  50 Batch 1350/2739 train_loss = 2.163\n",
      "Epoch  50 Batch 1400/2739 train_loss = 3.789\n",
      "Epoch  50 Batch 1450/2739 train_loss = 2.000\n",
      "Epoch  50 Batch 1500/2739 train_loss = 2.230\n",
      "Epoch  50 Batch 1550/2739 train_loss = 2.660\n",
      "Epoch  50 Batch 1600/2739 train_loss = 3.939\n",
      "Epoch  50 Batch 1650/2739 train_loss = 2.635\n",
      "Epoch  50 Batch 1700/2739 train_loss = 2.750\n",
      "Epoch  50 Batch 1750/2739 train_loss = 3.210\n",
      "Epoch  50 Batch 1800/2739 train_loss = 2.084\n",
      "Epoch  50 Batch 1850/2739 train_loss = 1.083\n",
      "Epoch  50 Batch 1900/2739 train_loss = 1.416\n",
      "Epoch  50 Batch 1950/2739 train_loss = 1.433\n",
      "Epoch  50 Batch 2000/2739 train_loss = 2.485\n",
      "Epoch  50 Batch 2050/2739 train_loss = 3.952\n",
      "Epoch  50 Batch 2100/2739 train_loss = 3.202\n",
      "Epoch  50 Batch 2150/2739 train_loss = 3.761\n",
      "Epoch  50 Batch 2200/2739 train_loss = 3.695\n",
      "Epoch  50 Batch 2250/2739 train_loss = 1.983\n",
      "Epoch  50 Batch 2300/2739 train_loss = 3.128\n",
      "Epoch  50 Batch 2350/2739 train_loss = 2.657\n",
      "Epoch  50 Batch 2400/2739 train_loss = 2.718\n",
      "Epoch  50 Batch 2450/2739 train_loss = 2.539\n",
      "Epoch  50 Batch 2500/2739 train_loss = 3.578\n",
      "Epoch  50 Batch 2550/2739 train_loss = 2.344\n",
      "Epoch  50 Batch 2600/2739 train_loss = 1.276\n",
      "Epoch  50 Batch 2650/2739 train_loss = 2.314\n",
      "Epoch  50 Batch 2700/2739 train_loss = 3.758\n",
      "Epoch  51 Batch   11/2739 train_loss = 1.246\n",
      "Epoch  51 Batch   61/2739 train_loss = 2.713\n",
      "Epoch  51 Batch  111/2739 train_loss = 2.263\n",
      "Epoch  51 Batch  161/2739 train_loss = 2.534\n",
      "Epoch  51 Batch  211/2739 train_loss = 2.679\n",
      "Epoch  51 Batch  261/2739 train_loss = 1.775\n",
      "Epoch  51 Batch  311/2739 train_loss = 2.794\n",
      "Epoch  51 Batch  361/2739 train_loss = 1.665\n",
      "Epoch  51 Batch  411/2739 train_loss = 1.993\n",
      "Epoch  51 Batch  461/2739 train_loss = 2.490\n",
      "Epoch  51 Batch  511/2739 train_loss = 2.206\n",
      "Epoch  51 Batch  561/2739 train_loss = 1.915\n",
      "Epoch  51 Batch  611/2739 train_loss = 1.985\n",
      "Epoch  51 Batch  661/2739 train_loss = 1.763\n",
      "Epoch  51 Batch  711/2739 train_loss = 3.115\n",
      "Epoch  51 Batch  761/2739 train_loss = 3.529\n",
      "Epoch  51 Batch  811/2739 train_loss = 2.445\n",
      "Epoch  51 Batch  861/2739 train_loss = 2.670\n",
      "Epoch  51 Batch  911/2739 train_loss = 2.667\n",
      "Epoch  51 Batch  961/2739 train_loss = 3.992\n",
      "Epoch  51 Batch 1011/2739 train_loss = 2.063\n",
      "Epoch  51 Batch 1061/2739 train_loss = 1.078\n",
      "Epoch  51 Batch 1111/2739 train_loss = 2.451\n",
      "Epoch  51 Batch 1161/2739 train_loss = 1.787\n",
      "Epoch  51 Batch 1211/2739 train_loss = 1.619\n",
      "Epoch  51 Batch 1261/2739 train_loss = 1.603\n",
      "Epoch  51 Batch 1311/2739 train_loss = 2.759\n",
      "Epoch  51 Batch 1361/2739 train_loss = 1.541\n",
      "Epoch  51 Batch 1411/2739 train_loss = 4.086\n",
      "Epoch  51 Batch 1461/2739 train_loss = 1.877\n",
      "Epoch  51 Batch 1511/2739 train_loss = 1.535\n",
      "Epoch  51 Batch 1561/2739 train_loss = 3.446\n",
      "Epoch  51 Batch 1611/2739 train_loss = 2.229\n",
      "Epoch  51 Batch 1661/2739 train_loss = 2.028\n",
      "Epoch  51 Batch 1711/2739 train_loss = 1.797\n",
      "Epoch  51 Batch 1761/2739 train_loss = 4.066\n",
      "Epoch  51 Batch 1811/2739 train_loss = 2.709\n",
      "Epoch  51 Batch 1861/2739 train_loss = 3.695\n",
      "Epoch  51 Batch 1911/2739 train_loss = 3.810\n",
      "Epoch  51 Batch 1961/2739 train_loss = 2.163\n",
      "Epoch  51 Batch 2011/2739 train_loss = 3.774\n",
      "Epoch  51 Batch 2061/2739 train_loss = 1.864\n",
      "Epoch  51 Batch 2111/2739 train_loss = 3.079\n",
      "Epoch  51 Batch 2161/2739 train_loss = 2.436\n",
      "Epoch  51 Batch 2211/2739 train_loss = 2.001\n",
      "Epoch  51 Batch 2261/2739 train_loss = 2.743\n",
      "Epoch  51 Batch 2311/2739 train_loss = 2.801\n",
      "Epoch  51 Batch 2361/2739 train_loss = 2.247\n",
      "Epoch  51 Batch 2411/2739 train_loss = 2.548\n",
      "Epoch  51 Batch 2461/2739 train_loss = 2.474\n",
      "Epoch  51 Batch 2511/2739 train_loss = 2.661\n",
      "Epoch  51 Batch 2561/2739 train_loss = 2.353\n",
      "Epoch  51 Batch 2611/2739 train_loss = 1.828\n",
      "Epoch  51 Batch 2661/2739 train_loss = 1.441\n",
      "Epoch  51 Batch 2711/2739 train_loss = 3.229\n",
      "Epoch  52 Batch   22/2739 train_loss = 2.370\n",
      "Epoch  52 Batch   72/2739 train_loss = 2.814\n",
      "Epoch  52 Batch  122/2739 train_loss = 2.620\n",
      "Epoch  52 Batch  172/2739 train_loss = 2.088\n",
      "Epoch  52 Batch  222/2739 train_loss = 1.609\n",
      "Epoch  52 Batch  272/2739 train_loss = 2.545\n",
      "Epoch  52 Batch  322/2739 train_loss = 3.447\n",
      "Epoch  52 Batch  372/2739 train_loss = 1.793\n",
      "Epoch  52 Batch  422/2739 train_loss = 3.042\n",
      "Epoch  52 Batch  472/2739 train_loss = 1.315\n",
      "Epoch  52 Batch  522/2739 train_loss = 1.287\n",
      "Epoch  52 Batch  572/2739 train_loss = 2.106\n",
      "Epoch  52 Batch  622/2739 train_loss = 2.481\n",
      "Epoch  52 Batch  672/2739 train_loss = 3.154\n",
      "Epoch  52 Batch  722/2739 train_loss = 2.028\n",
      "Epoch  52 Batch  772/2739 train_loss = 3.492\n",
      "Epoch  52 Batch  822/2739 train_loss = 1.794\n",
      "Epoch  52 Batch  872/2739 train_loss = 2.124\n",
      "Epoch  52 Batch  922/2739 train_loss = 1.872\n",
      "Epoch  52 Batch  972/2739 train_loss = 3.666\n",
      "Epoch  52 Batch 1022/2739 train_loss = 2.049\n",
      "Epoch  52 Batch 1072/2739 train_loss = 3.406\n",
      "Epoch  52 Batch 1122/2739 train_loss = 3.614\n",
      "Epoch  52 Batch 1172/2739 train_loss = 4.085\n",
      "Epoch  52 Batch 1222/2739 train_loss = 3.405\n",
      "Epoch  52 Batch 1272/2739 train_loss = 2.837\n",
      "Epoch  52 Batch 1322/2739 train_loss = 2.107\n",
      "Epoch  52 Batch 1372/2739 train_loss = 2.021\n",
      "Epoch  52 Batch 1422/2739 train_loss = 2.276\n",
      "Epoch  52 Batch 1472/2739 train_loss = 1.777\n",
      "Epoch  52 Batch 1522/2739 train_loss = 3.680\n",
      "Epoch  52 Batch 1572/2739 train_loss = 2.342\n",
      "Epoch  52 Batch 1622/2739 train_loss = 3.633\n",
      "Epoch  52 Batch 1672/2739 train_loss = 2.009\n",
      "Epoch  52 Batch 1722/2739 train_loss = 3.210\n",
      "Epoch  52 Batch 1772/2739 train_loss = 1.981\n",
      "Epoch  52 Batch 1822/2739 train_loss = 2.883\n",
      "Epoch  52 Batch 1872/2739 train_loss = 3.654\n",
      "Epoch  52 Batch 1922/2739 train_loss = 1.768\n",
      "Epoch  52 Batch 1972/2739 train_loss = 1.343\n",
      "Epoch  52 Batch 2022/2739 train_loss = 1.911\n",
      "Epoch  52 Batch 2072/2739 train_loss = 2.033\n",
      "Epoch  52 Batch 2122/2739 train_loss = 1.028\n",
      "Epoch  52 Batch 2172/2739 train_loss = 2.531\n",
      "Epoch  52 Batch 2222/2739 train_loss = 1.943\n",
      "Epoch  52 Batch 2272/2739 train_loss = 1.860\n",
      "Epoch  52 Batch 2322/2739 train_loss = 4.457\n",
      "Epoch  52 Batch 2372/2739 train_loss = 2.623\n",
      "Epoch  52 Batch 2422/2739 train_loss = 1.729\n",
      "Epoch  52 Batch 2472/2739 train_loss = 2.698\n",
      "Epoch  52 Batch 2522/2739 train_loss = 3.449\n",
      "Epoch  52 Batch 2572/2739 train_loss = 1.627\n",
      "Epoch  52 Batch 2622/2739 train_loss = 3.025\n",
      "Epoch  52 Batch 2672/2739 train_loss = 2.290\n",
      "Epoch  52 Batch 2722/2739 train_loss = 3.862\n",
      "Epoch  53 Batch   33/2739 train_loss = 2.693\n",
      "Epoch  53 Batch   83/2739 train_loss = 1.930\n",
      "Epoch  53 Batch  133/2739 train_loss = 2.294\n",
      "Epoch  53 Batch  183/2739 train_loss = 2.101\n",
      "Epoch  53 Batch  233/2739 train_loss = 2.892\n",
      "Epoch  53 Batch  283/2739 train_loss = 1.637\n",
      "Epoch  53 Batch  333/2739 train_loss = 2.906\n",
      "Epoch  53 Batch  383/2739 train_loss = 2.214\n",
      "Epoch  53 Batch  433/2739 train_loss = 2.864\n",
      "Epoch  53 Batch  483/2739 train_loss = 1.608\n",
      "Epoch  53 Batch  533/2739 train_loss = 2.398\n",
      "Epoch  53 Batch  583/2739 train_loss = 1.266\n",
      "Epoch  53 Batch  633/2739 train_loss = 2.047\n",
      "Epoch  53 Batch  683/2739 train_loss = 1.324\n",
      "Epoch  53 Batch  733/2739 train_loss = 3.275\n",
      "Epoch  53 Batch  783/2739 train_loss = 2.353\n",
      "Epoch  53 Batch  833/2739 train_loss = 3.035\n",
      "Epoch  53 Batch  883/2739 train_loss = 3.474\n",
      "Epoch  53 Batch  933/2739 train_loss = 1.996\n",
      "Epoch  53 Batch  983/2739 train_loss = 3.499\n",
      "Epoch  53 Batch 1033/2739 train_loss = 1.330\n",
      "Epoch  53 Batch 1083/2739 train_loss = 1.544\n",
      "Epoch  53 Batch 1133/2739 train_loss = 3.066\n",
      "Epoch  53 Batch 1183/2739 train_loss = 2.103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  53 Batch 1233/2739 train_loss = 1.665\n",
      "Epoch  53 Batch 1283/2739 train_loss = 4.290\n",
      "Epoch  53 Batch 1333/2739 train_loss = 3.984\n",
      "Epoch  53 Batch 1383/2739 train_loss = 2.552\n",
      "Epoch  53 Batch 1433/2739 train_loss = 1.871\n",
      "Epoch  53 Batch 1483/2739 train_loss = 3.521\n",
      "Epoch  53 Batch 1533/2739 train_loss = 2.145\n",
      "Epoch  53 Batch 1583/2739 train_loss = 2.281\n",
      "Epoch  53 Batch 1633/2739 train_loss = 1.409\n",
      "Epoch  53 Batch 1683/2739 train_loss = 2.192\n",
      "Epoch  53 Batch 1733/2739 train_loss = 3.202\n",
      "Epoch  53 Batch 1783/2739 train_loss = 3.703\n",
      "Epoch  53 Batch 1833/2739 train_loss = 3.122\n",
      "Epoch  53 Batch 1883/2739 train_loss = 3.580\n",
      "Epoch  53 Batch 1933/2739 train_loss = 3.280\n",
      "Epoch  53 Batch 1983/2739 train_loss = 4.187\n",
      "Epoch  53 Batch 2033/2739 train_loss = 4.148\n",
      "Epoch  53 Batch 2083/2739 train_loss = 3.299\n",
      "Epoch  53 Batch 2133/2739 train_loss = 3.292\n",
      "Epoch  53 Batch 2183/2739 train_loss = 2.436\n",
      "Epoch  53 Batch 2233/2739 train_loss = 2.528\n",
      "Epoch  53 Batch 2283/2739 train_loss = 2.315\n",
      "Epoch  53 Batch 2333/2739 train_loss = 2.343\n",
      "Epoch  53 Batch 2383/2739 train_loss = 1.481\n",
      "Epoch  53 Batch 2433/2739 train_loss = 3.884\n",
      "Epoch  53 Batch 2483/2739 train_loss = 1.325\n",
      "Epoch  53 Batch 2533/2739 train_loss = 2.762\n",
      "Epoch  53 Batch 2583/2739 train_loss = 2.135\n",
      "Epoch  53 Batch 2633/2739 train_loss = 2.155\n",
      "Epoch  53 Batch 2683/2739 train_loss = 1.526\n",
      "Epoch  53 Batch 2733/2739 train_loss = 1.286\n",
      "Epoch  54 Batch   44/2739 train_loss = 1.818\n",
      "Epoch  54 Batch   94/2739 train_loss = 2.166\n",
      "Epoch  54 Batch  144/2739 train_loss = 2.632\n",
      "Epoch  54 Batch  194/2739 train_loss = 2.941\n",
      "Epoch  54 Batch  244/2739 train_loss = 2.802\n",
      "Epoch  54 Batch  294/2739 train_loss = 2.497\n",
      "Epoch  54 Batch  344/2739 train_loss = 2.070\n",
      "Epoch  54 Batch  394/2739 train_loss = 2.281\n",
      "Epoch  54 Batch  444/2739 train_loss = 1.957\n",
      "Epoch  54 Batch  494/2739 train_loss = 1.453\n",
      "Epoch  54 Batch  544/2739 train_loss = 2.400\n",
      "Epoch  54 Batch  594/2739 train_loss = 3.015\n",
      "Epoch  54 Batch  644/2739 train_loss = 1.311\n",
      "Epoch  54 Batch  694/2739 train_loss = 1.550\n",
      "Epoch  54 Batch  744/2739 train_loss = 2.072\n",
      "Epoch  54 Batch  794/2739 train_loss = 2.400\n",
      "Epoch  54 Batch  844/2739 train_loss = 3.032\n",
      "Epoch  54 Batch  894/2739 train_loss = 2.140\n",
      "Epoch  54 Batch  944/2739 train_loss = 1.936\n",
      "Epoch  54 Batch  994/2739 train_loss = 2.315\n",
      "Epoch  54 Batch 1044/2739 train_loss = 2.189\n",
      "Epoch  54 Batch 1094/2739 train_loss = 1.841\n",
      "Epoch  54 Batch 1144/2739 train_loss = 3.609\n",
      "Epoch  54 Batch 1194/2739 train_loss = 3.757\n",
      "Epoch  54 Batch 1244/2739 train_loss = 1.596\n",
      "Epoch  54 Batch 1294/2739 train_loss = 0.858\n",
      "Epoch  54 Batch 1344/2739 train_loss = 3.632\n",
      "Epoch  54 Batch 1394/2739 train_loss = 3.096\n",
      "Epoch  54 Batch 1444/2739 train_loss = 1.385\n",
      "Epoch  54 Batch 1494/2739 train_loss = 2.501\n",
      "Epoch  54 Batch 1544/2739 train_loss = 2.638\n",
      "Epoch  54 Batch 1594/2739 train_loss = 2.243\n",
      "Epoch  54 Batch 1644/2739 train_loss = 3.588\n",
      "Epoch  54 Batch 1694/2739 train_loss = 1.948\n",
      "Epoch  54 Batch 1744/2739 train_loss = 2.148\n",
      "Epoch  54 Batch 1794/2739 train_loss = 2.457\n",
      "Epoch  54 Batch 1844/2739 train_loss = 2.538\n",
      "Epoch  54 Batch 1894/2739 train_loss = 3.419\n",
      "Epoch  54 Batch 1944/2739 train_loss = 3.324\n",
      "Epoch  54 Batch 1994/2739 train_loss = 1.551\n",
      "Epoch  54 Batch 2044/2739 train_loss = 2.775\n",
      "Epoch  54 Batch 2094/2739 train_loss = 1.692\n",
      "Epoch  54 Batch 2144/2739 train_loss = 3.527\n",
      "Epoch  54 Batch 2194/2739 train_loss = 3.779\n",
      "Epoch  54 Batch 2244/2739 train_loss = 1.611\n",
      "Epoch  54 Batch 2294/2739 train_loss = 1.877\n",
      "Epoch  54 Batch 2344/2739 train_loss = 4.273\n",
      "Epoch  54 Batch 2394/2739 train_loss = 3.356\n",
      "Epoch  54 Batch 2444/2739 train_loss = 1.974\n",
      "Epoch  54 Batch 2494/2739 train_loss = 2.089\n",
      "Epoch  54 Batch 2544/2739 train_loss = 1.460\n",
      "Epoch  54 Batch 2594/2739 train_loss = 1.599\n",
      "Epoch  54 Batch 2644/2739 train_loss = 1.460\n",
      "Epoch  54 Batch 2694/2739 train_loss = 2.110\n",
      "Epoch  55 Batch    5/2739 train_loss = 1.698\n",
      "Epoch  55 Batch   55/2739 train_loss = 1.681\n",
      "Epoch  55 Batch  105/2739 train_loss = 2.927\n",
      "Epoch  55 Batch  155/2739 train_loss = 2.425\n",
      "Epoch  55 Batch  205/2739 train_loss = 2.551\n",
      "Epoch  55 Batch  255/2739 train_loss = 1.913\n",
      "Epoch  55 Batch  305/2739 train_loss = 1.324\n",
      "Epoch  55 Batch  355/2739 train_loss = 1.705\n",
      "Epoch  55 Batch  405/2739 train_loss = 2.067\n",
      "Epoch  55 Batch  455/2739 train_loss = 3.102\n",
      "Epoch  55 Batch  505/2739 train_loss = 2.740\n",
      "Epoch  55 Batch  555/2739 train_loss = 2.594\n",
      "Epoch  55 Batch  605/2739 train_loss = 2.074\n",
      "Epoch  55 Batch  655/2739 train_loss = 1.227\n",
      "Epoch  55 Batch  705/2739 train_loss = 2.491\n",
      "Epoch  55 Batch  755/2739 train_loss = 3.264\n",
      "Epoch  55 Batch  805/2739 train_loss = 2.008\n",
      "Epoch  55 Batch  855/2739 train_loss = 2.422\n",
      "Epoch  55 Batch  905/2739 train_loss = 2.760\n",
      "Epoch  55 Batch  955/2739 train_loss = 3.603\n",
      "Epoch  55 Batch 1005/2739 train_loss = 2.227\n",
      "Epoch  55 Batch 1055/2739 train_loss = 3.768\n",
      "Epoch  55 Batch 1105/2739 train_loss = 1.457\n",
      "Epoch  55 Batch 1155/2739 train_loss = 3.148\n",
      "Epoch  55 Batch 1205/2739 train_loss = 2.297\n",
      "Epoch  55 Batch 1255/2739 train_loss = 3.754\n",
      "Epoch  55 Batch 1305/2739 train_loss = 2.401\n",
      "Epoch  55 Batch 1355/2739 train_loss = 2.539\n",
      "Epoch  55 Batch 1405/2739 train_loss = 3.212\n",
      "Epoch  55 Batch 1455/2739 train_loss = 1.269\n",
      "Epoch  55 Batch 1505/2739 train_loss = 1.144\n",
      "Epoch  55 Batch 1555/2739 train_loss = 2.601\n",
      "Epoch  55 Batch 1605/2739 train_loss = 2.316\n",
      "Epoch  55 Batch 1655/2739 train_loss = 2.541\n",
      "Epoch  55 Batch 1705/2739 train_loss = 3.130\n",
      "Epoch  55 Batch 1755/2739 train_loss = 2.542\n",
      "Epoch  55 Batch 1805/2739 train_loss = 2.172\n",
      "Epoch  55 Batch 1855/2739 train_loss = 1.765\n",
      "Epoch  55 Batch 1905/2739 train_loss = 1.707\n",
      "Epoch  55 Batch 1955/2739 train_loss = 2.033\n",
      "Epoch  55 Batch 2005/2739 train_loss = 1.835\n",
      "Epoch  55 Batch 2055/2739 train_loss = 2.347\n",
      "Epoch  55 Batch 2105/2739 train_loss = 2.578\n",
      "Epoch  55 Batch 2155/2739 train_loss = 3.571\n",
      "Epoch  55 Batch 2205/2739 train_loss = 2.215\n",
      "Epoch  55 Batch 2255/2739 train_loss = 1.615\n",
      "Epoch  55 Batch 2305/2739 train_loss = 1.950\n",
      "Epoch  55 Batch 2355/2739 train_loss = 2.574\n",
      "Epoch  55 Batch 2405/2739 train_loss = 1.467\n",
      "Epoch  55 Batch 2455/2739 train_loss = 4.103\n",
      "Epoch  55 Batch 2505/2739 train_loss = 2.546\n",
      "Epoch  55 Batch 2555/2739 train_loss = 2.006\n",
      "Epoch  55 Batch 2605/2739 train_loss = 4.202\n",
      "Epoch  55 Batch 2655/2739 train_loss = 2.252\n",
      "Epoch  55 Batch 2705/2739 train_loss = 2.449\n",
      "Epoch  56 Batch   16/2739 train_loss = 1.421\n",
      "Epoch  56 Batch   66/2739 train_loss = 4.122\n",
      "Epoch  56 Batch  116/2739 train_loss = 2.571\n",
      "Epoch  56 Batch  166/2739 train_loss = 2.724\n",
      "Epoch  56 Batch  216/2739 train_loss = 1.798\n",
      "Epoch  56 Batch  266/2739 train_loss = 2.451\n",
      "Epoch  56 Batch  316/2739 train_loss = 2.783\n",
      "Epoch  56 Batch  366/2739 train_loss = 2.443\n",
      "Epoch  56 Batch  416/2739 train_loss = 2.793\n",
      "Epoch  56 Batch  466/2739 train_loss = 2.694\n",
      "Epoch  56 Batch  516/2739 train_loss = 2.398\n",
      "Epoch  56 Batch  566/2739 train_loss = 2.063\n",
      "Epoch  56 Batch  616/2739 train_loss = 4.013\n",
      "Epoch  56 Batch  666/2739 train_loss = 3.807\n",
      "Epoch  56 Batch  716/2739 train_loss = 2.218\n",
      "Epoch  56 Batch  766/2739 train_loss = 1.257\n",
      "Epoch  56 Batch  816/2739 train_loss = 3.087\n",
      "Epoch  56 Batch  866/2739 train_loss = 2.537\n",
      "Epoch  56 Batch  916/2739 train_loss = 3.226\n",
      "Epoch  56 Batch  966/2739 train_loss = 3.430\n",
      "Epoch  56 Batch 1016/2739 train_loss = 2.833\n",
      "Epoch  56 Batch 1066/2739 train_loss = 1.312\n",
      "Epoch  56 Batch 1116/2739 train_loss = 2.061\n",
      "Epoch  56 Batch 1166/2739 train_loss = 2.561\n",
      "Epoch  56 Batch 1216/2739 train_loss = 3.600\n",
      "Epoch  56 Batch 1266/2739 train_loss = 1.656\n",
      "Epoch  56 Batch 1316/2739 train_loss = 1.310\n",
      "Epoch  56 Batch 1366/2739 train_loss = 1.399\n",
      "Epoch  56 Batch 1416/2739 train_loss = 2.576\n",
      "Epoch  56 Batch 1466/2739 train_loss = 1.757\n",
      "Epoch  56 Batch 1516/2739 train_loss = 2.419\n",
      "Epoch  56 Batch 1566/2739 train_loss = 2.539\n",
      "Epoch  56 Batch 1616/2739 train_loss = 3.673\n",
      "Epoch  56 Batch 1666/2739 train_loss = 2.014\n",
      "Epoch  56 Batch 1716/2739 train_loss = 2.154\n",
      "Epoch  56 Batch 1766/2739 train_loss = 1.503\n",
      "Epoch  56 Batch 1816/2739 train_loss = 3.641\n",
      "Epoch  56 Batch 1866/2739 train_loss = 1.442\n",
      "Epoch  56 Batch 1916/2739 train_loss = 2.558\n",
      "Epoch  56 Batch 1966/2739 train_loss = 4.431\n",
      "Epoch  56 Batch 2016/2739 train_loss = 1.298\n",
      "Epoch  56 Batch 2066/2739 train_loss = 4.196\n",
      "Epoch  56 Batch 2116/2739 train_loss = 2.356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  56 Batch 2166/2739 train_loss = 3.669\n",
      "Epoch  56 Batch 2216/2739 train_loss = 3.044\n",
      "Epoch  56 Batch 2266/2739 train_loss = 4.307\n",
      "Epoch  56 Batch 2316/2739 train_loss = 4.160\n",
      "Epoch  56 Batch 2366/2739 train_loss = 2.335\n",
      "Epoch  56 Batch 2416/2739 train_loss = 4.046\n",
      "Epoch  56 Batch 2466/2739 train_loss = 1.079\n",
      "Epoch  56 Batch 2516/2739 train_loss = 1.918\n",
      "Epoch  56 Batch 2566/2739 train_loss = 3.975\n",
      "Epoch  56 Batch 2616/2739 train_loss = 3.092\n",
      "Epoch  56 Batch 2666/2739 train_loss = 3.470\n",
      "Epoch  56 Batch 2716/2739 train_loss = 2.015\n",
      "Epoch  57 Batch   27/2739 train_loss = 1.728\n",
      "Epoch  57 Batch   77/2739 train_loss = 1.953\n",
      "Epoch  57 Batch  127/2739 train_loss = 3.161\n",
      "Epoch  57 Batch  177/2739 train_loss = 2.257\n",
      "Epoch  57 Batch  227/2739 train_loss = 1.942\n",
      "Epoch  57 Batch  277/2739 train_loss = 2.425\n",
      "Epoch  57 Batch  327/2739 train_loss = 3.818\n",
      "Epoch  57 Batch  377/2739 train_loss = 2.288\n",
      "Epoch  57 Batch  427/2739 train_loss = 2.796\n",
      "Epoch  57 Batch  477/2739 train_loss = 2.479\n",
      "Epoch  57 Batch  527/2739 train_loss = 2.223\n",
      "Epoch  57 Batch  577/2739 train_loss = 3.654\n",
      "Epoch  57 Batch  627/2739 train_loss = 3.599\n",
      "Epoch  57 Batch  677/2739 train_loss = 2.184\n",
      "Epoch  57 Batch  727/2739 train_loss = 2.806\n",
      "Epoch  57 Batch  777/2739 train_loss = 3.263\n",
      "Epoch  57 Batch  827/2739 train_loss = 2.368\n",
      "Epoch  57 Batch  877/2739 train_loss = 1.614\n",
      "Epoch  57 Batch  927/2739 train_loss = 1.844\n",
      "Epoch  57 Batch  977/2739 train_loss = 2.733\n",
      "Epoch  57 Batch 1027/2739 train_loss = 1.661\n",
      "Epoch  57 Batch 1077/2739 train_loss = 2.445\n",
      "Epoch  57 Batch 1127/2739 train_loss = 1.401\n",
      "Epoch  57 Batch 1177/2739 train_loss = 2.035\n",
      "Epoch  57 Batch 1227/2739 train_loss = 3.297\n",
      "Epoch  57 Batch 1277/2739 train_loss = 2.682\n",
      "Epoch  57 Batch 1327/2739 train_loss = 2.430\n",
      "Epoch  57 Batch 1377/2739 train_loss = 2.332\n",
      "Epoch  57 Batch 1427/2739 train_loss = 3.101\n",
      "Epoch  57 Batch 1477/2739 train_loss = 2.316\n",
      "Epoch  57 Batch 1527/2739 train_loss = 1.615\n",
      "Epoch  57 Batch 1577/2739 train_loss = 2.531\n",
      "Epoch  57 Batch 1627/2739 train_loss = 3.806\n",
      "Epoch  57 Batch 1677/2739 train_loss = 3.673\n",
      "Epoch  57 Batch 1727/2739 train_loss = 3.063\n",
      "Epoch  57 Batch 1777/2739 train_loss = 3.953\n",
      "Epoch  57 Batch 1827/2739 train_loss = 2.438\n",
      "Epoch  57 Batch 1877/2739 train_loss = 2.700\n",
      "Epoch  57 Batch 1927/2739 train_loss = 1.302\n",
      "Epoch  57 Batch 1977/2739 train_loss = 1.738\n",
      "Epoch  57 Batch 2027/2739 train_loss = 3.929\n",
      "Epoch  57 Batch 2077/2739 train_loss = 3.823\n",
      "Epoch  57 Batch 2127/2739 train_loss = 2.078\n",
      "Epoch  57 Batch 2177/2739 train_loss = 2.516\n",
      "Epoch  57 Batch 2227/2739 train_loss = 1.557\n",
      "Epoch  57 Batch 2277/2739 train_loss = 3.558\n",
      "Epoch  57 Batch 2327/2739 train_loss = 2.073\n",
      "Epoch  57 Batch 2377/2739 train_loss = 1.577\n",
      "Epoch  57 Batch 2427/2739 train_loss = 2.381\n",
      "Epoch  57 Batch 2477/2739 train_loss = 2.058\n",
      "Epoch  57 Batch 2527/2739 train_loss = 2.578\n",
      "Epoch  57 Batch 2577/2739 train_loss = 2.484\n",
      "Epoch  57 Batch 2627/2739 train_loss = 2.548\n",
      "Epoch  57 Batch 2677/2739 train_loss = 0.949\n",
      "Epoch  57 Batch 2727/2739 train_loss = 2.068\n",
      "Epoch  58 Batch   38/2739 train_loss = 1.818\n",
      "Epoch  58 Batch   88/2739 train_loss = 2.378\n",
      "Epoch  58 Batch  138/2739 train_loss = 1.649\n",
      "Epoch  58 Batch  188/2739 train_loss = 2.772\n",
      "Epoch  58 Batch  238/2739 train_loss = 1.864\n",
      "Epoch  58 Batch  288/2739 train_loss = 2.170\n",
      "Epoch  58 Batch  338/2739 train_loss = 1.996\n",
      "Epoch  58 Batch  388/2739 train_loss = 1.897\n",
      "Epoch  58 Batch  438/2739 train_loss = 3.108\n",
      "Epoch  58 Batch  488/2739 train_loss = 3.339\n",
      "Epoch  58 Batch  538/2739 train_loss = 2.951\n",
      "Epoch  58 Batch  588/2739 train_loss = 3.974\n",
      "Epoch  58 Batch  638/2739 train_loss = 2.844\n",
      "Epoch  58 Batch  688/2739 train_loss = 2.088\n",
      "Epoch  58 Batch  738/2739 train_loss = 1.842\n",
      "Epoch  58 Batch  788/2739 train_loss = 2.871\n",
      "Epoch  58 Batch  838/2739 train_loss = 2.719\n",
      "Epoch  58 Batch  888/2739 train_loss = 2.300\n",
      "Epoch  58 Batch  938/2739 train_loss = 2.717\n",
      "Epoch  58 Batch  988/2739 train_loss = 3.621\n",
      "Epoch  58 Batch 1038/2739 train_loss = 2.827\n",
      "Epoch  58 Batch 1088/2739 train_loss = 2.109\n",
      "Epoch  58 Batch 1138/2739 train_loss = 2.646\n",
      "Epoch  58 Batch 1188/2739 train_loss = 1.659\n",
      "Epoch  58 Batch 1238/2739 train_loss = 1.546\n",
      "Epoch  58 Batch 1288/2739 train_loss = 3.315\n",
      "Epoch  58 Batch 1338/2739 train_loss = 4.304\n",
      "Epoch  58 Batch 1388/2739 train_loss = 2.785\n",
      "Epoch  58 Batch 1438/2739 train_loss = 2.906\n",
      "Epoch  58 Batch 1488/2739 train_loss = 4.083\n",
      "Epoch  58 Batch 1538/2739 train_loss = 2.031\n",
      "Epoch  58 Batch 1588/2739 train_loss = 2.311\n",
      "Epoch  58 Batch 1638/2739 train_loss = 2.141\n",
      "Epoch  58 Batch 1688/2739 train_loss = 3.429\n",
      "Epoch  58 Batch 1738/2739 train_loss = 1.895\n",
      "Epoch  58 Batch 1788/2739 train_loss = 2.660\n",
      "Epoch  58 Batch 1838/2739 train_loss = 2.859\n",
      "Epoch  58 Batch 1888/2739 train_loss = 3.776\n",
      "Epoch  58 Batch 1938/2739 train_loss = 2.422\n",
      "Epoch  58 Batch 1988/2739 train_loss = 1.739\n",
      "Epoch  58 Batch 2038/2739 train_loss = 3.469\n",
      "Epoch  58 Batch 2088/2739 train_loss = 3.964\n",
      "Epoch  58 Batch 2138/2739 train_loss = 3.633\n",
      "Epoch  58 Batch 2188/2739 train_loss = 3.971\n",
      "Epoch  58 Batch 2238/2739 train_loss = 4.046\n",
      "Epoch  58 Batch 2288/2739 train_loss = 4.339\n",
      "Epoch  58 Batch 2338/2739 train_loss = 1.280\n",
      "Epoch  58 Batch 2388/2739 train_loss = 1.547\n",
      "Epoch  58 Batch 2438/2739 train_loss = 4.398\n",
      "Epoch  58 Batch 2488/2739 train_loss = 2.142\n",
      "Epoch  58 Batch 2538/2739 train_loss = 2.447\n",
      "Epoch  58 Batch 2588/2739 train_loss = 3.870\n",
      "Epoch  58 Batch 2638/2739 train_loss = 2.091\n",
      "Epoch  58 Batch 2688/2739 train_loss = 1.839\n",
      "Epoch  58 Batch 2738/2739 train_loss = 3.978\n",
      "Epoch  59 Batch   49/2739 train_loss = 1.765\n",
      "Epoch  59 Batch   99/2739 train_loss = 2.440\n",
      "Epoch  59 Batch  149/2739 train_loss = 3.332\n",
      "Epoch  59 Batch  199/2739 train_loss = 1.270\n",
      "Epoch  59 Batch  249/2739 train_loss = 1.749\n",
      "Epoch  59 Batch  299/2739 train_loss = 2.355\n",
      "Epoch  59 Batch  349/2739 train_loss = 3.540\n",
      "Epoch  59 Batch  399/2739 train_loss = 2.175\n",
      "Epoch  59 Batch  449/2739 train_loss = 2.894\n",
      "Epoch  59 Batch  499/2739 train_loss = 3.443\n",
      "Epoch  59 Batch  549/2739 train_loss = 1.144\n",
      "Epoch  59 Batch  599/2739 train_loss = 1.704\n",
      "Epoch  59 Batch  649/2739 train_loss = 2.104\n",
      "Epoch  59 Batch  699/2739 train_loss = 1.702\n",
      "Epoch  59 Batch  749/2739 train_loss = 2.282\n",
      "Epoch  59 Batch  799/2739 train_loss = 2.193\n",
      "Epoch  59 Batch  849/2739 train_loss = 1.922\n",
      "Epoch  59 Batch  899/2739 train_loss = 2.441\n",
      "Epoch  59 Batch  949/2739 train_loss = 2.732\n",
      "Epoch  59 Batch  999/2739 train_loss = 1.817\n",
      "Epoch  59 Batch 1049/2739 train_loss = 2.785\n",
      "Epoch  59 Batch 1099/2739 train_loss = 2.675\n",
      "Epoch  59 Batch 1149/2739 train_loss = 3.542\n",
      "Epoch  59 Batch 1199/2739 train_loss = 1.972\n",
      "Epoch  59 Batch 1249/2739 train_loss = 1.966\n",
      "Epoch  59 Batch 1299/2739 train_loss = 1.670\n",
      "Epoch  59 Batch 1349/2739 train_loss = 2.713\n",
      "Epoch  59 Batch 1399/2739 train_loss = 1.575\n",
      "Epoch  59 Batch 1449/2739 train_loss = 2.462\n",
      "Epoch  59 Batch 1499/2739 train_loss = 1.617\n",
      "Epoch  59 Batch 1549/2739 train_loss = 4.375\n",
      "Epoch  59 Batch 1599/2739 train_loss = 2.844\n",
      "Epoch  59 Batch 1649/2739 train_loss = 3.144\n",
      "Epoch  59 Batch 1699/2739 train_loss = 2.458\n",
      "Epoch  59 Batch 1749/2739 train_loss = 3.278\n",
      "Epoch  59 Batch 1799/2739 train_loss = 2.034\n",
      "Epoch  59 Batch 1849/2739 train_loss = 2.072\n",
      "Epoch  59 Batch 1899/2739 train_loss = 2.766\n",
      "Epoch  59 Batch 1949/2739 train_loss = 2.187\n",
      "Epoch  59 Batch 1999/2739 train_loss = 2.780\n",
      "Epoch  59 Batch 2049/2739 train_loss = 2.396\n",
      "Epoch  59 Batch 2099/2739 train_loss = 2.741\n",
      "Epoch  59 Batch 2149/2739 train_loss = 2.716\n",
      "Epoch  59 Batch 2199/2739 train_loss = 4.294\n",
      "Epoch  59 Batch 2249/2739 train_loss = 1.542\n",
      "Epoch  59 Batch 2299/2739 train_loss = 3.139\n",
      "Epoch  59 Batch 2349/2739 train_loss = 1.442\n",
      "Epoch  59 Batch 2399/2739 train_loss = 3.686\n",
      "Epoch  59 Batch 2449/2739 train_loss = 1.637\n",
      "Epoch  59 Batch 2499/2739 train_loss = 2.939\n",
      "Epoch  59 Batch 2549/2739 train_loss = 2.367\n",
      "Epoch  59 Batch 2599/2739 train_loss = 3.430\n",
      "Epoch  59 Batch 2649/2739 train_loss = 1.182\n",
      "Epoch  59 Batch 2699/2739 train_loss = 1.464\n",
      "Epoch  60 Batch   10/2739 train_loss = 2.191\n",
      "Epoch  60 Batch   60/2739 train_loss = 2.274\n",
      "Epoch  60 Batch  110/2739 train_loss = 2.238\n",
      "Epoch  60 Batch  160/2739 train_loss = 2.204\n",
      "Epoch  60 Batch  210/2739 train_loss = 1.726\n",
      "Epoch  60 Batch  260/2739 train_loss = 2.399\n",
      "Epoch  60 Batch  310/2739 train_loss = 2.798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  60 Batch  360/2739 train_loss = 2.640\n",
      "Epoch  60 Batch  410/2739 train_loss = 2.151\n",
      "Epoch  60 Batch  460/2739 train_loss = 1.608\n",
      "Epoch  60 Batch  510/2739 train_loss = 3.163\n",
      "Epoch  60 Batch  560/2739 train_loss = 2.501\n",
      "Epoch  60 Batch  610/2739 train_loss = 1.546\n",
      "Epoch  60 Batch  660/2739 train_loss = 2.368\n",
      "Epoch  60 Batch  710/2739 train_loss = 1.972\n",
      "Epoch  60 Batch  760/2739 train_loss = 3.370\n",
      "Epoch  60 Batch  810/2739 train_loss = 2.630\n",
      "Epoch  60 Batch  860/2739 train_loss = 3.494\n",
      "Epoch  60 Batch  910/2739 train_loss = 2.120\n",
      "Epoch  60 Batch  960/2739 train_loss = 4.167\n",
      "Epoch  60 Batch 1010/2739 train_loss = 1.390\n",
      "Epoch  60 Batch 1060/2739 train_loss = 2.924\n",
      "Epoch  60 Batch 1110/2739 train_loss = 1.656\n",
      "Epoch  60 Batch 1160/2739 train_loss = 1.364\n",
      "Epoch  60 Batch 1210/2739 train_loss = 2.034\n",
      "Epoch  60 Batch 1260/2739 train_loss = 1.967\n",
      "Epoch  60 Batch 1310/2739 train_loss = 2.788\n",
      "Epoch  60 Batch 1360/2739 train_loss = 1.817\n",
      "Epoch  60 Batch 1410/2739 train_loss = 4.039\n",
      "Epoch  60 Batch 1460/2739 train_loss = 2.869\n",
      "Epoch  60 Batch 1510/2739 train_loss = 1.292\n",
      "Epoch  60 Batch 1560/2739 train_loss = 2.461\n",
      "Epoch  60 Batch 1610/2739 train_loss = 3.583\n",
      "Epoch  60 Batch 1660/2739 train_loss = 2.317\n",
      "Epoch  60 Batch 1710/2739 train_loss = 2.093\n",
      "Epoch  60 Batch 1760/2739 train_loss = 2.296\n",
      "Epoch  60 Batch 1810/2739 train_loss = 1.677\n",
      "Epoch  60 Batch 1860/2739 train_loss = 1.896\n",
      "Epoch  60 Batch 1910/2739 train_loss = 1.967\n",
      "Epoch  60 Batch 1960/2739 train_loss = 1.906\n",
      "Epoch  60 Batch 2010/2739 train_loss = 2.070\n",
      "Epoch  60 Batch 2060/2739 train_loss = 4.194\n",
      "Epoch  60 Batch 2110/2739 train_loss = 2.348\n",
      "Epoch  60 Batch 2160/2739 train_loss = 1.929\n",
      "Epoch  60 Batch 2210/2739 train_loss = 2.869\n",
      "Epoch  60 Batch 2260/2739 train_loss = 4.344\n",
      "Epoch  60 Batch 2310/2739 train_loss = 2.996\n",
      "Epoch  60 Batch 2360/2739 train_loss = 2.506\n",
      "Epoch  60 Batch 2410/2739 train_loss = 2.562\n",
      "Epoch  60 Batch 2460/2739 train_loss = 2.135\n",
      "Epoch  60 Batch 2510/2739 train_loss = 1.875\n",
      "Epoch  60 Batch 2560/2739 train_loss = 1.785\n",
      "Epoch  60 Batch 2610/2739 train_loss = 4.201\n",
      "Epoch  60 Batch 2660/2739 train_loss = 2.132\n",
      "Epoch  60 Batch 2710/2739 train_loss = 2.572\n",
      "Epoch  61 Batch   21/2739 train_loss = 1.584\n",
      "Epoch  61 Batch   71/2739 train_loss = 2.377\n",
      "Epoch  61 Batch  121/2739 train_loss = 2.183\n",
      "Epoch  61 Batch  171/2739 train_loss = 2.793\n",
      "Epoch  61 Batch  221/2739 train_loss = 2.940\n",
      "Epoch  61 Batch  271/2739 train_loss = 1.967\n",
      "Epoch  61 Batch  321/2739 train_loss = 2.572\n",
      "Epoch  61 Batch  371/2739 train_loss = 2.214\n",
      "Epoch  61 Batch  421/2739 train_loss = 4.045\n",
      "Epoch  61 Batch  471/2739 train_loss = 2.540\n",
      "Epoch  61 Batch  521/2739 train_loss = 2.312\n",
      "Epoch  61 Batch  571/2739 train_loss = 1.905\n",
      "Epoch  61 Batch  621/2739 train_loss = 2.013\n",
      "Epoch  61 Batch  671/2739 train_loss = 2.410\n",
      "Epoch  61 Batch  721/2739 train_loss = 3.684\n",
      "Epoch  61 Batch  771/2739 train_loss = 2.085\n",
      "Epoch  61 Batch  821/2739 train_loss = 2.420\n",
      "Epoch  61 Batch  871/2739 train_loss = 2.153\n",
      "Epoch  61 Batch  921/2739 train_loss = 1.313\n",
      "Epoch  61 Batch  971/2739 train_loss = 2.097\n",
      "Epoch  61 Batch 1021/2739 train_loss = 1.026\n",
      "Epoch  61 Batch 1071/2739 train_loss = 2.793\n",
      "Epoch  61 Batch 1121/2739 train_loss = 3.484\n",
      "Epoch  61 Batch 1171/2739 train_loss = 2.797\n",
      "Epoch  61 Batch 1221/2739 train_loss = 3.287\n",
      "Epoch  61 Batch 1271/2739 train_loss = 2.823\n",
      "Epoch  61 Batch 1321/2739 train_loss = 2.329\n",
      "Epoch  61 Batch 1371/2739 train_loss = 2.471\n",
      "Epoch  61 Batch 1421/2739 train_loss = 1.500\n",
      "Epoch  61 Batch 1471/2739 train_loss = 1.469\n",
      "Epoch  61 Batch 1521/2739 train_loss = 1.647\n",
      "Epoch  61 Batch 1571/2739 train_loss = 3.632\n",
      "Epoch  61 Batch 1621/2739 train_loss = 1.863\n",
      "Epoch  61 Batch 1671/2739 train_loss = 2.319\n",
      "Epoch  61 Batch 1721/2739 train_loss = 2.225\n",
      "Epoch  61 Batch 1771/2739 train_loss = 2.636\n",
      "Epoch  61 Batch 1821/2739 train_loss = 2.260\n",
      "Epoch  61 Batch 1871/2739 train_loss = 3.825\n",
      "Epoch  61 Batch 1921/2739 train_loss = 1.669\n",
      "Epoch  61 Batch 1971/2739 train_loss = 3.933\n",
      "Epoch  61 Batch 2021/2739 train_loss = 2.650\n",
      "Epoch  61 Batch 2071/2739 train_loss = 3.391\n",
      "Epoch  61 Batch 2121/2739 train_loss = 1.714\n",
      "Epoch  61 Batch 2171/2739 train_loss = 2.544\n",
      "Epoch  61 Batch 2221/2739 train_loss = 2.836\n",
      "Epoch  61 Batch 2271/2739 train_loss = 2.460\n",
      "Epoch  61 Batch 2321/2739 train_loss = 4.244\n",
      "Epoch  61 Batch 2371/2739 train_loss = 2.796\n",
      "Epoch  61 Batch 2421/2739 train_loss = 3.555\n",
      "Epoch  61 Batch 2471/2739 train_loss = 1.965\n",
      "Epoch  61 Batch 2521/2739 train_loss = 2.583\n",
      "Epoch  61 Batch 2571/2739 train_loss = 2.202\n",
      "Epoch  61 Batch 2621/2739 train_loss = 2.415\n",
      "Epoch  61 Batch 2671/2739 train_loss = 2.031\n",
      "Epoch  61 Batch 2721/2739 train_loss = 2.469\n",
      "Epoch  62 Batch   32/2739 train_loss = 4.154\n",
      "Epoch  62 Batch   82/2739 train_loss = 1.794\n",
      "Epoch  62 Batch  132/2739 train_loss = 2.440\n",
      "Epoch  62 Batch  182/2739 train_loss = 2.149\n",
      "Epoch  62 Batch  232/2739 train_loss = 3.241\n",
      "Epoch  62 Batch  282/2739 train_loss = 2.633\n",
      "Epoch  62 Batch  332/2739 train_loss = 3.841\n",
      "Epoch  62 Batch  382/2739 train_loss = 1.904\n",
      "Epoch  62 Batch  432/2739 train_loss = 1.351\n",
      "Epoch  62 Batch  482/2739 train_loss = 2.187\n",
      "Epoch  62 Batch  532/2739 train_loss = 2.167\n",
      "Epoch  62 Batch  582/2739 train_loss = 2.089\n",
      "Epoch  62 Batch  632/2739 train_loss = 1.375\n",
      "Epoch  62 Batch  682/2739 train_loss = 1.429\n",
      "Epoch  62 Batch  732/2739 train_loss = 1.982\n",
      "Epoch  62 Batch  782/2739 train_loss = 1.697\n",
      "Epoch  62 Batch  832/2739 train_loss = 2.774\n",
      "Epoch  62 Batch  882/2739 train_loss = 2.555\n",
      "Epoch  62 Batch  932/2739 train_loss = 2.238\n",
      "Epoch  62 Batch  982/2739 train_loss = 3.140\n",
      "Epoch  62 Batch 1032/2739 train_loss = 1.904\n",
      "Epoch  62 Batch 1082/2739 train_loss = 3.349\n",
      "Epoch  62 Batch 1132/2739 train_loss = 2.872\n",
      "Epoch  62 Batch 1182/2739 train_loss = 1.377\n",
      "Epoch  62 Batch 1232/2739 train_loss = 3.366\n",
      "Epoch  62 Batch 1282/2739 train_loss = 2.400\n",
      "Epoch  62 Batch 1332/2739 train_loss = 2.167\n",
      "Epoch  62 Batch 1382/2739 train_loss = 1.781\n",
      "Epoch  62 Batch 1432/2739 train_loss = 3.231\n",
      "Epoch  62 Batch 1482/2739 train_loss = 2.241\n",
      "Epoch  62 Batch 1532/2739 train_loss = 2.076\n",
      "Epoch  62 Batch 1582/2739 train_loss = 1.863\n",
      "Epoch  62 Batch 1632/2739 train_loss = 2.430\n",
      "Epoch  62 Batch 1682/2739 train_loss = 2.311\n",
      "Epoch  62 Batch 1732/2739 train_loss = 3.835\n",
      "Epoch  62 Batch 1782/2739 train_loss = 1.929\n",
      "Epoch  62 Batch 1832/2739 train_loss = 1.557\n",
      "Epoch  62 Batch 1882/2739 train_loss = 3.767\n",
      "Epoch  62 Batch 1932/2739 train_loss = 4.296\n",
      "Epoch  62 Batch 1982/2739 train_loss = 1.619\n",
      "Epoch  62 Batch 2032/2739 train_loss = 3.678\n",
      "Epoch  62 Batch 2082/2739 train_loss = 1.671\n",
      "Epoch  62 Batch 2132/2739 train_loss = 2.472\n",
      "Epoch  62 Batch 2182/2739 train_loss = 2.618\n",
      "Epoch  62 Batch 2232/2739 train_loss = 1.922\n",
      "Epoch  62 Batch 2282/2739 train_loss = 4.101\n",
      "Epoch  62 Batch 2332/2739 train_loss = 1.500\n",
      "Epoch  62 Batch 2382/2739 train_loss = 1.489\n",
      "Epoch  62 Batch 2432/2739 train_loss = 3.814\n",
      "Epoch  62 Batch 2482/2739 train_loss = 1.995\n",
      "Epoch  62 Batch 2532/2739 train_loss = 2.633\n",
      "Epoch  62 Batch 2582/2739 train_loss = 2.542\n",
      "Epoch  62 Batch 2632/2739 train_loss = 1.481\n",
      "Epoch  62 Batch 2682/2739 train_loss = 2.139\n",
      "Epoch  62 Batch 2732/2739 train_loss = 1.490\n",
      "Epoch  63 Batch   43/2739 train_loss = 1.505\n",
      "Epoch  63 Batch   93/2739 train_loss = 1.489\n",
      "Epoch  63 Batch  143/2739 train_loss = 2.621\n",
      "Epoch  63 Batch  193/2739 train_loss = 2.094\n",
      "Epoch  63 Batch  243/2739 train_loss = 2.776\n",
      "Epoch  63 Batch  293/2739 train_loss = 2.798\n",
      "Epoch  63 Batch  343/2739 train_loss = 1.968\n",
      "Epoch  63 Batch  393/2739 train_loss = 1.683\n",
      "Epoch  63 Batch  443/2739 train_loss = 2.891\n",
      "Epoch  63 Batch  493/2739 train_loss = 2.575\n",
      "Epoch  63 Batch  543/2739 train_loss = 2.000\n",
      "Epoch  63 Batch  593/2739 train_loss = 2.010\n",
      "Epoch  63 Batch  643/2739 train_loss = 2.404\n",
      "Epoch  63 Batch  693/2739 train_loss = 2.032\n",
      "Epoch  63 Batch  743/2739 train_loss = 2.723\n",
      "Epoch  63 Batch  793/2739 train_loss = 3.907\n",
      "Epoch  63 Batch  843/2739 train_loss = 2.795\n",
      "Epoch  63 Batch  893/2739 train_loss = 1.795\n",
      "Epoch  63 Batch  943/2739 train_loss = 1.437\n",
      "Epoch  63 Batch  993/2739 train_loss = 3.230\n",
      "Epoch  63 Batch 1043/2739 train_loss = 2.602\n",
      "Epoch  63 Batch 1093/2739 train_loss = 1.602\n",
      "Epoch  63 Batch 1143/2739 train_loss = 1.907\n",
      "Epoch  63 Batch 1193/2739 train_loss = 1.893\n",
      "Epoch  63 Batch 1243/2739 train_loss = 3.045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  63 Batch 1293/2739 train_loss = 1.463\n",
      "Epoch  63 Batch 1343/2739 train_loss = 2.106\n",
      "Epoch  63 Batch 1393/2739 train_loss = 2.274\n",
      "Epoch  63 Batch 1443/2739 train_loss = 3.402\n",
      "Epoch  63 Batch 1493/2739 train_loss = 2.602\n",
      "Epoch  63 Batch 1543/2739 train_loss = 1.811\n",
      "Epoch  63 Batch 1593/2739 train_loss = 2.472\n",
      "Epoch  63 Batch 1643/2739 train_loss = 3.006\n",
      "Epoch  63 Batch 1693/2739 train_loss = 3.433\n",
      "Epoch  63 Batch 1743/2739 train_loss = 3.913\n",
      "Epoch  63 Batch 1793/2739 train_loss = 2.861\n",
      "Epoch  63 Batch 1843/2739 train_loss = 2.187\n",
      "Epoch  63 Batch 1893/2739 train_loss = 3.349\n",
      "Epoch  63 Batch 1943/2739 train_loss = 1.747\n",
      "Epoch  63 Batch 1993/2739 train_loss = 1.699\n",
      "Epoch  63 Batch 2043/2739 train_loss = 2.041\n",
      "Epoch  63 Batch 2093/2739 train_loss = 2.033\n",
      "Epoch  63 Batch 2143/2739 train_loss = 1.966\n",
      "Epoch  63 Batch 2193/2739 train_loss = 2.280\n",
      "Epoch  63 Batch 2243/2739 train_loss = 4.024\n",
      "Epoch  63 Batch 2293/2739 train_loss = 4.159\n",
      "Epoch  63 Batch 2343/2739 train_loss = 2.602\n",
      "Epoch  63 Batch 2393/2739 train_loss = 3.346\n",
      "Epoch  63 Batch 2443/2739 train_loss = 2.121\n",
      "Epoch  63 Batch 2493/2739 train_loss = 1.664\n",
      "Epoch  63 Batch 2543/2739 train_loss = 1.272\n",
      "Epoch  63 Batch 2593/2739 train_loss = 3.892\n",
      "Epoch  63 Batch 2643/2739 train_loss = 1.167\n",
      "Epoch  63 Batch 2693/2739 train_loss = 2.907\n",
      "Epoch  64 Batch    4/2739 train_loss = 3.104\n",
      "Epoch  64 Batch   54/2739 train_loss = 2.606\n",
      "Epoch  64 Batch  104/2739 train_loss = 1.944\n",
      "Epoch  64 Batch  154/2739 train_loss = 2.190\n",
      "Epoch  64 Batch  204/2739 train_loss = 1.641\n",
      "Epoch  64 Batch  254/2739 train_loss = 2.190\n",
      "Epoch  64 Batch  304/2739 train_loss = 1.466\n",
      "Epoch  64 Batch  354/2739 train_loss = 3.496\n",
      "Epoch  64 Batch  404/2739 train_loss = 2.109\n",
      "Epoch  64 Batch  454/2739 train_loss = 3.578\n",
      "Epoch  64 Batch  504/2739 train_loss = 1.745\n",
      "Epoch  64 Batch  554/2739 train_loss = 2.701\n",
      "Epoch  64 Batch  604/2739 train_loss = 1.251\n",
      "Epoch  64 Batch  654/2739 train_loss = 1.540\n",
      "Epoch  64 Batch  704/2739 train_loss = 2.059\n",
      "Epoch  64 Batch  754/2739 train_loss = 1.857\n",
      "Epoch  64 Batch  804/2739 train_loss = 2.587\n",
      "Epoch  64 Batch  854/2739 train_loss = 2.346\n",
      "Epoch  64 Batch  904/2739 train_loss = 2.943\n",
      "Epoch  64 Batch  954/2739 train_loss = 1.922\n",
      "Epoch  64 Batch 1004/2739 train_loss = 1.357\n",
      "Epoch  64 Batch 1054/2739 train_loss = 3.674\n",
      "Epoch  64 Batch 1104/2739 train_loss = 2.369\n",
      "Epoch  64 Batch 1154/2739 train_loss = 2.349\n",
      "Epoch  64 Batch 1204/2739 train_loss = 1.587\n",
      "Epoch  64 Batch 1254/2739 train_loss = 3.362\n",
      "Epoch  64 Batch 1304/2739 train_loss = 1.788\n",
      "Epoch  64 Batch 1354/2739 train_loss = 2.436\n",
      "Epoch  64 Batch 1404/2739 train_loss = 2.165\n",
      "Epoch  64 Batch 1454/2739 train_loss = 1.600\n",
      "Epoch  64 Batch 1504/2739 train_loss = 1.894\n",
      "Epoch  64 Batch 1554/2739 train_loss = 1.685\n",
      "Epoch  64 Batch 1604/2739 train_loss = 2.631\n",
      "Epoch  64 Batch 1654/2739 train_loss = 1.853\n",
      "Epoch  64 Batch 1704/2739 train_loss = 1.615\n",
      "Epoch  64 Batch 1754/2739 train_loss = 1.946\n",
      "Epoch  64 Batch 1804/2739 train_loss = 1.440\n",
      "Epoch  64 Batch 1854/2739 train_loss = 1.991\n",
      "Epoch  64 Batch 1904/2739 train_loss = 2.810\n",
      "Epoch  64 Batch 1954/2739 train_loss = 3.220\n",
      "Epoch  64 Batch 2004/2739 train_loss = 1.710\n",
      "Epoch  64 Batch 2054/2739 train_loss = 2.397\n",
      "Epoch  64 Batch 2104/2739 train_loss = 3.907\n",
      "Epoch  64 Batch 2154/2739 train_loss = 3.327\n",
      "Epoch  64 Batch 2204/2739 train_loss = 3.414\n",
      "Epoch  64 Batch 2254/2739 train_loss = 2.422\n",
      "Epoch  64 Batch 2304/2739 train_loss = 2.731\n",
      "Epoch  64 Batch 2354/2739 train_loss = 1.824\n",
      "Epoch  64 Batch 2404/2739 train_loss = 3.234\n",
      "Epoch  64 Batch 2454/2739 train_loss = 2.774\n",
      "Epoch  64 Batch 2504/2739 train_loss = 3.050\n",
      "Epoch  64 Batch 2554/2739 train_loss = 2.266\n",
      "Epoch  64 Batch 2604/2739 train_loss = 4.217\n",
      "Epoch  64 Batch 2654/2739 train_loss = 1.679\n",
      "Epoch  64 Batch 2704/2739 train_loss = 2.804\n",
      "Epoch  65 Batch   15/2739 train_loss = 2.364\n",
      "Epoch  65 Batch   65/2739 train_loss = 2.501\n",
      "Epoch  65 Batch  115/2739 train_loss = 1.564\n",
      "Epoch  65 Batch  165/2739 train_loss = 1.694\n",
      "Epoch  65 Batch  215/2739 train_loss = 2.743\n",
      "Epoch  65 Batch  265/2739 train_loss = 3.206\n",
      "Epoch  65 Batch  315/2739 train_loss = 2.935\n",
      "Epoch  65 Batch  365/2739 train_loss = 1.769\n",
      "Epoch  65 Batch  415/2739 train_loss = 4.095\n",
      "Epoch  65 Batch  465/2739 train_loss = 3.553\n",
      "Epoch  65 Batch  515/2739 train_loss = 1.213\n",
      "Epoch  65 Batch  565/2739 train_loss = 1.886\n",
      "Epoch  65 Batch  615/2739 train_loss = 3.902\n",
      "Epoch  65 Batch  665/2739 train_loss = 2.610\n",
      "Epoch  65 Batch  715/2739 train_loss = 1.974\n",
      "Epoch  65 Batch  765/2739 train_loss = 1.849\n",
      "Epoch  65 Batch  815/2739 train_loss = 1.764\n",
      "Epoch  65 Batch  865/2739 train_loss = 2.481\n",
      "Epoch  65 Batch  915/2739 train_loss = 2.269\n",
      "Epoch  65 Batch  965/2739 train_loss = 2.633\n",
      "Epoch  65 Batch 1015/2739 train_loss = 1.911\n",
      "Epoch  65 Batch 1065/2739 train_loss = 3.195\n",
      "Epoch  65 Batch 1115/2739 train_loss = 2.936\n",
      "Epoch  65 Batch 1165/2739 train_loss = 1.975\n",
      "Epoch  65 Batch 1215/2739 train_loss = 3.543\n",
      "Epoch  65 Batch 1265/2739 train_loss = 1.259\n",
      "Epoch  65 Batch 1315/2739 train_loss = 2.281\n",
      "Epoch  65 Batch 1365/2739 train_loss = 1.916\n",
      "Epoch  65 Batch 1415/2739 train_loss = 2.735\n",
      "Epoch  65 Batch 1465/2739 train_loss = 2.296\n",
      "Epoch  65 Batch 1515/2739 train_loss = 1.359\n",
      "Epoch  65 Batch 1565/2739 train_loss = 3.813\n",
      "Epoch  65 Batch 1615/2739 train_loss = 2.891\n",
      "Epoch  65 Batch 1665/2739 train_loss = 2.192\n",
      "Epoch  65 Batch 1715/2739 train_loss = 1.781\n",
      "Epoch  65 Batch 1765/2739 train_loss = 1.356\n",
      "Epoch  65 Batch 1815/2739 train_loss = 2.380\n",
      "Epoch  65 Batch 1865/2739 train_loss = 2.986\n",
      "Epoch  65 Batch 1915/2739 train_loss = 3.785\n",
      "Epoch  65 Batch 1965/2739 train_loss = 4.388\n",
      "Epoch  65 Batch 2015/2739 train_loss = 3.668\n",
      "Epoch  65 Batch 2065/2739 train_loss = 4.218\n",
      "Epoch  65 Batch 2115/2739 train_loss = 2.498\n",
      "Epoch  65 Batch 2165/2739 train_loss = 2.673\n",
      "Epoch  65 Batch 2215/2739 train_loss = 3.287\n",
      "Epoch  65 Batch 2265/2739 train_loss = 1.885\n",
      "Epoch  65 Batch 2315/2739 train_loss = 2.542\n",
      "Epoch  65 Batch 2365/2739 train_loss = 4.324\n",
      "Epoch  65 Batch 2415/2739 train_loss = 3.536\n",
      "Epoch  65 Batch 2465/2739 train_loss = 2.499\n",
      "Epoch  65 Batch 2515/2739 train_loss = 2.991\n",
      "Epoch  65 Batch 2565/2739 train_loss = 2.014\n",
      "Epoch  65 Batch 2615/2739 train_loss = 2.206\n",
      "Epoch  65 Batch 2665/2739 train_loss = 2.375\n",
      "Epoch  65 Batch 2715/2739 train_loss = 2.108\n",
      "Epoch  66 Batch   26/2739 train_loss = 1.932\n",
      "Epoch  66 Batch   76/2739 train_loss = 3.063\n",
      "Epoch  66 Batch  126/2739 train_loss = 2.604\n",
      "Epoch  66 Batch  176/2739 train_loss = 1.328\n",
      "Epoch  66 Batch  226/2739 train_loss = 2.856\n",
      "Epoch  66 Batch  276/2739 train_loss = 2.890\n",
      "Epoch  66 Batch  326/2739 train_loss = 3.744\n",
      "Epoch  66 Batch  376/2739 train_loss = 1.990\n",
      "Epoch  66 Batch  426/2739 train_loss = 2.170\n",
      "Epoch  66 Batch  476/2739 train_loss = 2.408\n",
      "Epoch  66 Batch  526/2739 train_loss = 2.528\n",
      "Epoch  66 Batch  576/2739 train_loss = 2.212\n",
      "Epoch  66 Batch  626/2739 train_loss = 2.100\n",
      "Epoch  66 Batch  676/2739 train_loss = 1.585\n",
      "Epoch  66 Batch  726/2739 train_loss = 2.803\n",
      "Epoch  66 Batch  776/2739 train_loss = 1.965\n",
      "Epoch  66 Batch  826/2739 train_loss = 1.652\n",
      "Epoch  66 Batch  876/2739 train_loss = 1.423\n",
      "Epoch  66 Batch  926/2739 train_loss = 1.210\n",
      "Epoch  66 Batch  976/2739 train_loss = 3.076\n",
      "Epoch  66 Batch 1026/2739 train_loss = 1.569\n",
      "Epoch  66 Batch 1076/2739 train_loss = 3.428\n",
      "Epoch  66 Batch 1126/2739 train_loss = 2.604\n",
      "Epoch  66 Batch 1176/2739 train_loss = 4.072\n",
      "Epoch  66 Batch 1226/2739 train_loss = 3.026\n",
      "Epoch  66 Batch 1276/2739 train_loss = 2.195\n",
      "Epoch  66 Batch 1326/2739 train_loss = 2.889\n",
      "Epoch  66 Batch 1376/2739 train_loss = 1.957\n",
      "Epoch  66 Batch 1426/2739 train_loss = 1.644\n",
      "Epoch  66 Batch 1476/2739 train_loss = 3.336\n",
      "Epoch  66 Batch 1526/2739 train_loss = 2.860\n",
      "Epoch  66 Batch 1576/2739 train_loss = 1.938\n",
      "Epoch  66 Batch 1626/2739 train_loss = 2.387\n",
      "Epoch  66 Batch 1676/2739 train_loss = 3.730\n",
      "Epoch  66 Batch 1726/2739 train_loss = 1.365\n",
      "Epoch  66 Batch 1776/2739 train_loss = 3.723\n",
      "Epoch  66 Batch 1826/2739 train_loss = 2.729\n",
      "Epoch  66 Batch 1876/2739 train_loss = 2.849\n",
      "Epoch  66 Batch 1926/2739 train_loss = 3.895\n",
      "Epoch  66 Batch 1976/2739 train_loss = 2.664\n",
      "Epoch  66 Batch 2026/2739 train_loss = 2.336\n",
      "Epoch  66 Batch 2076/2739 train_loss = 3.930\n",
      "Epoch  66 Batch 2126/2739 train_loss = 0.830\n",
      "Epoch  66 Batch 2176/2739 train_loss = 2.231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  66 Batch 2226/2739 train_loss = 3.214\n",
      "Epoch  66 Batch 2276/2739 train_loss = 1.289\n",
      "Epoch  66 Batch 2326/2739 train_loss = 1.395\n",
      "Epoch  66 Batch 2376/2739 train_loss = 3.205\n",
      "Epoch  66 Batch 2426/2739 train_loss = 2.264\n",
      "Epoch  66 Batch 2476/2739 train_loss = 2.558\n",
      "Epoch  66 Batch 2526/2739 train_loss = 3.108\n",
      "Epoch  66 Batch 2576/2739 train_loss = 3.810\n",
      "Epoch  66 Batch 2626/2739 train_loss = 1.855\n",
      "Epoch  66 Batch 2676/2739 train_loss = 1.680\n",
      "Epoch  66 Batch 2726/2739 train_loss = 2.976\n",
      "Epoch  67 Batch   37/2739 train_loss = 1.352\n",
      "Epoch  67 Batch   87/2739 train_loss = 3.821\n",
      "Epoch  67 Batch  137/2739 train_loss = 2.412\n",
      "Epoch  67 Batch  187/2739 train_loss = 2.754\n",
      "Epoch  67 Batch  237/2739 train_loss = 2.397\n",
      "Epoch  67 Batch  287/2739 train_loss = 1.223\n",
      "Epoch  67 Batch  337/2739 train_loss = 1.982\n",
      "Epoch  67 Batch  387/2739 train_loss = 2.102\n",
      "Epoch  67 Batch  437/2739 train_loss = 2.572\n",
      "Epoch  67 Batch  487/2739 train_loss = 3.573\n",
      "Epoch  67 Batch  537/2739 train_loss = 1.835\n",
      "Epoch  67 Batch  587/2739 train_loss = 2.050\n",
      "Epoch  67 Batch  637/2739 train_loss = 3.920\n",
      "Epoch  67 Batch  687/2739 train_loss = 2.070\n",
      "Epoch  67 Batch  737/2739 train_loss = 1.710\n",
      "Epoch  67 Batch  787/2739 train_loss = 2.406\n",
      "Epoch  67 Batch  837/2739 train_loss = 2.825\n",
      "Epoch  67 Batch  887/2739 train_loss = 2.116\n",
      "Epoch  67 Batch  937/2739 train_loss = 1.568\n",
      "Epoch  67 Batch  987/2739 train_loss = 3.925\n",
      "Epoch  67 Batch 1037/2739 train_loss = 3.084\n",
      "Epoch  67 Batch 1087/2739 train_loss = 2.510\n",
      "Epoch  67 Batch 1137/2739 train_loss = 3.274\n",
      "Epoch  67 Batch 1187/2739 train_loss = 4.366\n",
      "Epoch  67 Batch 1237/2739 train_loss = 1.382\n",
      "Epoch  67 Batch 1287/2739 train_loss = 2.092\n",
      "Epoch  67 Batch 1337/2739 train_loss = 4.136\n",
      "Epoch  67 Batch 1387/2739 train_loss = 1.987\n",
      "Epoch  67 Batch 1437/2739 train_loss = 3.678\n",
      "Epoch  67 Batch 1487/2739 train_loss = 1.021\n",
      "Epoch  67 Batch 1537/2739 train_loss = 2.026\n",
      "Epoch  67 Batch 1587/2739 train_loss = 2.871\n",
      "Epoch  67 Batch 1637/2739 train_loss = 3.665\n",
      "Epoch  67 Batch 1687/2739 train_loss = 3.941\n",
      "Epoch  67 Batch 1737/2739 train_loss = 2.347\n",
      "Epoch  67 Batch 1787/2739 train_loss = 2.339\n",
      "Epoch  67 Batch 1837/2739 train_loss = 2.227\n",
      "Epoch  67 Batch 1887/2739 train_loss = 3.753\n",
      "Epoch  67 Batch 1937/2739 train_loss = 1.937\n",
      "Epoch  67 Batch 1987/2739 train_loss = 2.633\n",
      "Epoch  67 Batch 2037/2739 train_loss = 4.189\n",
      "Epoch  67 Batch 2087/2739 train_loss = 4.097\n",
      "Epoch  67 Batch 2137/2739 train_loss = 1.824\n",
      "Epoch  67 Batch 2187/2739 train_loss = 2.115\n",
      "Epoch  67 Batch 2237/2739 train_loss = 3.941\n",
      "Epoch  67 Batch 2287/2739 train_loss = 4.136\n",
      "Epoch  67 Batch 2337/2739 train_loss = 3.482\n",
      "Epoch  67 Batch 2387/2739 train_loss = 1.747\n",
      "Epoch  67 Batch 2437/2739 train_loss = 1.735\n",
      "Epoch  67 Batch 2487/2739 train_loss = 1.879\n",
      "Epoch  67 Batch 2537/2739 train_loss = 1.711\n",
      "Epoch  67 Batch 2587/2739 train_loss = 3.919\n",
      "Epoch  67 Batch 2637/2739 train_loss = 4.348\n",
      "Epoch  67 Batch 2687/2739 train_loss = 1.379\n",
      "Epoch  67 Batch 2737/2739 train_loss = 3.298\n",
      "Epoch  68 Batch   48/2739 train_loss = 1.239\n",
      "Epoch  68 Batch   98/2739 train_loss = 1.770\n",
      "Epoch  68 Batch  148/2739 train_loss = 4.114\n",
      "Epoch  68 Batch  198/2739 train_loss = 2.031\n",
      "Epoch  68 Batch  248/2739 train_loss = 2.889\n",
      "Epoch  68 Batch  298/2739 train_loss = 2.535\n",
      "Epoch  68 Batch  348/2739 train_loss = 2.400\n",
      "Epoch  68 Batch  398/2739 train_loss = 2.275\n",
      "Epoch  68 Batch  448/2739 train_loss = 3.565\n",
      "Epoch  68 Batch  498/2739 train_loss = 2.698\n",
      "Epoch  68 Batch  548/2739 train_loss = 1.621\n",
      "Epoch  68 Batch  598/2739 train_loss = 2.194\n",
      "Epoch  68 Batch  648/2739 train_loss = 2.047\n",
      "Epoch  68 Batch  698/2739 train_loss = 3.002\n",
      "Epoch  68 Batch  748/2739 train_loss = 2.118\n",
      "Epoch  68 Batch  798/2739 train_loss = 2.418\n",
      "Epoch  68 Batch  848/2739 train_loss = 3.629\n",
      "Epoch  68 Batch  898/2739 train_loss = 3.417\n",
      "Epoch  68 Batch  948/2739 train_loss = 2.313\n",
      "Epoch  68 Batch  998/2739 train_loss = 2.305\n",
      "Epoch  68 Batch 1048/2739 train_loss = 2.195\n",
      "Epoch  68 Batch 1098/2739 train_loss = 2.132\n",
      "Epoch  68 Batch 1148/2739 train_loss = 3.311\n",
      "Epoch  68 Batch 1198/2739 train_loss = 2.052\n",
      "Epoch  68 Batch 1248/2739 train_loss = 1.399\n",
      "Epoch  68 Batch 1298/2739 train_loss = 1.994\n",
      "Epoch  68 Batch 1348/2739 train_loss = 1.279\n",
      "Epoch  68 Batch 1398/2739 train_loss = 2.754\n",
      "Epoch  68 Batch 1448/2739 train_loss = 3.193\n",
      "Epoch  68 Batch 1498/2739 train_loss = 2.328\n",
      "Epoch  68 Batch 1548/2739 train_loss = 1.604\n",
      "Epoch  68 Batch 1598/2739 train_loss = 1.575\n",
      "Epoch  68 Batch 1648/2739 train_loss = 1.517\n",
      "Epoch  68 Batch 1698/2739 train_loss = 2.295\n",
      "Epoch  68 Batch 1748/2739 train_loss = 2.093\n",
      "Epoch  68 Batch 1798/2739 train_loss = 1.991\n",
      "Epoch  68 Batch 1848/2739 train_loss = 2.145\n",
      "Epoch  68 Batch 1898/2739 train_loss = 2.201\n",
      "Epoch  68 Batch 1948/2739 train_loss = 3.631\n",
      "Epoch  68 Batch 1998/2739 train_loss = 2.238\n",
      "Epoch  68 Batch 2048/2739 train_loss = 3.623\n",
      "Epoch  68 Batch 2098/2739 train_loss = 4.070\n",
      "Epoch  68 Batch 2148/2739 train_loss = 2.705\n",
      "Epoch  68 Batch 2198/2739 train_loss = 4.076\n",
      "Epoch  68 Batch 2248/2739 train_loss = 3.111\n",
      "Epoch  68 Batch 2298/2739 train_loss = 1.746\n",
      "Epoch  68 Batch 2348/2739 train_loss = 2.830\n",
      "Epoch  68 Batch 2398/2739 train_loss = 3.471\n",
      "Epoch  68 Batch 2448/2739 train_loss = 1.302\n",
      "Epoch  68 Batch 2498/2739 train_loss = 2.134\n",
      "Epoch  68 Batch 2548/2739 train_loss = 1.608\n",
      "Epoch  68 Batch 2598/2739 train_loss = 4.441\n",
      "Epoch  68 Batch 2648/2739 train_loss = 2.980\n",
      "Epoch  68 Batch 2698/2739 train_loss = 1.251\n",
      "Epoch  69 Batch    9/2739 train_loss = 0.910\n",
      "Epoch  69 Batch   59/2739 train_loss = 2.086\n",
      "Epoch  69 Batch  109/2739 train_loss = 2.782\n",
      "Epoch  69 Batch  159/2739 train_loss = 2.233\n",
      "Epoch  69 Batch  209/2739 train_loss = 1.978\n",
      "Epoch  69 Batch  259/2739 train_loss = 1.514\n",
      "Epoch  69 Batch  309/2739 train_loss = 2.800\n",
      "Epoch  69 Batch  359/2739 train_loss = 1.584\n",
      "Epoch  69 Batch  409/2739 train_loss = 1.898\n",
      "Epoch  69 Batch  459/2739 train_loss = 3.324\n",
      "Epoch  69 Batch  509/2739 train_loss = 2.211\n",
      "Epoch  69 Batch  559/2739 train_loss = 2.165\n",
      "Epoch  69 Batch  609/2739 train_loss = 2.088\n",
      "Epoch  69 Batch  659/2739 train_loss = 2.141\n",
      "Epoch  69 Batch  709/2739 train_loss = 2.181\n",
      "Epoch  69 Batch  759/2739 train_loss = 3.244\n",
      "Epoch  69 Batch  809/2739 train_loss = 2.108\n",
      "Epoch  69 Batch  859/2739 train_loss = 3.502\n",
      "Epoch  69 Batch  909/2739 train_loss = 1.352\n",
      "Epoch  69 Batch  959/2739 train_loss = 3.925\n",
      "Epoch  69 Batch 1009/2739 train_loss = 1.705\n",
      "Epoch  69 Batch 1059/2739 train_loss = 1.733\n",
      "Epoch  69 Batch 1109/2739 train_loss = 2.855\n",
      "Epoch  69 Batch 1159/2739 train_loss = 3.391\n",
      "Epoch  69 Batch 1209/2739 train_loss = 1.800\n",
      "Epoch  69 Batch 1259/2739 train_loss = 1.695\n",
      "Epoch  69 Batch 1309/2739 train_loss = 2.494\n",
      "Epoch  69 Batch 1359/2739 train_loss = 2.566\n",
      "Epoch  69 Batch 1409/2739 train_loss = 3.121\n",
      "Epoch  69 Batch 1459/2739 train_loss = 3.847\n",
      "Epoch  69 Batch 1509/2739 train_loss = 2.056\n",
      "Epoch  69 Batch 1559/2739 train_loss = 2.038\n",
      "Epoch  69 Batch 1609/2739 train_loss = 2.030\n",
      "Epoch  69 Batch 1659/2739 train_loss = 3.082\n",
      "Epoch  69 Batch 1709/2739 train_loss = 3.276\n",
      "Epoch  69 Batch 1759/2739 train_loss = 1.780\n",
      "Epoch  69 Batch 1809/2739 train_loss = 2.345\n",
      "Epoch  69 Batch 1859/2739 train_loss = 2.771\n",
      "Epoch  69 Batch 1909/2739 train_loss = 3.114\n",
      "Epoch  69 Batch 1959/2739 train_loss = 4.419\n",
      "Epoch  69 Batch 2009/2739 train_loss = 2.410\n",
      "Epoch  69 Batch 2059/2739 train_loss = 2.544\n",
      "Epoch  69 Batch 2109/2739 train_loss = 1.747\n",
      "Epoch  69 Batch 2159/2739 train_loss = 2.531\n",
      "Epoch  69 Batch 2209/2739 train_loss = 3.690\n",
      "Epoch  69 Batch 2259/2739 train_loss = 4.275\n",
      "Epoch  69 Batch 2309/2739 train_loss = 3.267\n",
      "Epoch  69 Batch 2359/2739 train_loss = 2.061\n",
      "Epoch  69 Batch 2409/2739 train_loss = 2.256\n",
      "Epoch  69 Batch 2459/2739 train_loss = 2.335\n",
      "Epoch  69 Batch 2509/2739 train_loss = 1.724\n",
      "Epoch  69 Batch 2559/2739 train_loss = 2.165\n",
      "Epoch  69 Batch 2609/2739 train_loss = 4.064\n",
      "Epoch  69 Batch 2659/2739 train_loss = 1.191\n",
      "Epoch  69 Batch 2709/2739 train_loss = 2.486\n",
      "Epoch  70 Batch   20/2739 train_loss = 1.868\n",
      "Epoch  70 Batch   70/2739 train_loss = 2.831\n",
      "Epoch  70 Batch  120/2739 train_loss = 3.984\n",
      "Epoch  70 Batch  170/2739 train_loss = 2.307\n",
      "Epoch  70 Batch  220/2739 train_loss = 1.892\n",
      "Epoch  70 Batch  270/2739 train_loss = 1.473\n",
      "Epoch  70 Batch  320/2739 train_loss = 2.743\n",
      "Epoch  70 Batch  370/2739 train_loss = 1.931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  70 Batch  420/2739 train_loss = 3.642\n",
      "Epoch  70 Batch  470/2739 train_loss = 3.086\n",
      "Epoch  70 Batch  520/2739 train_loss = 2.310\n",
      "Epoch  70 Batch  570/2739 train_loss = 2.274\n",
      "Epoch  70 Batch  620/2739 train_loss = 2.461\n",
      "Epoch  70 Batch  670/2739 train_loss = 2.400\n",
      "Epoch  70 Batch  720/2739 train_loss = 2.394\n",
      "Epoch  70 Batch  770/2739 train_loss = 1.032\n",
      "Epoch  70 Batch  820/2739 train_loss = 2.580\n",
      "Epoch  70 Batch  870/2739 train_loss = 3.386\n",
      "Epoch  70 Batch  920/2739 train_loss = 1.878\n",
      "Epoch  70 Batch  970/2739 train_loss = 3.533\n",
      "Epoch  70 Batch 1020/2739 train_loss = 2.367\n",
      "Epoch  70 Batch 1070/2739 train_loss = 3.716\n",
      "Epoch  70 Batch 1120/2739 train_loss = 3.871\n",
      "Epoch  70 Batch 1170/2739 train_loss = 2.385\n",
      "Epoch  70 Batch 1220/2739 train_loss = 1.479\n",
      "Epoch  70 Batch 1270/2739 train_loss = 3.239\n",
      "Epoch  70 Batch 1320/2739 train_loss = 3.905\n",
      "Epoch  70 Batch 1370/2739 train_loss = 3.163\n",
      "Epoch  70 Batch 1420/2739 train_loss = 1.776\n",
      "Epoch  70 Batch 1470/2739 train_loss = 2.208\n",
      "Epoch  70 Batch 1520/2739 train_loss = 3.565\n",
      "Epoch  70 Batch 1570/2739 train_loss = 2.216\n",
      "Epoch  70 Batch 1620/2739 train_loss = 1.456\n",
      "Epoch  70 Batch 1670/2739 train_loss = 2.869\n",
      "Epoch  70 Batch 1720/2739 train_loss = 2.209\n",
      "Epoch  70 Batch 1770/2739 train_loss = 2.002\n",
      "Epoch  70 Batch 1820/2739 train_loss = 3.902\n",
      "Epoch  70 Batch 1870/2739 train_loss = 3.681\n",
      "Epoch  70 Batch 1920/2739 train_loss = 3.113\n",
      "Epoch  70 Batch 1970/2739 train_loss = 2.277\n",
      "Epoch  70 Batch 2020/2739 train_loss = 1.696\n",
      "Epoch  70 Batch 2070/2739 train_loss = 2.782\n",
      "Epoch  70 Batch 2120/2739 train_loss = 2.152\n",
      "Epoch  70 Batch 2170/2739 train_loss = 2.054\n",
      "Epoch  70 Batch 2220/2739 train_loss = 1.509\n",
      "Epoch  70 Batch 2270/2739 train_loss = 2.396\n",
      "Epoch  70 Batch 2320/2739 train_loss = 4.417\n",
      "Epoch  70 Batch 2370/2739 train_loss = 1.469\n",
      "Epoch  70 Batch 2420/2739 train_loss = 2.892\n",
      "Epoch  70 Batch 2470/2739 train_loss = 1.856\n",
      "Epoch  70 Batch 2520/2739 train_loss = 2.548\n",
      "Epoch  70 Batch 2570/2739 train_loss = 1.606\n",
      "Epoch  70 Batch 2620/2739 train_loss = 3.031\n",
      "Epoch  70 Batch 2670/2739 train_loss = 2.140\n",
      "Epoch  70 Batch 2720/2739 train_loss = 1.777\n",
      "Epoch  71 Batch   31/2739 train_loss = 3.294\n",
      "Epoch  71 Batch   81/2739 train_loss = 1.384\n",
      "Epoch  71 Batch  131/2739 train_loss = 2.431\n",
      "Epoch  71 Batch  181/2739 train_loss = 2.507\n",
      "Epoch  71 Batch  231/2739 train_loss = 1.619\n",
      "Epoch  71 Batch  281/2739 train_loss = 2.092\n",
      "Epoch  71 Batch  331/2739 train_loss = 2.073\n",
      "Epoch  71 Batch  381/2739 train_loss = 1.763\n",
      "Epoch  71 Batch  431/2739 train_loss = 1.755\n",
      "Epoch  71 Batch  481/2739 train_loss = 2.830\n",
      "Epoch  71 Batch  531/2739 train_loss = 2.008\n",
      "Epoch  71 Batch  581/2739 train_loss = 2.825\n",
      "Epoch  71 Batch  631/2739 train_loss = 3.112\n",
      "Epoch  71 Batch  681/2739 train_loss = 1.562\n",
      "Epoch  71 Batch  731/2739 train_loss = 3.511\n",
      "Epoch  71 Batch  781/2739 train_loss = 2.139\n",
      "Epoch  71 Batch  831/2739 train_loss = 2.338\n",
      "Epoch  71 Batch  881/2739 train_loss = 1.919\n",
      "Epoch  71 Batch  931/2739 train_loss = 3.175\n",
      "Epoch  71 Batch  981/2739 train_loss = 1.859\n",
      "Epoch  71 Batch 1031/2739 train_loss = 1.897\n",
      "Epoch  71 Batch 1081/2739 train_loss = 2.002\n",
      "Epoch  71 Batch 1131/2739 train_loss = 1.627\n",
      "Epoch  71 Batch 1181/2739 train_loss = 3.796\n",
      "Epoch  71 Batch 1231/2739 train_loss = 3.179\n",
      "Epoch  71 Batch 1281/2739 train_loss = 1.713\n",
      "Epoch  71 Batch 1331/2739 train_loss = 3.969\n",
      "Epoch  71 Batch 1381/2739 train_loss = 3.025\n",
      "Epoch  71 Batch 1431/2739 train_loss = 2.868\n",
      "Epoch  71 Batch 1481/2739 train_loss = 1.225\n",
      "Epoch  71 Batch 1531/2739 train_loss = 1.798\n",
      "Epoch  71 Batch 1581/2739 train_loss = 2.265\n",
      "Epoch  71 Batch 1631/2739 train_loss = 3.788\n",
      "Epoch  71 Batch 1681/2739 train_loss = 2.161\n",
      "Epoch  71 Batch 1731/2739 train_loss = 4.002\n",
      "Epoch  71 Batch 1781/2739 train_loss = 2.578\n",
      "Epoch  71 Batch 1831/2739 train_loss = 2.396\n",
      "Epoch  71 Batch 1881/2739 train_loss = 3.671\n",
      "Epoch  71 Batch 1931/2739 train_loss = 4.526\n",
      "Epoch  71 Batch 1981/2739 train_loss = 2.054\n",
      "Epoch  71 Batch 2031/2739 train_loss = 2.685\n",
      "Epoch  71 Batch 2081/2739 train_loss = 3.685\n",
      "Epoch  71 Batch 2131/2739 train_loss = 2.864\n",
      "Epoch  71 Batch 2181/2739 train_loss = 2.512\n",
      "Epoch  71 Batch 2231/2739 train_loss = 1.723\n",
      "Epoch  71 Batch 2281/2739 train_loss = 4.436\n",
      "Epoch  71 Batch 2331/2739 train_loss = 1.708\n",
      "Epoch  71 Batch 2381/2739 train_loss = 1.638\n",
      "Epoch  71 Batch 2431/2739 train_loss = 1.666\n",
      "Epoch  71 Batch 2481/2739 train_loss = 3.105\n",
      "Epoch  71 Batch 2531/2739 train_loss = 3.333\n",
      "Epoch  71 Batch 2581/2739 train_loss = 2.224\n",
      "Epoch  71 Batch 2631/2739 train_loss = 1.942\n",
      "Epoch  71 Batch 2681/2739 train_loss = 2.222\n",
      "Epoch  71 Batch 2731/2739 train_loss = 1.392\n",
      "Epoch  72 Batch   42/2739 train_loss = 2.931\n",
      "Epoch  72 Batch   92/2739 train_loss = 1.407\n",
      "Epoch  72 Batch  142/2739 train_loss = 2.381\n",
      "Epoch  72 Batch  192/2739 train_loss = 1.095\n",
      "Epoch  72 Batch  242/2739 train_loss = 2.684\n",
      "Epoch  72 Batch  292/2739 train_loss = 2.639\n",
      "Epoch  72 Batch  342/2739 train_loss = 2.192\n",
      "Epoch  72 Batch  392/2739 train_loss = 1.351\n",
      "Epoch  72 Batch  442/2739 train_loss = 1.074\n",
      "Epoch  72 Batch  492/2739 train_loss = 2.262\n",
      "Epoch  72 Batch  542/2739 train_loss = 2.321\n",
      "Epoch  72 Batch  592/2739 train_loss = 3.321\n",
      "Epoch  72 Batch  642/2739 train_loss = 2.838\n",
      "Epoch  72 Batch  692/2739 train_loss = 1.527\n",
      "Epoch  72 Batch  742/2739 train_loss = 2.083\n",
      "Epoch  72 Batch  792/2739 train_loss = 3.515\n",
      "Epoch  72 Batch  842/2739 train_loss = 2.253\n",
      "Epoch  72 Batch  892/2739 train_loss = 1.689\n",
      "Epoch  72 Batch  942/2739 train_loss = 3.049\n",
      "Epoch  72 Batch  992/2739 train_loss = 1.836\n",
      "Epoch  72 Batch 1042/2739 train_loss = 3.998\n",
      "Epoch  72 Batch 1092/2739 train_loss = 2.935\n",
      "Epoch  72 Batch 1142/2739 train_loss = 3.953\n",
      "Epoch  72 Batch 1192/2739 train_loss = 1.783\n",
      "Epoch  72 Batch 1242/2739 train_loss = 1.947\n",
      "Epoch  72 Batch 1292/2739 train_loss = 2.079\n",
      "Epoch  72 Batch 1342/2739 train_loss = 1.742\n",
      "Epoch  72 Batch 1392/2739 train_loss = 3.752\n",
      "Epoch  72 Batch 1442/2739 train_loss = 1.906\n",
      "Epoch  72 Batch 1492/2739 train_loss = 1.823\n",
      "Epoch  72 Batch 1542/2739 train_loss = 2.910\n",
      "Epoch  72 Batch 1592/2739 train_loss = 2.730\n",
      "Epoch  72 Batch 1642/2739 train_loss = 3.664\n",
      "Epoch  72 Batch 1692/2739 train_loss = 3.565\n",
      "Epoch  72 Batch 1742/2739 train_loss = 3.829\n",
      "Epoch  72 Batch 1792/2739 train_loss = 2.532\n",
      "Epoch  72 Batch 1842/2739 train_loss = 2.037\n",
      "Epoch  72 Batch 1892/2739 train_loss = 3.020\n",
      "Epoch  72 Batch 1942/2739 train_loss = 2.344\n",
      "Epoch  72 Batch 1992/2739 train_loss = 2.012\n",
      "Epoch  72 Batch 2042/2739 train_loss = 3.466\n",
      "Epoch  72 Batch 2092/2739 train_loss = 3.436\n",
      "Epoch  72 Batch 2142/2739 train_loss = 3.578\n",
      "Epoch  72 Batch 2192/2739 train_loss = 2.578\n",
      "Epoch  72 Batch 2242/2739 train_loss = 1.853\n",
      "Epoch  72 Batch 2292/2739 train_loss = 1.825\n",
      "Epoch  72 Batch 2342/2739 train_loss = 1.730\n",
      "Epoch  72 Batch 2392/2739 train_loss = 2.873\n",
      "Epoch  72 Batch 2442/2739 train_loss = 1.823\n",
      "Epoch  72 Batch 2492/2739 train_loss = 1.801\n",
      "Epoch  72 Batch 2542/2739 train_loss = 3.897\n",
      "Epoch  72 Batch 2592/2739 train_loss = 3.828\n",
      "Epoch  72 Batch 2642/2739 train_loss = 1.104\n",
      "Epoch  72 Batch 2692/2739 train_loss = 2.169\n",
      "Epoch  73 Batch    3/2739 train_loss = 2.616\n",
      "Epoch  73 Batch   53/2739 train_loss = 1.804\n",
      "Epoch  73 Batch  103/2739 train_loss = 1.861\n",
      "Epoch  73 Batch  153/2739 train_loss = 2.212\n",
      "Epoch  73 Batch  203/2739 train_loss = 1.560\n",
      "Epoch  73 Batch  253/2739 train_loss = 2.329\n",
      "Epoch  73 Batch  303/2739 train_loss = 2.421\n",
      "Epoch  73 Batch  353/2739 train_loss = 2.543\n",
      "Epoch  73 Batch  403/2739 train_loss = 2.107\n",
      "Epoch  73 Batch  453/2739 train_loss = 3.350\n",
      "Epoch  73 Batch  503/2739 train_loss = 2.155\n",
      "Epoch  73 Batch  553/2739 train_loss = 1.739\n",
      "Epoch  73 Batch  603/2739 train_loss = 2.326\n",
      "Epoch  73 Batch  653/2739 train_loss = 1.703\n",
      "Epoch  73 Batch  703/2739 train_loss = 2.000\n",
      "Epoch  73 Batch  753/2739 train_loss = 1.933\n",
      "Epoch  73 Batch  803/2739 train_loss = 1.670\n",
      "Epoch  73 Batch  853/2739 train_loss = 1.920\n",
      "Epoch  73 Batch  903/2739 train_loss = 3.396\n",
      "Epoch  73 Batch  953/2739 train_loss = 2.925\n",
      "Epoch  73 Batch 1003/2739 train_loss = 3.332\n",
      "Epoch  73 Batch 1053/2739 train_loss = 3.832\n",
      "Epoch  73 Batch 1103/2739 train_loss = 2.258\n",
      "Epoch  73 Batch 1153/2739 train_loss = 2.303\n",
      "Epoch  73 Batch 1203/2739 train_loss = 1.612\n",
      "Epoch  73 Batch 1253/2739 train_loss = 3.476\n",
      "Epoch  73 Batch 1303/2739 train_loss = 1.931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  73 Batch 1353/2739 train_loss = 3.772\n",
      "Epoch  73 Batch 1403/2739 train_loss = 2.355\n",
      "Epoch  73 Batch 1453/2739 train_loss = 0.991\n",
      "Epoch  73 Batch 1503/2739 train_loss = 1.599\n",
      "Epoch  73 Batch 1553/2739 train_loss = 3.599\n",
      "Epoch  73 Batch 1603/2739 train_loss = 1.621\n",
      "Epoch  73 Batch 1653/2739 train_loss = 3.053\n",
      "Epoch  73 Batch 1703/2739 train_loss = 2.992\n",
      "Epoch  73 Batch 1753/2739 train_loss = 3.725\n",
      "Epoch  73 Batch 1803/2739 train_loss = 3.177\n",
      "Epoch  73 Batch 1853/2739 train_loss = 1.721\n",
      "Epoch  73 Batch 1903/2739 train_loss = 2.144\n",
      "Epoch  73 Batch 1953/2739 train_loss = 1.372\n",
      "Epoch  73 Batch 2003/2739 train_loss = 2.190\n",
      "Epoch  73 Batch 2053/2739 train_loss = 2.324\n",
      "Epoch  73 Batch 2103/2739 train_loss = 2.563\n",
      "Epoch  73 Batch 2153/2739 train_loss = 2.949\n",
      "Epoch  73 Batch 2203/2739 train_loss = 2.984\n",
      "Epoch  73 Batch 2253/2739 train_loss = 1.990\n",
      "Epoch  73 Batch 2303/2739 train_loss = 4.608\n",
      "Epoch  73 Batch 2353/2739 train_loss = 4.134\n",
      "Epoch  73 Batch 2403/2739 train_loss = 3.104\n",
      "Epoch  73 Batch 2453/2739 train_loss = 2.382\n",
      "Epoch  73 Batch 2503/2739 train_loss = 1.867\n",
      "Epoch  73 Batch 2553/2739 train_loss = 1.855\n",
      "Epoch  73 Batch 2603/2739 train_loss = 2.361\n",
      "Epoch  73 Batch 2653/2739 train_loss = 1.267\n",
      "Epoch  73 Batch 2703/2739 train_loss = 2.628\n",
      "Epoch  74 Batch   14/2739 train_loss = 1.653\n",
      "Epoch  74 Batch   64/2739 train_loss = 2.413\n",
      "Epoch  74 Batch  114/2739 train_loss = 1.781\n",
      "Epoch  74 Batch  164/2739 train_loss = 2.436\n",
      "Epoch  74 Batch  214/2739 train_loss = 2.134\n",
      "Epoch  74 Batch  264/2739 train_loss = 2.106\n",
      "Epoch  74 Batch  314/2739 train_loss = 2.442\n",
      "Epoch  74 Batch  364/2739 train_loss = 2.550\n",
      "Epoch  74 Batch  414/2739 train_loss = 2.491\n",
      "Epoch  74 Batch  464/2739 train_loss = 2.335\n",
      "Epoch  74 Batch  514/2739 train_loss = 2.269\n",
      "Epoch  74 Batch  564/2739 train_loss = 2.435\n",
      "Epoch  74 Batch  614/2739 train_loss = 4.138\n",
      "Epoch  74 Batch  664/2739 train_loss = 3.282\n",
      "Epoch  74 Batch  714/2739 train_loss = 2.310\n",
      "Epoch  74 Batch  764/2739 train_loss = 1.768\n",
      "Epoch  74 Batch  814/2739 train_loss = 2.449\n",
      "Epoch  74 Batch  864/2739 train_loss = 3.505\n",
      "Epoch  74 Batch  914/2739 train_loss = 2.033\n",
      "Epoch  74 Batch  964/2739 train_loss = 4.141\n",
      "Epoch  74 Batch 1014/2739 train_loss = 2.170\n",
      "Epoch  74 Batch 1064/2739 train_loss = 1.899\n",
      "Epoch  74 Batch 1114/2739 train_loss = 1.745\n",
      "Epoch  74 Batch 1164/2739 train_loss = 1.824\n",
      "Epoch  74 Batch 1214/2739 train_loss = 1.759\n",
      "Epoch  74 Batch 1264/2739 train_loss = 2.764\n",
      "Epoch  74 Batch 1314/2739 train_loss = 2.450\n",
      "Epoch  74 Batch 1364/2739 train_loss = 3.111\n",
      "Epoch  74 Batch 1414/2739 train_loss = 2.269\n",
      "Epoch  74 Batch 1464/2739 train_loss = 1.268\n",
      "Epoch  74 Batch 1514/2739 train_loss = 1.699\n",
      "Epoch  74 Batch 1564/2739 train_loss = 2.179\n",
      "Epoch  74 Batch 1614/2739 train_loss = 1.327\n",
      "Epoch  74 Batch 1664/2739 train_loss = 2.032\n",
      "Epoch  74 Batch 1714/2739 train_loss = 2.576\n",
      "Epoch  74 Batch 1764/2739 train_loss = 2.305\n",
      "Epoch  74 Batch 1814/2739 train_loss = 2.105\n",
      "Epoch  74 Batch 1864/2739 train_loss = 2.340\n",
      "Epoch  74 Batch 1914/2739 train_loss = 3.874\n",
      "Epoch  74 Batch 1964/2739 train_loss = 2.705\n",
      "Epoch  74 Batch 2014/2739 train_loss = 1.805\n",
      "Epoch  74 Batch 2064/2739 train_loss = 4.099\n",
      "Epoch  74 Batch 2114/2739 train_loss = 3.788\n",
      "Epoch  74 Batch 2164/2739 train_loss = 2.044\n",
      "Epoch  74 Batch 2214/2739 train_loss = 3.094\n",
      "Epoch  74 Batch 2264/2739 train_loss = 4.031\n",
      "Epoch  74 Batch 2314/2739 train_loss = 3.955\n",
      "Epoch  74 Batch 2364/2739 train_loss = 1.562\n",
      "Epoch  74 Batch 2414/2739 train_loss = 2.410\n",
      "Epoch  74 Batch 2464/2739 train_loss = 2.658\n",
      "Epoch  74 Batch 2514/2739 train_loss = 1.813\n",
      "Epoch  74 Batch 2564/2739 train_loss = 1.405\n",
      "Epoch  74 Batch 2614/2739 train_loss = 2.395\n",
      "Epoch  74 Batch 2664/2739 train_loss = 2.372\n",
      "Epoch  74 Batch 2714/2739 train_loss = 2.531\n",
      "Epoch  75 Batch   25/2739 train_loss = 1.881\n",
      "Epoch  75 Batch   75/2739 train_loss = 3.505\n",
      "Epoch  75 Batch  125/2739 train_loss = 2.862\n",
      "Epoch  75 Batch  175/2739 train_loss = 2.349\n",
      "Epoch  75 Batch  225/2739 train_loss = 2.524\n",
      "Epoch  75 Batch  275/2739 train_loss = 2.981\n",
      "Epoch  75 Batch  325/2739 train_loss = 3.223\n",
      "Epoch  75 Batch  375/2739 train_loss = 2.935\n",
      "Epoch  75 Batch  425/2739 train_loss = 1.422\n",
      "Epoch  75 Batch  475/2739 train_loss = 1.731\n",
      "Epoch  75 Batch  525/2739 train_loss = 1.850\n",
      "Epoch  75 Batch  575/2739 train_loss = 3.718\n",
      "Epoch  75 Batch  625/2739 train_loss = 2.032\n",
      "Epoch  75 Batch  675/2739 train_loss = 2.780\n",
      "Epoch  75 Batch  725/2739 train_loss = 2.193\n",
      "Epoch  75 Batch  775/2739 train_loss = 3.497\n",
      "Epoch  75 Batch  825/2739 train_loss = 2.978\n",
      "Epoch  75 Batch  875/2739 train_loss = 1.605\n",
      "Epoch  75 Batch  925/2739 train_loss = 1.881\n",
      "Epoch  75 Batch  975/2739 train_loss = 1.918\n",
      "Epoch  75 Batch 1025/2739 train_loss = 1.773\n",
      "Epoch  75 Batch 1075/2739 train_loss = 1.576\n",
      "Epoch  75 Batch 1125/2739 train_loss = 3.262\n",
      "Epoch  75 Batch 1175/2739 train_loss = 4.054\n",
      "Epoch  75 Batch 1225/2739 train_loss = 2.152\n",
      "Epoch  75 Batch 1275/2739 train_loss = 2.097\n",
      "Epoch  75 Batch 1325/2739 train_loss = 2.946\n",
      "Epoch  75 Batch 1375/2739 train_loss = 2.047\n",
      "Epoch  75 Batch 1425/2739 train_loss = 1.373\n",
      "Epoch  75 Batch 1475/2739 train_loss = 1.842\n",
      "Epoch  75 Batch 1525/2739 train_loss = 2.857\n",
      "Epoch  75 Batch 1575/2739 train_loss = 3.462\n",
      "Epoch  75 Batch 1625/2739 train_loss = 1.889\n",
      "Epoch  75 Batch 1675/2739 train_loss = 1.935\n",
      "Epoch  75 Batch 1725/2739 train_loss = 2.056\n",
      "Epoch  75 Batch 1775/2739 train_loss = 1.352\n",
      "Epoch  75 Batch 1825/2739 train_loss = 3.877\n",
      "Epoch  75 Batch 1875/2739 train_loss = 1.296\n",
      "Epoch  75 Batch 1925/2739 train_loss = 2.138\n",
      "Epoch  75 Batch 1975/2739 train_loss = 1.963\n",
      "Epoch  75 Batch 2025/2739 train_loss = 1.697\n",
      "Epoch  75 Batch 2075/2739 train_loss = 2.992\n",
      "Epoch  75 Batch 2125/2739 train_loss = 3.817\n",
      "Epoch  75 Batch 2175/2739 train_loss = 2.304\n",
      "Epoch  75 Batch 2225/2739 train_loss = 3.406\n",
      "Epoch  75 Batch 2275/2739 train_loss = 2.284\n",
      "Epoch  75 Batch 2325/2739 train_loss = 2.566\n",
      "Epoch  75 Batch 2375/2739 train_loss = 1.487\n",
      "Epoch  75 Batch 2425/2739 train_loss = 2.237\n",
      "Epoch  75 Batch 2475/2739 train_loss = 1.601\n",
      "Epoch  75 Batch 2525/2739 train_loss = 1.056\n",
      "Epoch  75 Batch 2575/2739 train_loss = 3.883\n",
      "Epoch  75 Batch 2625/2739 train_loss = 2.261\n",
      "Epoch  75 Batch 2675/2739 train_loss = 1.807\n",
      "Epoch  75 Batch 2725/2739 train_loss = 3.978\n",
      "Epoch  76 Batch   36/2739 train_loss = 3.172\n",
      "Epoch  76 Batch   86/2739 train_loss = 2.296\n",
      "Epoch  76 Batch  136/2739 train_loss = 1.917\n",
      "Epoch  76 Batch  186/2739 train_loss = 1.627\n",
      "Epoch  76 Batch  236/2739 train_loss = 1.661\n",
      "Epoch  76 Batch  286/2739 train_loss = 2.028\n",
      "Epoch  76 Batch  336/2739 train_loss = 2.727\n",
      "Epoch  76 Batch  386/2739 train_loss = 3.465\n",
      "Epoch  76 Batch  436/2739 train_loss = 2.692\n",
      "Epoch  76 Batch  486/2739 train_loss = 3.389\n",
      "Epoch  76 Batch  536/2739 train_loss = 1.976\n",
      "Epoch  76 Batch  586/2739 train_loss = 2.791\n",
      "Epoch  76 Batch  636/2739 train_loss = 3.983\n",
      "Epoch  76 Batch  686/2739 train_loss = 2.645\n",
      "Epoch  76 Batch  736/2739 train_loss = 3.569\n",
      "Epoch  76 Batch  786/2739 train_loss = 3.624\n",
      "Epoch  76 Batch  836/2739 train_loss = 2.560\n",
      "Epoch  76 Batch  886/2739 train_loss = 3.024\n",
      "Epoch  76 Batch  936/2739 train_loss = 2.427\n",
      "Epoch  76 Batch  986/2739 train_loss = 1.425\n",
      "Epoch  76 Batch 1036/2739 train_loss = 2.267\n",
      "Epoch  76 Batch 1086/2739 train_loss = 2.409\n",
      "Epoch  76 Batch 1136/2739 train_loss = 2.809\n",
      "Epoch  76 Batch 1186/2739 train_loss = 4.250\n",
      "Epoch  76 Batch 1236/2739 train_loss = 2.710\n",
      "Epoch  76 Batch 1286/2739 train_loss = 1.976\n",
      "Epoch  76 Batch 1336/2739 train_loss = 4.229\n",
      "Epoch  76 Batch 1386/2739 train_loss = 2.458\n",
      "Epoch  76 Batch 1436/2739 train_loss = 2.073\n",
      "Epoch  76 Batch 1486/2739 train_loss = 3.349\n",
      "Epoch  76 Batch 1536/2739 train_loss = 2.097\n",
      "Epoch  76 Batch 1586/2739 train_loss = 1.935\n",
      "Epoch  76 Batch 1636/2739 train_loss = 1.335\n",
      "Epoch  76 Batch 1686/2739 train_loss = 3.796\n",
      "Epoch  76 Batch 1736/2739 train_loss = 2.074\n",
      "Epoch  76 Batch 1786/2739 train_loss = 2.162\n",
      "Epoch  76 Batch 1836/2739 train_loss = 3.778\n",
      "Epoch  76 Batch 1886/2739 train_loss = 3.714\n",
      "Epoch  76 Batch 1936/2739 train_loss = 1.856\n",
      "Epoch  76 Batch 1986/2739 train_loss = 2.586\n",
      "Epoch  76 Batch 2036/2739 train_loss = 3.885\n",
      "Epoch  76 Batch 2086/2739 train_loss = 2.613\n",
      "Epoch  76 Batch 2136/2739 train_loss = 4.191\n",
      "Epoch  76 Batch 2186/2739 train_loss = 1.693\n",
      "Epoch  76 Batch 2236/2739 train_loss = 2.252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  76 Batch 2286/2739 train_loss = 2.320\n",
      "Epoch  76 Batch 2336/2739 train_loss = 3.906\n",
      "Epoch  76 Batch 2386/2739 train_loss = 2.254\n",
      "Epoch  76 Batch 2436/2739 train_loss = 2.395\n",
      "Epoch  76 Batch 2486/2739 train_loss = 2.483\n",
      "Epoch  76 Batch 2536/2739 train_loss = 2.475\n",
      "Epoch  76 Batch 2586/2739 train_loss = 3.937\n",
      "Epoch  76 Batch 2636/2739 train_loss = 2.824\n",
      "Epoch  76 Batch 2686/2739 train_loss = 2.118\n",
      "Epoch  76 Batch 2736/2739 train_loss = 2.053\n",
      "Epoch  77 Batch   47/2739 train_loss = 1.015\n",
      "Epoch  77 Batch   97/2739 train_loss = 2.077\n",
      "Epoch  77 Batch  147/2739 train_loss = 3.026\n",
      "Epoch  77 Batch  197/2739 train_loss = 2.546\n",
      "Epoch  77 Batch  247/2739 train_loss = 3.012\n",
      "Epoch  77 Batch  297/2739 train_loss = 1.369\n",
      "Epoch  77 Batch  347/2739 train_loss = 1.739\n",
      "Epoch  77 Batch  397/2739 train_loss = 1.331\n",
      "Epoch  77 Batch  447/2739 train_loss = 1.956\n",
      "Epoch  77 Batch  497/2739 train_loss = 1.309\n",
      "Epoch  77 Batch  547/2739 train_loss = 1.862\n",
      "Epoch  77 Batch  597/2739 train_loss = 2.554\n",
      "Epoch  77 Batch  647/2739 train_loss = 2.439\n",
      "Epoch  77 Batch  697/2739 train_loss = 1.602\n",
      "Epoch  77 Batch  747/2739 train_loss = 2.416\n",
      "Epoch  77 Batch  797/2739 train_loss = 2.253\n",
      "Epoch  77 Batch  847/2739 train_loss = 3.659\n",
      "Epoch  77 Batch  897/2739 train_loss = 3.251\n",
      "Epoch  77 Batch  947/2739 train_loss = 2.114\n",
      "Epoch  77 Batch  997/2739 train_loss = 2.572\n",
      "Epoch  77 Batch 1047/2739 train_loss = 2.480\n",
      "Epoch  77 Batch 1097/2739 train_loss = 1.569\n",
      "Epoch  77 Batch 1147/2739 train_loss = 2.337\n",
      "Epoch  77 Batch 1197/2739 train_loss = 2.419\n",
      "Epoch  77 Batch 1247/2739 train_loss = 2.238\n",
      "Epoch  77 Batch 1297/2739 train_loss = 1.574\n",
      "Epoch  77 Batch 1347/2739 train_loss = 2.952\n",
      "Epoch  77 Batch 1397/2739 train_loss = 4.210\n",
      "Epoch  77 Batch 1447/2739 train_loss = 2.801\n",
      "Epoch  77 Batch 1497/2739 train_loss = 1.723\n",
      "Epoch  77 Batch 1547/2739 train_loss = 2.767\n",
      "Epoch  77 Batch 1597/2739 train_loss = 1.357\n",
      "Epoch  77 Batch 1647/2739 train_loss = 3.891\n",
      "Epoch  77 Batch 1697/2739 train_loss = 3.218\n",
      "Epoch  77 Batch 1747/2739 train_loss = 3.795\n",
      "Epoch  77 Batch 1797/2739 train_loss = 1.773\n",
      "Epoch  77 Batch 1847/2739 train_loss = 2.326\n",
      "Epoch  77 Batch 1897/2739 train_loss = 1.507\n",
      "Epoch  77 Batch 1947/2739 train_loss = 2.674\n",
      "Epoch  77 Batch 1997/2739 train_loss = 2.368\n",
      "Epoch  77 Batch 2047/2739 train_loss = 3.590\n",
      "Epoch  77 Batch 2097/2739 train_loss = 3.083\n",
      "Epoch  77 Batch 2147/2739 train_loss = 2.250\n",
      "Epoch  77 Batch 2197/2739 train_loss = 2.324\n",
      "Epoch  77 Batch 2247/2739 train_loss = 3.858\n",
      "Epoch  77 Batch 2297/2739 train_loss = 3.825\n",
      "Epoch  77 Batch 2347/2739 train_loss = 3.767\n",
      "Epoch  77 Batch 2397/2739 train_loss = 1.673\n",
      "Epoch  77 Batch 2447/2739 train_loss = 2.768\n",
      "Epoch  77 Batch 2497/2739 train_loss = 3.035\n",
      "Epoch  77 Batch 2547/2739 train_loss = 1.865\n",
      "Epoch  77 Batch 2597/2739 train_loss = 4.308\n",
      "Epoch  77 Batch 2647/2739 train_loss = 2.069\n",
      "Epoch  77 Batch 2697/2739 train_loss = 1.689\n",
      "Epoch  78 Batch    8/2739 train_loss = 2.562\n",
      "Epoch  78 Batch   58/2739 train_loss = 1.459\n",
      "Epoch  78 Batch  108/2739 train_loss = 2.621\n",
      "Epoch  78 Batch  158/2739 train_loss = 2.771\n",
      "Epoch  78 Batch  208/2739 train_loss = 2.023\n",
      "Epoch  78 Batch  258/2739 train_loss = 3.170\n",
      "Epoch  78 Batch  308/2739 train_loss = 2.519\n",
      "Epoch  78 Batch  358/2739 train_loss = 2.004\n",
      "Epoch  78 Batch  408/2739 train_loss = 2.138\n",
      "Epoch  78 Batch  458/2739 train_loss = 3.063\n",
      "Epoch  78 Batch  508/2739 train_loss = 2.307\n",
      "Epoch  78 Batch  558/2739 train_loss = 1.861\n",
      "Epoch  78 Batch  608/2739 train_loss = 2.219\n",
      "Epoch  78 Batch  658/2739 train_loss = 2.652\n",
      "Epoch  78 Batch  708/2739 train_loss = 1.469\n",
      "Epoch  78 Batch  758/2739 train_loss = 3.195\n",
      "Epoch  78 Batch  808/2739 train_loss = 2.342\n",
      "Epoch  78 Batch  858/2739 train_loss = 1.820\n",
      "Epoch  78 Batch  908/2739 train_loss = 2.629\n",
      "Epoch  78 Batch  958/2739 train_loss = 3.449\n",
      "Epoch  78 Batch 1008/2739 train_loss = 1.450\n",
      "Epoch  78 Batch 1058/2739 train_loss = 3.526\n",
      "Epoch  78 Batch 1108/2739 train_loss = 2.283\n",
      "Epoch  78 Batch 1158/2739 train_loss = 2.475\n",
      "Epoch  78 Batch 1208/2739 train_loss = 1.541\n",
      "Epoch  78 Batch 1258/2739 train_loss = 2.396\n",
      "Epoch  78 Batch 1308/2739 train_loss = 2.256\n",
      "Epoch  78 Batch 1358/2739 train_loss = 1.306\n",
      "Epoch  78 Batch 1408/2739 train_loss = 3.082\n",
      "Epoch  78 Batch 1458/2739 train_loss = 2.106\n",
      "Epoch  78 Batch 1508/2739 train_loss = 2.288\n",
      "Epoch  78 Batch 1558/2739 train_loss = 4.310\n",
      "Epoch  78 Batch 1608/2739 train_loss = 1.460\n",
      "Epoch  78 Batch 1658/2739 train_loss = 2.714\n",
      "Epoch  78 Batch 1708/2739 train_loss = 2.138\n",
      "Epoch  78 Batch 1758/2739 train_loss = 3.109\n",
      "Epoch  78 Batch 1808/2739 train_loss = 2.826\n",
      "Epoch  78 Batch 1858/2739 train_loss = 3.414\n",
      "Epoch  78 Batch 1908/2739 train_loss = 3.413\n",
      "Epoch  78 Batch 1958/2739 train_loss = 1.800\n",
      "Epoch  78 Batch 2008/2739 train_loss = 2.593\n",
      "Epoch  78 Batch 2058/2739 train_loss = 3.841\n",
      "Epoch  78 Batch 2108/2739 train_loss = 1.061\n",
      "Epoch  78 Batch 2158/2739 train_loss = 1.804\n",
      "Epoch  78 Batch 2208/2739 train_loss = 3.640\n",
      "Epoch  78 Batch 2258/2739 train_loss = 4.260\n",
      "Epoch  78 Batch 2308/2739 train_loss = 4.147\n",
      "Epoch  78 Batch 2358/2739 train_loss = 1.259\n",
      "Epoch  78 Batch 2408/2739 train_loss = 3.467\n",
      "Epoch  78 Batch 2458/2739 train_loss = 2.711\n",
      "Epoch  78 Batch 2508/2739 train_loss = 1.832\n",
      "Epoch  78 Batch 2558/2739 train_loss = 2.865\n",
      "Epoch  78 Batch 2608/2739 train_loss = 4.197\n",
      "Epoch  78 Batch 2658/2739 train_loss = 1.294\n",
      "Epoch  78 Batch 2708/2739 train_loss = 3.078\n",
      "Epoch  79 Batch   19/2739 train_loss = 1.570\n",
      "Epoch  79 Batch   69/2739 train_loss = 2.588\n",
      "Epoch  79 Batch  119/2739 train_loss = 1.091\n",
      "Epoch  79 Batch  169/2739 train_loss = 2.775\n",
      "Epoch  79 Batch  219/2739 train_loss = 1.211\n",
      "Epoch  79 Batch  269/2739 train_loss = 2.548\n",
      "Epoch  79 Batch  319/2739 train_loss = 2.743\n",
      "Epoch  79 Batch  369/2739 train_loss = 1.842\n",
      "Epoch  79 Batch  419/2739 train_loss = 2.720\n",
      "Epoch  79 Batch  469/2739 train_loss = 2.762\n",
      "Epoch  79 Batch  519/2739 train_loss = 1.425\n",
      "Epoch  79 Batch  569/2739 train_loss = 2.394\n",
      "Epoch  79 Batch  619/2739 train_loss = 1.448\n",
      "Epoch  79 Batch  669/2739 train_loss = 1.560\n",
      "Epoch  79 Batch  719/2739 train_loss = 2.399\n",
      "Epoch  79 Batch  769/2739 train_loss = 3.501\n",
      "Epoch  79 Batch  819/2739 train_loss = 2.536\n",
      "Epoch  79 Batch  869/2739 train_loss = 2.868\n",
      "Epoch  79 Batch  919/2739 train_loss = 2.177\n",
      "Epoch  79 Batch  969/2739 train_loss = 3.637\n",
      "Epoch  79 Batch 1019/2739 train_loss = 1.896\n",
      "Epoch  79 Batch 1069/2739 train_loss = 2.314\n",
      "Epoch  79 Batch 1119/2739 train_loss = 2.421\n",
      "Epoch  79 Batch 1169/2739 train_loss = 2.600\n",
      "Epoch  79 Batch 1219/2739 train_loss = 1.495\n",
      "Epoch  79 Batch 1269/2739 train_loss = 3.297\n",
      "Epoch  79 Batch 1319/2739 train_loss = 1.704\n",
      "Epoch  79 Batch 1369/2739 train_loss = 2.640\n",
      "Epoch  79 Batch 1419/2739 train_loss = 3.582\n",
      "Epoch  79 Batch 1469/2739 train_loss = 1.894\n",
      "Epoch  79 Batch 1519/2739 train_loss = 2.075\n",
      "Epoch  79 Batch 1569/2739 train_loss = 2.156\n",
      "Epoch  79 Batch 1619/2739 train_loss = 2.458\n",
      "Epoch  79 Batch 1669/2739 train_loss = 1.415\n",
      "Epoch  79 Batch 1719/2739 train_loss = 2.295\n",
      "Epoch  79 Batch 1769/2739 train_loss = 3.671\n",
      "Epoch  79 Batch 1819/2739 train_loss = 1.529\n",
      "Epoch  79 Batch 1869/2739 train_loss = 3.593\n",
      "Epoch  79 Batch 1919/2739 train_loss = 1.932\n",
      "Epoch  79 Batch 1969/2739 train_loss = 4.004\n",
      "Epoch  79 Batch 2019/2739 train_loss = 3.599\n",
      "Epoch  79 Batch 2069/2739 train_loss = 2.136\n",
      "Epoch  79 Batch 2119/2739 train_loss = 2.551\n",
      "Epoch  79 Batch 2169/2739 train_loss = 1.592\n",
      "Epoch  79 Batch 2219/2739 train_loss = 3.127\n",
      "Epoch  79 Batch 2269/2739 train_loss = 3.018\n",
      "Epoch  79 Batch 2319/2739 train_loss = 4.238\n",
      "Epoch  79 Batch 2369/2739 train_loss = 3.350\n",
      "Epoch  79 Batch 2419/2739 train_loss = 1.894\n",
      "Epoch  79 Batch 2469/2739 train_loss = 1.406\n",
      "Epoch  79 Batch 2519/2739 train_loss = 2.176\n",
      "Epoch  79 Batch 2569/2739 train_loss = 3.877\n",
      "Epoch  79 Batch 2619/2739 train_loss = 2.432\n",
      "Epoch  79 Batch 2669/2739 train_loss = 2.497\n",
      "Epoch  79 Batch 2719/2739 train_loss = 1.850\n",
      "Epoch  80 Batch   30/2739 train_loss = 3.415\n",
      "Epoch  80 Batch   80/2739 train_loss = 1.293\n",
      "Epoch  80 Batch  130/2739 train_loss = 2.223\n",
      "Epoch  80 Batch  180/2739 train_loss = 3.148\n",
      "Epoch  80 Batch  230/2739 train_loss = 1.955\n",
      "Epoch  80 Batch  280/2739 train_loss = 1.786\n",
      "Epoch  80 Batch  330/2739 train_loss = 2.443\n",
      "Epoch  80 Batch  380/2739 train_loss = 2.268\n",
      "Epoch  80 Batch  430/2739 train_loss = 1.785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  80 Batch  480/2739 train_loss = 3.235\n",
      "Epoch  80 Batch  530/2739 train_loss = 2.414\n",
      "Epoch  80 Batch  580/2739 train_loss = 3.686\n",
      "Epoch  80 Batch  630/2739 train_loss = 2.229\n",
      "Epoch  80 Batch  680/2739 train_loss = 3.646\n",
      "Epoch  80 Batch  730/2739 train_loss = 3.644\n",
      "Epoch  80 Batch  780/2739 train_loss = 2.809\n",
      "Epoch  80 Batch  830/2739 train_loss = 1.890\n",
      "Epoch  80 Batch  880/2739 train_loss = 1.619\n",
      "Epoch  80 Batch  930/2739 train_loss = 1.877\n",
      "Epoch  80 Batch  980/2739 train_loss = 2.546\n",
      "Epoch  80 Batch 1030/2739 train_loss = 2.548\n",
      "Epoch  80 Batch 1080/2739 train_loss = 2.992\n",
      "Epoch  80 Batch 1130/2739 train_loss = 3.790\n",
      "Epoch  80 Batch 1180/2739 train_loss = 3.910\n",
      "Epoch  80 Batch 1230/2739 train_loss = 3.468\n",
      "Epoch  80 Batch 1280/2739 train_loss = 1.893\n",
      "Epoch  80 Batch 1330/2739 train_loss = 4.115\n",
      "Epoch  80 Batch 1380/2739 train_loss = 2.362\n",
      "Epoch  80 Batch 1430/2739 train_loss = 2.524\n",
      "Epoch  80 Batch 1480/2739 train_loss = 2.736\n",
      "Epoch  80 Batch 1530/2739 train_loss = 2.584\n",
      "Epoch  80 Batch 1580/2739 train_loss = 2.059\n",
      "Epoch  80 Batch 1630/2739 train_loss = 2.108\n",
      "Epoch  80 Batch 1680/2739 train_loss = 3.987\n",
      "Epoch  80 Batch 1730/2739 train_loss = 2.232\n",
      "Epoch  80 Batch 1780/2739 train_loss = 1.873\n",
      "Epoch  80 Batch 1830/2739 train_loss = 2.141\n",
      "Epoch  80 Batch 1880/2739 train_loss = 2.382\n",
      "Epoch  80 Batch 1930/2739 train_loss = 4.608\n",
      "Epoch  80 Batch 1980/2739 train_loss = 2.101\n",
      "Epoch  80 Batch 2030/2739 train_loss = 3.522\n",
      "Epoch  80 Batch 2080/2739 train_loss = 3.048\n",
      "Epoch  80 Batch 2130/2739 train_loss = 3.903\n",
      "Epoch  80 Batch 2180/2739 train_loss = 2.230\n",
      "Epoch  80 Batch 2230/2739 train_loss = 1.013\n",
      "Epoch  80 Batch 2280/2739 train_loss = 1.750\n",
      "Epoch  80 Batch 2330/2739 train_loss = 1.198\n",
      "Epoch  80 Batch 2380/2739 train_loss = 2.557\n",
      "Epoch  80 Batch 2430/2739 train_loss = 1.667\n",
      "Epoch  80 Batch 2480/2739 train_loss = 2.480\n",
      "Epoch  80 Batch 2530/2739 train_loss = 1.843\n",
      "Epoch  80 Batch 2580/2739 train_loss = 2.473\n",
      "Epoch  80 Batch 2630/2739 train_loss = 1.626\n",
      "Epoch  80 Batch 2680/2739 train_loss = 1.807\n",
      "Epoch  80 Batch 2730/2739 train_loss = 3.602\n",
      "Epoch  81 Batch   41/2739 train_loss = 3.997\n",
      "Epoch  81 Batch   91/2739 train_loss = 1.192\n",
      "Epoch  81 Batch  141/2739 train_loss = 2.560\n",
      "Epoch  81 Batch  191/2739 train_loss = 2.563\n",
      "Epoch  81 Batch  241/2739 train_loss = 2.526\n",
      "Epoch  81 Batch  291/2739 train_loss = 2.849\n",
      "Epoch  81 Batch  341/2739 train_loss = 3.961\n",
      "Epoch  81 Batch  391/2739 train_loss = 2.393\n",
      "Epoch  81 Batch  441/2739 train_loss = 2.301\n",
      "Epoch  81 Batch  491/2739 train_loss = 2.990\n",
      "Epoch  81 Batch  541/2739 train_loss = 2.024\n",
      "Epoch  81 Batch  591/2739 train_loss = 2.234\n",
      "Epoch  81 Batch  641/2739 train_loss = 3.574\n",
      "Epoch  81 Batch  691/2739 train_loss = 1.935\n",
      "Epoch  81 Batch  741/2739 train_loss = 1.074\n",
      "Epoch  81 Batch  791/2739 train_loss = 3.860\n",
      "Epoch  81 Batch  841/2739 train_loss = 1.705\n",
      "Epoch  81 Batch  891/2739 train_loss = 1.779\n",
      "Epoch  81 Batch  941/2739 train_loss = 2.660\n",
      "Epoch  81 Batch  991/2739 train_loss = 3.193\n",
      "Epoch  81 Batch 1041/2739 train_loss = 2.147\n",
      "Epoch  81 Batch 1091/2739 train_loss = 2.245\n",
      "Epoch  81 Batch 1141/2739 train_loss = 2.414\n",
      "Epoch  81 Batch 1191/2739 train_loss = 2.241\n",
      "Epoch  81 Batch 1241/2739 train_loss = 2.226\n",
      "Epoch  81 Batch 1291/2739 train_loss = 1.763\n",
      "Epoch  81 Batch 1341/2739 train_loss = 4.127\n",
      "Epoch  81 Batch 1391/2739 train_loss = 3.597\n",
      "Epoch  81 Batch 1441/2739 train_loss = 1.686\n",
      "Epoch  81 Batch 1491/2739 train_loss = 1.497\n",
      "Epoch  81 Batch 1541/2739 train_loss = 2.571\n",
      "Epoch  81 Batch 1591/2739 train_loss = 2.768\n",
      "Epoch  81 Batch 1641/2739 train_loss = 1.925\n",
      "Epoch  81 Batch 1691/2739 train_loss = 3.413\n",
      "Epoch  81 Batch 1741/2739 train_loss = 2.495\n",
      "Epoch  81 Batch 1791/2739 train_loss = 2.774\n",
      "Epoch  81 Batch 1841/2739 train_loss = 2.854\n",
      "Epoch  81 Batch 1891/2739 train_loss = 2.225\n",
      "Epoch  81 Batch 1941/2739 train_loss = 1.814\n",
      "Epoch  81 Batch 1991/2739 train_loss = 1.364\n",
      "Epoch  81 Batch 2041/2739 train_loss = 3.770\n",
      "Epoch  81 Batch 2091/2739 train_loss = 3.221\n",
      "Epoch  81 Batch 2141/2739 train_loss = 3.554\n",
      "Epoch  81 Batch 2191/2739 train_loss = 3.172\n",
      "Epoch  81 Batch 2241/2739 train_loss = 2.401\n",
      "Epoch  81 Batch 2291/2739 train_loss = 1.442\n",
      "Epoch  81 Batch 2341/2739 train_loss = 3.796\n",
      "Epoch  81 Batch 2391/2739 train_loss = 2.227\n",
      "Epoch  81 Batch 2441/2739 train_loss = 2.777\n",
      "Epoch  81 Batch 2491/2739 train_loss = 2.656\n",
      "Epoch  81 Batch 2541/2739 train_loss = 2.152\n",
      "Epoch  81 Batch 2591/2739 train_loss = 3.634\n",
      "Epoch  81 Batch 2641/2739 train_loss = 2.068\n",
      "Epoch  81 Batch 2691/2739 train_loss = 1.777\n",
      "Epoch  82 Batch    2/2739 train_loss = 2.622\n",
      "Epoch  82 Batch   52/2739 train_loss = 1.979\n",
      "Epoch  82 Batch  102/2739 train_loss = 2.066\n",
      "Epoch  82 Batch  152/2739 train_loss = 2.805\n",
      "Epoch  82 Batch  202/2739 train_loss = 2.542\n",
      "Epoch  82 Batch  252/2739 train_loss = 2.058\n",
      "Epoch  82 Batch  302/2739 train_loss = 2.534\n",
      "Epoch  82 Batch  352/2739 train_loss = 2.098\n",
      "Epoch  82 Batch  402/2739 train_loss = 1.952\n",
      "Epoch  82 Batch  452/2739 train_loss = 2.352\n",
      "Epoch  82 Batch  502/2739 train_loss = 2.380\n",
      "Epoch  82 Batch  552/2739 train_loss = 1.777\n",
      "Epoch  82 Batch  602/2739 train_loss = 1.439\n",
      "Epoch  82 Batch  652/2739 train_loss = 2.583\n",
      "Epoch  82 Batch  702/2739 train_loss = 3.017\n",
      "Epoch  82 Batch  752/2739 train_loss = 1.847\n",
      "Epoch  82 Batch  802/2739 train_loss = 2.621\n",
      "Epoch  82 Batch  852/2739 train_loss = 2.177\n",
      "Epoch  82 Batch  902/2739 train_loss = 3.859\n",
      "Epoch  82 Batch  952/2739 train_loss = 1.339\n",
      "Epoch  82 Batch 1002/2739 train_loss = 2.556\n",
      "Epoch  82 Batch 1052/2739 train_loss = 2.384\n",
      "Epoch  82 Batch 1102/2739 train_loss = 2.117\n",
      "Epoch  82 Batch 1152/2739 train_loss = 2.361\n",
      "Epoch  82 Batch 1202/2739 train_loss = 1.873\n",
      "Epoch  82 Batch 1252/2739 train_loss = 2.870\n",
      "Epoch  82 Batch 1302/2739 train_loss = 2.345\n",
      "Epoch  82 Batch 1352/2739 train_loss = 2.649\n",
      "Epoch  82 Batch 1402/2739 train_loss = 3.567\n",
      "Epoch  82 Batch 1452/2739 train_loss = 1.511\n",
      "Epoch  82 Batch 1502/2739 train_loss = 1.316\n",
      "Epoch  82 Batch 1552/2739 train_loss = 1.539\n",
      "Epoch  82 Batch 1602/2739 train_loss = 3.859\n",
      "Epoch  82 Batch 1652/2739 train_loss = 3.556\n",
      "Epoch  82 Batch 1702/2739 train_loss = 2.124\n",
      "Epoch  82 Batch 1752/2739 train_loss = 2.141\n",
      "Epoch  82 Batch 1802/2739 train_loss = 4.129\n",
      "Epoch  82 Batch 1852/2739 train_loss = 3.110\n",
      "Epoch  82 Batch 1902/2739 train_loss = 2.600\n",
      "Epoch  82 Batch 1952/2739 train_loss = 1.355\n",
      "Epoch  82 Batch 2002/2739 train_loss = 2.094\n",
      "Epoch  82 Batch 2052/2739 train_loss = 3.976\n",
      "Epoch  82 Batch 2102/2739 train_loss = 2.126\n",
      "Epoch  82 Batch 2152/2739 train_loss = 4.080\n",
      "Epoch  82 Batch 2202/2739 train_loss = 3.273\n",
      "Epoch  82 Batch 2252/2739 train_loss = 3.622\n",
      "Epoch  82 Batch 2302/2739 train_loss = 3.119\n",
      "Epoch  82 Batch 2352/2739 train_loss = 2.433\n",
      "Epoch  82 Batch 2402/2739 train_loss = 1.977\n",
      "Epoch  82 Batch 2452/2739 train_loss = 1.482\n",
      "Epoch  82 Batch 2502/2739 train_loss = 2.267\n",
      "Epoch  82 Batch 2552/2739 train_loss = 4.024\n",
      "Epoch  82 Batch 2602/2739 train_loss = 2.359\n",
      "Epoch  82 Batch 2652/2739 train_loss = 1.189\n",
      "Epoch  82 Batch 2702/2739 train_loss = 1.823\n",
      "Epoch  83 Batch   13/2739 train_loss = 2.306\n",
      "Epoch  83 Batch   63/2739 train_loss = 1.263\n",
      "Epoch  83 Batch  113/2739 train_loss = 2.951\n",
      "Epoch  83 Batch  163/2739 train_loss = 2.059\n",
      "Epoch  83 Batch  213/2739 train_loss = 2.713\n",
      "Epoch  83 Batch  263/2739 train_loss = 2.134\n",
      "Epoch  83 Batch  313/2739 train_loss = 2.212\n",
      "Epoch  83 Batch  363/2739 train_loss = 2.633\n",
      "Epoch  83 Batch  413/2739 train_loss = 2.852\n",
      "Epoch  83 Batch  463/2739 train_loss = 2.484\n",
      "Epoch  83 Batch  513/2739 train_loss = 2.500\n",
      "Epoch  83 Batch  563/2739 train_loss = 1.466\n",
      "Epoch  83 Batch  613/2739 train_loss = 2.630\n",
      "Epoch  83 Batch  663/2739 train_loss = 2.244\n",
      "Epoch  83 Batch  713/2739 train_loss = 1.399\n",
      "Epoch  83 Batch  763/2739 train_loss = 2.305\n",
      "Epoch  83 Batch  813/2739 train_loss = 1.954\n",
      "Epoch  83 Batch  863/2739 train_loss = 3.613\n",
      "Epoch  83 Batch  913/2739 train_loss = 2.131\n",
      "Epoch  83 Batch  963/2739 train_loss = 3.925\n",
      "Epoch  83 Batch 1013/2739 train_loss = 2.045\n",
      "Epoch  83 Batch 1063/2739 train_loss = 3.121\n",
      "Epoch  83 Batch 1113/2739 train_loss = 2.332\n",
      "Epoch  83 Batch 1163/2739 train_loss = 3.702\n",
      "Epoch  83 Batch 1213/2739 train_loss = 3.673\n",
      "Epoch  83 Batch 1263/2739 train_loss = 1.964\n",
      "Epoch  83 Batch 1313/2739 train_loss = 1.980\n",
      "Epoch  83 Batch 1363/2739 train_loss = 1.541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  83 Batch 1413/2739 train_loss = 2.281\n",
      "Epoch  83 Batch 1463/2739 train_loss = 1.974\n",
      "Epoch  83 Batch 1513/2739 train_loss = 2.197\n",
      "Epoch  83 Batch 1563/2739 train_loss = 2.335\n",
      "Epoch  83 Batch 1613/2739 train_loss = 1.840\n",
      "Epoch  83 Batch 1663/2739 train_loss = 1.959\n",
      "Epoch  83 Batch 1713/2739 train_loss = 1.495\n",
      "Epoch  83 Batch 1763/2739 train_loss = 1.393\n",
      "Epoch  83 Batch 1813/2739 train_loss = 2.040\n",
      "Epoch  83 Batch 1863/2739 train_loss = 2.775\n",
      "Epoch  83 Batch 1913/2739 train_loss = 3.606\n",
      "Epoch  83 Batch 1963/2739 train_loss = 1.933\n",
      "Epoch  83 Batch 2013/2739 train_loss = 2.749\n",
      "Epoch  83 Batch 2063/2739 train_loss = 2.385\n",
      "Epoch  83 Batch 2113/2739 train_loss = 3.733\n",
      "Epoch  83 Batch 2163/2739 train_loss = 1.603\n",
      "Epoch  83 Batch 2213/2739 train_loss = 1.897\n",
      "Epoch  83 Batch 2263/2739 train_loss = 2.329\n",
      "Epoch  83 Batch 2313/2739 train_loss = 1.348\n",
      "Epoch  83 Batch 2363/2739 train_loss = 1.481\n",
      "Epoch  83 Batch 2413/2739 train_loss = 1.621\n",
      "Epoch  83 Batch 2463/2739 train_loss = 2.230\n",
      "Epoch  83 Batch 2513/2739 train_loss = 2.929\n",
      "Epoch  83 Batch 2563/2739 train_loss = 1.975\n",
      "Epoch  83 Batch 2613/2739 train_loss = 3.867\n",
      "Epoch  83 Batch 2663/2739 train_loss = 1.455\n",
      "Epoch  83 Batch 2713/2739 train_loss = 2.278\n",
      "Epoch  84 Batch   24/2739 train_loss = 1.675\n",
      "Epoch  84 Batch   74/2739 train_loss = 2.987\n",
      "Epoch  84 Batch  124/2739 train_loss = 2.291\n",
      "Epoch  84 Batch  174/2739 train_loss = 2.603\n",
      "Epoch  84 Batch  224/2739 train_loss = 2.857\n",
      "Epoch  84 Batch  274/2739 train_loss = 2.239\n",
      "Epoch  84 Batch  324/2739 train_loss = 3.231\n",
      "Epoch  84 Batch  374/2739 train_loss = 1.841\n",
      "Epoch  84 Batch  424/2739 train_loss = 2.780\n",
      "Epoch  84 Batch  474/2739 train_loss = 1.963\n",
      "Epoch  84 Batch  524/2739 train_loss = 1.382\n",
      "Epoch  84 Batch  574/2739 train_loss = 1.743\n",
      "Epoch  84 Batch  624/2739 train_loss = 2.322\n",
      "Epoch  84 Batch  674/2739 train_loss = 2.384\n",
      "Epoch  84 Batch  724/2739 train_loss = 3.180\n",
      "Epoch  84 Batch  774/2739 train_loss = 1.028\n",
      "Epoch  84 Batch  824/2739 train_loss = 1.706\n",
      "Epoch  84 Batch  874/2739 train_loss = 1.642\n",
      "Epoch  84 Batch  924/2739 train_loss = 2.144\n",
      "Epoch  84 Batch  974/2739 train_loss = 4.119\n",
      "Epoch  84 Batch 1024/2739 train_loss = 1.572\n",
      "Epoch  84 Batch 1074/2739 train_loss = 3.447\n",
      "Epoch  84 Batch 1124/2739 train_loss = 3.513\n",
      "Epoch  84 Batch 1174/2739 train_loss = 3.319\n",
      "Epoch  84 Batch 1224/2739 train_loss = 2.008\n",
      "Epoch  84 Batch 1274/2739 train_loss = 2.780\n",
      "Epoch  84 Batch 1324/2739 train_loss = 2.291\n",
      "Epoch  84 Batch 1374/2739 train_loss = 2.390\n",
      "Epoch  84 Batch 1424/2739 train_loss = 2.718\n",
      "Epoch  84 Batch 1474/2739 train_loss = 2.252\n",
      "Epoch  84 Batch 1524/2739 train_loss = 3.676\n",
      "Epoch  84 Batch 1574/2739 train_loss = 3.507\n",
      "Epoch  84 Batch 1624/2739 train_loss = 3.432\n",
      "Epoch  84 Batch 1674/2739 train_loss = 3.961\n",
      "Epoch  84 Batch 1724/2739 train_loss = 2.471\n",
      "Epoch  84 Batch 1774/2739 train_loss = 2.673\n",
      "Epoch  84 Batch 1824/2739 train_loss = 3.991\n",
      "Epoch  84 Batch 1874/2739 train_loss = 1.728\n",
      "Epoch  84 Batch 1924/2739 train_loss = 2.005\n",
      "Epoch  84 Batch 1974/2739 train_loss = 2.227\n",
      "Epoch  84 Batch 2024/2739 train_loss = 1.508\n",
      "Epoch  84 Batch 2074/2739 train_loss = 2.716\n",
      "Epoch  84 Batch 2124/2739 train_loss = 1.806\n",
      "Epoch  84 Batch 2174/2739 train_loss = 2.089\n",
      "Epoch  84 Batch 2224/2739 train_loss = 1.370\n",
      "Epoch  84 Batch 2274/2739 train_loss = 2.309\n",
      "Epoch  84 Batch 2324/2739 train_loss = 1.379\n",
      "Epoch  84 Batch 2374/2739 train_loss = 2.536\n",
      "Epoch  84 Batch 2424/2739 train_loss = 1.998\n",
      "Epoch  84 Batch 2474/2739 train_loss = 1.215\n",
      "Epoch  84 Batch 2524/2739 train_loss = 2.364\n",
      "Epoch  84 Batch 2574/2739 train_loss = 2.115\n",
      "Epoch  84 Batch 2624/2739 train_loss = 3.506\n",
      "Epoch  84 Batch 2674/2739 train_loss = 2.003\n",
      "Epoch  84 Batch 2724/2739 train_loss = 3.722\n",
      "Epoch  85 Batch   35/2739 train_loss = 2.431\n",
      "Epoch  85 Batch   85/2739 train_loss = 3.081\n",
      "Epoch  85 Batch  135/2739 train_loss = 1.744\n",
      "Epoch  85 Batch  185/2739 train_loss = 2.911\n",
      "Epoch  85 Batch  235/2739 train_loss = 2.343\n",
      "Epoch  85 Batch  285/2739 train_loss = 1.838\n",
      "Epoch  85 Batch  335/2739 train_loss = 2.067\n",
      "Epoch  85 Batch  385/2739 train_loss = 2.216\n",
      "Epoch  85 Batch  435/2739 train_loss = 3.697\n",
      "Epoch  85 Batch  485/2739 train_loss = 3.449\n",
      "Epoch  85 Batch  535/2739 train_loss = 2.133\n",
      "Epoch  85 Batch  585/2739 train_loss = 2.414\n",
      "Epoch  85 Batch  635/2739 train_loss = 1.627\n",
      "Epoch  85 Batch  685/2739 train_loss = 2.262\n",
      "Epoch  85 Batch  735/2739 train_loss = 2.657\n",
      "Epoch  85 Batch  785/2739 train_loss = 3.265\n",
      "Epoch  85 Batch  835/2739 train_loss = 1.921\n",
      "Epoch  85 Batch  885/2739 train_loss = 2.291\n",
      "Epoch  85 Batch  935/2739 train_loss = 2.895\n",
      "Epoch  85 Batch  985/2739 train_loss = 2.004\n",
      "Epoch  85 Batch 1035/2739 train_loss = 3.208\n",
      "Epoch  85 Batch 1085/2739 train_loss = 1.275\n",
      "Epoch  85 Batch 1135/2739 train_loss = 2.627\n",
      "Epoch  85 Batch 1185/2739 train_loss = 2.624\n",
      "Epoch  85 Batch 1235/2739 train_loss = 2.181\n",
      "Epoch  85 Batch 1285/2739 train_loss = 1.786\n",
      "Epoch  85 Batch 1335/2739 train_loss = 3.659\n",
      "Epoch  85 Batch 1385/2739 train_loss = 1.779\n",
      "Epoch  85 Batch 1435/2739 train_loss = 2.747\n",
      "Epoch  85 Batch 1485/2739 train_loss = 1.772\n",
      "Epoch  85 Batch 1535/2739 train_loss = 1.862\n",
      "Epoch  85 Batch 1585/2739 train_loss = 1.127\n",
      "Epoch  85 Batch 1635/2739 train_loss = 2.049\n",
      "Epoch  85 Batch 1685/2739 train_loss = 2.177\n",
      "Epoch  85 Batch 1735/2739 train_loss = 2.712\n",
      "Epoch  85 Batch 1785/2739 train_loss = 2.287\n",
      "Epoch  85 Batch 1835/2739 train_loss = 4.014\n",
      "Epoch  85 Batch 1885/2739 train_loss = 3.485\n",
      "Epoch  85 Batch 1935/2739 train_loss = 2.474\n",
      "Epoch  85 Batch 1985/2739 train_loss = 2.116\n",
      "Epoch  85 Batch 2035/2739 train_loss = 2.027\n",
      "Epoch  85 Batch 2085/2739 train_loss = 3.852\n",
      "Epoch  85 Batch 2135/2739 train_loss = 4.181\n",
      "Epoch  85 Batch 2185/2739 train_loss = 2.450\n",
      "Epoch  85 Batch 2235/2739 train_loss = 3.943\n",
      "Epoch  85 Batch 2285/2739 train_loss = 3.172\n",
      "Epoch  85 Batch 2335/2739 train_loss = 2.879\n",
      "Epoch  85 Batch 2385/2739 train_loss = 1.369\n",
      "Epoch  85 Batch 2435/2739 train_loss = 2.236\n",
      "Epoch  85 Batch 2485/2739 train_loss = 1.293\n",
      "Epoch  85 Batch 2535/2739 train_loss = 3.008\n",
      "Epoch  85 Batch 2585/2739 train_loss = 3.796\n",
      "Epoch  85 Batch 2635/2739 train_loss = 1.487\n",
      "Epoch  85 Batch 2685/2739 train_loss = 1.444\n",
      "Epoch  85 Batch 2735/2739 train_loss = 2.194\n",
      "Epoch  86 Batch   46/2739 train_loss = 2.446\n",
      "Epoch  86 Batch   96/2739 train_loss = 4.262\n",
      "Epoch  86 Batch  146/2739 train_loss = 1.799\n",
      "Epoch  86 Batch  196/2739 train_loss = 1.399\n",
      "Epoch  86 Batch  246/2739 train_loss = 1.701\n",
      "Epoch  86 Batch  296/2739 train_loss = 1.981\n",
      "Epoch  86 Batch  346/2739 train_loss = 2.248\n",
      "Epoch  86 Batch  396/2739 train_loss = 2.310\n",
      "Epoch  86 Batch  446/2739 train_loss = 2.180\n",
      "Epoch  86 Batch  496/2739 train_loss = 2.421\n",
      "Epoch  86 Batch  546/2739 train_loss = 2.057\n",
      "Epoch  86 Batch  596/2739 train_loss = 2.337\n",
      "Epoch  86 Batch  646/2739 train_loss = 1.562\n",
      "Epoch  86 Batch  696/2739 train_loss = 1.851\n",
      "Epoch  86 Batch  746/2739 train_loss = 3.241\n",
      "Epoch  86 Batch  796/2739 train_loss = 1.986\n",
      "Epoch  86 Batch  846/2739 train_loss = 1.829\n",
      "Epoch  86 Batch  896/2739 train_loss = 1.772\n",
      "Epoch  86 Batch  946/2739 train_loss = 4.476\n",
      "Epoch  86 Batch  996/2739 train_loss = 2.541\n",
      "Epoch  86 Batch 1046/2739 train_loss = 2.158\n",
      "Epoch  86 Batch 1096/2739 train_loss = 2.899\n",
      "Epoch  86 Batch 1146/2739 train_loss = 2.553\n",
      "Epoch  86 Batch 1196/2739 train_loss = 1.204\n",
      "Epoch  86 Batch 1246/2739 train_loss = 2.123\n",
      "Epoch  86 Batch 1296/2739 train_loss = 2.150\n",
      "Epoch  86 Batch 1346/2739 train_loss = 1.560\n",
      "Epoch  86 Batch 1396/2739 train_loss = 3.085\n",
      "Epoch  86 Batch 1446/2739 train_loss = 2.078\n",
      "Epoch  86 Batch 1496/2739 train_loss = 1.377\n",
      "Epoch  86 Batch 1546/2739 train_loss = 2.244\n",
      "Epoch  86 Batch 1596/2739 train_loss = 3.232\n",
      "Epoch  86 Batch 1646/2739 train_loss = 2.140\n",
      "Epoch  86 Batch 1696/2739 train_loss = 1.896\n",
      "Epoch  86 Batch 1746/2739 train_loss = 3.982\n",
      "Epoch  86 Batch 1796/2739 train_loss = 2.443\n",
      "Epoch  86 Batch 1846/2739 train_loss = 1.450\n",
      "Epoch  86 Batch 1896/2739 train_loss = 2.654\n",
      "Epoch  86 Batch 1946/2739 train_loss = 4.497\n",
      "Epoch  86 Batch 1996/2739 train_loss = 1.212\n",
      "Epoch  86 Batch 2046/2739 train_loss = 3.587\n",
      "Epoch  86 Batch 2096/2739 train_loss = 2.305\n",
      "Epoch  86 Batch 2146/2739 train_loss = 2.820\n",
      "Epoch  86 Batch 2196/2739 train_loss = 3.977\n",
      "Epoch  86 Batch 2246/2739 train_loss = 2.048\n",
      "Epoch  86 Batch 2296/2739 train_loss = 3.872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  86 Batch 2346/2739 train_loss = 1.914\n",
      "Epoch  86 Batch 2396/2739 train_loss = 2.771\n",
      "Epoch  86 Batch 2446/2739 train_loss = 2.847\n",
      "Epoch  86 Batch 2496/2739 train_loss = 2.503\n",
      "Epoch  86 Batch 2546/2739 train_loss = 1.375\n",
      "Epoch  86 Batch 2596/2739 train_loss = 4.179\n",
      "Epoch  86 Batch 2646/2739 train_loss = 1.321\n",
      "Epoch  86 Batch 2696/2739 train_loss = 2.458\n",
      "Epoch  87 Batch    7/2739 train_loss = 3.088\n",
      "Epoch  87 Batch   57/2739 train_loss = 2.355\n",
      "Epoch  87 Batch  107/2739 train_loss = 1.993\n",
      "Epoch  87 Batch  157/2739 train_loss = 1.898\n",
      "Epoch  87 Batch  207/2739 train_loss = 1.972\n",
      "Epoch  87 Batch  257/2739 train_loss = 2.393\n",
      "Epoch  87 Batch  307/2739 train_loss = 1.847\n",
      "Epoch  87 Batch  357/2739 train_loss = 1.943\n",
      "Epoch  87 Batch  407/2739 train_loss = 2.731\n",
      "Epoch  87 Batch  457/2739 train_loss = 1.746\n",
      "Epoch  87 Batch  507/2739 train_loss = 1.906\n",
      "Epoch  87 Batch  557/2739 train_loss = 1.375\n",
      "Epoch  87 Batch  607/2739 train_loss = 3.784\n",
      "Epoch  87 Batch  657/2739 train_loss = 2.023\n",
      "Epoch  87 Batch  707/2739 train_loss = 1.761\n",
      "Epoch  87 Batch  757/2739 train_loss = 3.364\n",
      "Epoch  87 Batch  807/2739 train_loss = 2.753\n",
      "Epoch  87 Batch  857/2739 train_loss = 2.508\n",
      "Epoch  87 Batch  907/2739 train_loss = 3.592\n",
      "Epoch  87 Batch  957/2739 train_loss = 4.040\n",
      "Epoch  87 Batch 1007/2739 train_loss = 2.626\n",
      "Epoch  87 Batch 1057/2739 train_loss = 3.546\n",
      "Epoch  87 Batch 1107/2739 train_loss = 1.147\n",
      "Epoch  87 Batch 1157/2739 train_loss = 2.867\n",
      "Epoch  87 Batch 1207/2739 train_loss = 2.308\n",
      "Epoch  87 Batch 1257/2739 train_loss = 1.788\n",
      "Epoch  87 Batch 1307/2739 train_loss = 2.140\n",
      "Epoch  87 Batch 1357/2739 train_loss = 1.960\n",
      "Epoch  87 Batch 1407/2739 train_loss = 3.180\n",
      "Epoch  87 Batch 1457/2739 train_loss = 1.742\n",
      "Epoch  87 Batch 1507/2739 train_loss = 2.459\n",
      "Epoch  87 Batch 1557/2739 train_loss = 2.365\n",
      "Epoch  87 Batch 1607/2739 train_loss = 2.001\n",
      "Epoch  87 Batch 1657/2739 train_loss = 2.746\n",
      "Epoch  87 Batch 1707/2739 train_loss = 1.760\n",
      "Epoch  87 Batch 1757/2739 train_loss = 1.958\n",
      "Epoch  87 Batch 1807/2739 train_loss = 4.160\n",
      "Epoch  87 Batch 1857/2739 train_loss = 1.835\n",
      "Epoch  87 Batch 1907/2739 train_loss = 2.302\n",
      "Epoch  87 Batch 1957/2739 train_loss = 2.149\n",
      "Epoch  87 Batch 2007/2739 train_loss = 3.210\n",
      "Epoch  87 Batch 2057/2739 train_loss = 3.546\n",
      "Epoch  87 Batch 2107/2739 train_loss = 3.187\n",
      "Epoch  87 Batch 2157/2739 train_loss = 2.761\n",
      "Epoch  87 Batch 2207/2739 train_loss = 3.541\n",
      "Epoch  87 Batch 2257/2739 train_loss = 2.084\n",
      "Epoch  87 Batch 2307/2739 train_loss = 4.143\n",
      "Epoch  87 Batch 2357/2739 train_loss = 1.781\n",
      "Epoch  87 Batch 2407/2739 train_loss = 1.177\n",
      "Epoch  87 Batch 2457/2739 train_loss = 2.857\n",
      "Epoch  87 Batch 2507/2739 train_loss = 2.877\n",
      "Epoch  87 Batch 2557/2739 train_loss = 1.406\n",
      "Epoch  87 Batch 2607/2739 train_loss = 4.211\n",
      "Epoch  87 Batch 2657/2739 train_loss = 1.240\n",
      "Epoch  87 Batch 2707/2739 train_loss = 3.204\n",
      "Epoch  88 Batch   18/2739 train_loss = 1.781\n",
      "Epoch  88 Batch   68/2739 train_loss = 1.822\n",
      "Epoch  88 Batch  118/2739 train_loss = 1.483\n",
      "Epoch  88 Batch  168/2739 train_loss = 1.570\n",
      "Epoch  88 Batch  218/2739 train_loss = 2.155\n",
      "Epoch  88 Batch  268/2739 train_loss = 2.405\n",
      "Epoch  88 Batch  318/2739 train_loss = 3.585\n",
      "Epoch  88 Batch  368/2739 train_loss = 2.302\n",
      "Epoch  88 Batch  418/2739 train_loss = 2.369\n",
      "Epoch  88 Batch  468/2739 train_loss = 1.119\n",
      "Epoch  88 Batch  518/2739 train_loss = 2.469\n",
      "Epoch  88 Batch  568/2739 train_loss = 2.863\n",
      "Epoch  88 Batch  618/2739 train_loss = 2.943\n",
      "Epoch  88 Batch  668/2739 train_loss = 2.232\n",
      "Epoch  88 Batch  718/2739 train_loss = 2.328\n",
      "Epoch  88 Batch  768/2739 train_loss = 2.466\n",
      "Epoch  88 Batch  818/2739 train_loss = 2.405\n",
      "Epoch  88 Batch  868/2739 train_loss = 3.503\n",
      "Epoch  88 Batch  918/2739 train_loss = 1.998\n",
      "Epoch  88 Batch  968/2739 train_loss = 3.766\n",
      "Epoch  88 Batch 1018/2739 train_loss = 2.641\n",
      "Epoch  88 Batch 1068/2739 train_loss = 1.738\n",
      "Epoch  88 Batch 1118/2739 train_loss = 2.158\n",
      "Epoch  88 Batch 1168/2739 train_loss = 2.345\n",
      "Epoch  88 Batch 1218/2739 train_loss = 1.634\n",
      "Epoch  88 Batch 1268/2739 train_loss = 3.258\n",
      "Epoch  88 Batch 1318/2739 train_loss = 2.660\n",
      "Epoch  88 Batch 1368/2739 train_loss = 1.965\n",
      "Epoch  88 Batch 1418/2739 train_loss = 3.259\n",
      "Epoch  88 Batch 1468/2739 train_loss = 2.383\n",
      "Epoch  88 Batch 1518/2739 train_loss = 2.418\n",
      "Epoch  88 Batch 1568/2739 train_loss = 1.581\n",
      "Epoch  88 Batch 1618/2739 train_loss = 3.544\n",
      "Epoch  88 Batch 1668/2739 train_loss = 2.325\n",
      "Epoch  88 Batch 1718/2739 train_loss = 1.954\n",
      "Epoch  88 Batch 1768/2739 train_loss = 1.920\n",
      "Epoch  88 Batch 1818/2739 train_loss = 2.066\n",
      "Epoch  88 Batch 1868/2739 train_loss = 3.370\n",
      "Epoch  88 Batch 1918/2739 train_loss = 2.059\n",
      "Epoch  88 Batch 1968/2739 train_loss = 1.480\n",
      "Epoch  88 Batch 2018/2739 train_loss = 2.155\n",
      "Epoch  88 Batch 2068/2739 train_loss = 3.215\n",
      "Epoch  88 Batch 2118/2739 train_loss = 2.273\n",
      "Epoch  88 Batch 2168/2739 train_loss = 1.670\n",
      "Epoch  88 Batch 2218/2739 train_loss = 3.542\n",
      "Epoch  88 Batch 2268/2739 train_loss = 4.337\n",
      "Epoch  88 Batch 2318/2739 train_loss = 4.361\n",
      "Epoch  88 Batch 2368/2739 train_loss = 1.980\n",
      "Epoch  88 Batch 2418/2739 train_loss = 3.126\n",
      "Epoch  88 Batch 2468/2739 train_loss = 1.643\n",
      "Epoch  88 Batch 2518/2739 train_loss = 3.086\n",
      "Epoch  88 Batch 2568/2739 train_loss = 2.183\n",
      "Epoch  88 Batch 2618/2739 train_loss = 1.825\n",
      "Epoch  88 Batch 2668/2739 train_loss = 2.623\n",
      "Epoch  88 Batch 2718/2739 train_loss = 1.811\n",
      "Epoch  89 Batch   29/2739 train_loss = 2.734\n",
      "Epoch  89 Batch   79/2739 train_loss = 2.646\n",
      "Epoch  89 Batch  129/2739 train_loss = 2.447\n",
      "Epoch  89 Batch  179/2739 train_loss = 2.491\n",
      "Epoch  89 Batch  229/2739 train_loss = 2.323\n",
      "Epoch  89 Batch  279/2739 train_loss = 1.175\n",
      "Epoch  89 Batch  329/2739 train_loss = 2.258\n",
      "Epoch  89 Batch  379/2739 train_loss = 1.921\n",
      "Epoch  89 Batch  429/2739 train_loss = 2.278\n",
      "Epoch  89 Batch  479/2739 train_loss = 3.375\n",
      "Epoch  89 Batch  529/2739 train_loss = 3.340\n",
      "Epoch  89 Batch  579/2739 train_loss = 2.717\n",
      "Epoch  89 Batch  629/2739 train_loss = 3.221\n",
      "Epoch  89 Batch  679/2739 train_loss = 3.528\n",
      "Epoch  89 Batch  729/2739 train_loss = 3.675\n",
      "Epoch  89 Batch  779/2739 train_loss = 3.337\n",
      "Epoch  89 Batch  829/2739 train_loss = 2.531\n",
      "Epoch  89 Batch  879/2739 train_loss = 2.083\n",
      "Epoch  89 Batch  929/2739 train_loss = 1.813\n",
      "Epoch  89 Batch  979/2739 train_loss = 4.263\n",
      "Epoch  89 Batch 1029/2739 train_loss = 2.754\n",
      "Epoch  89 Batch 1079/2739 train_loss = 1.929\n",
      "Epoch  89 Batch 1129/2739 train_loss = 1.551\n",
      "Epoch  89 Batch 1179/2739 train_loss = 2.287\n",
      "Epoch  89 Batch 1229/2739 train_loss = 3.144\n",
      "Epoch  89 Batch 1279/2739 train_loss = 2.168\n",
      "Epoch  89 Batch 1329/2739 train_loss = 4.093\n",
      "Epoch  89 Batch 1379/2739 train_loss = 2.053\n",
      "Epoch  89 Batch 1429/2739 train_loss = 3.427\n",
      "Epoch  89 Batch 1479/2739 train_loss = 1.910\n",
      "Epoch  89 Batch 1529/2739 train_loss = 4.207\n",
      "Epoch  89 Batch 1579/2739 train_loss = 1.690\n",
      "Epoch  89 Batch 1629/2739 train_loss = 2.998\n",
      "Epoch  89 Batch 1679/2739 train_loss = 2.104\n",
      "Epoch  89 Batch 1729/2739 train_loss = 1.631\n",
      "Epoch  89 Batch 1779/2739 train_loss = 1.792\n",
      "Epoch  89 Batch 1829/2739 train_loss = 2.620\n",
      "Epoch  89 Batch 1879/2739 train_loss = 2.045\n",
      "Epoch  89 Batch 1929/2739 train_loss = 1.218\n",
      "Epoch  89 Batch 1979/2739 train_loss = 2.385\n",
      "Epoch  89 Batch 2029/2739 train_loss = 3.566\n",
      "Epoch  89 Batch 2079/2739 train_loss = 1.711\n",
      "Epoch  89 Batch 2129/2739 train_loss = 1.741\n",
      "Epoch  89 Batch 2179/2739 train_loss = 3.707\n",
      "Epoch  89 Batch 2229/2739 train_loss = 2.424\n",
      "Epoch  89 Batch 2279/2739 train_loss = 1.437\n",
      "Epoch  89 Batch 2329/2739 train_loss = 2.114\n",
      "Epoch  89 Batch 2379/2739 train_loss = 2.107\n",
      "Epoch  89 Batch 2429/2739 train_loss = 3.991\n",
      "Epoch  89 Batch 2479/2739 train_loss = 2.602\n",
      "Epoch  89 Batch 2529/2739 train_loss = 2.780\n",
      "Epoch  89 Batch 2579/2739 train_loss = 1.188\n",
      "Epoch  89 Batch 2629/2739 train_loss = 1.920\n",
      "Epoch  89 Batch 2679/2739 train_loss = 2.086\n",
      "Epoch  89 Batch 2729/2739 train_loss = 3.676\n",
      "Epoch  90 Batch   40/2739 train_loss = 3.330\n",
      "Epoch  90 Batch   90/2739 train_loss = 1.694\n",
      "Epoch  90 Batch  140/2739 train_loss = 1.969\n",
      "Epoch  90 Batch  190/2739 train_loss = 2.332\n",
      "Epoch  90 Batch  240/2739 train_loss = 2.922\n",
      "Epoch  90 Batch  290/2739 train_loss = 1.511\n",
      "Epoch  90 Batch  340/2739 train_loss = 3.996\n",
      "Epoch  90 Batch  390/2739 train_loss = 2.185\n",
      "Epoch  90 Batch  440/2739 train_loss = 1.319\n",
      "Epoch  90 Batch  490/2739 train_loss = 3.266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  90 Batch  540/2739 train_loss = 2.215\n",
      "Epoch  90 Batch  590/2739 train_loss = 3.708\n",
      "Epoch  90 Batch  640/2739 train_loss = 1.883\n",
      "Epoch  90 Batch  690/2739 train_loss = 1.639\n",
      "Epoch  90 Batch  740/2739 train_loss = 2.239\n",
      "Epoch  90 Batch  790/2739 train_loss = 3.282\n",
      "Epoch  90 Batch  840/2739 train_loss = 3.422\n",
      "Epoch  90 Batch  890/2739 train_loss = 3.355\n",
      "Epoch  90 Batch  940/2739 train_loss = 2.442\n",
      "Epoch  90 Batch  990/2739 train_loss = 3.466\n",
      "Epoch  90 Batch 1040/2739 train_loss = 2.263\n",
      "Epoch  90 Batch 1090/2739 train_loss = 1.399\n",
      "Epoch  90 Batch 1140/2739 train_loss = 1.289\n",
      "Epoch  90 Batch 1190/2739 train_loss = 2.571\n",
      "Epoch  90 Batch 1240/2739 train_loss = 1.952\n",
      "Epoch  90 Batch 1290/2739 train_loss = 1.180\n",
      "Epoch  90 Batch 1340/2739 train_loss = 4.037\n",
      "Epoch  90 Batch 1390/2739 train_loss = 2.014\n",
      "Epoch  90 Batch 1440/2739 train_loss = 1.595\n",
      "Epoch  90 Batch 1490/2739 train_loss = 1.802\n",
      "Epoch  90 Batch 1540/2739 train_loss = 2.078\n",
      "Epoch  90 Batch 1590/2739 train_loss = 1.778\n",
      "Epoch  90 Batch 1640/2739 train_loss = 2.922\n",
      "Epoch  90 Batch 1690/2739 train_loss = 2.096\n",
      "Epoch  90 Batch 1740/2739 train_loss = 2.877\n",
      "Epoch  90 Batch 1790/2739 train_loss = 2.517\n",
      "Epoch  90 Batch 1840/2739 train_loss = 3.354\n",
      "Epoch  90 Batch 1890/2739 train_loss = 2.632\n",
      "Epoch  90 Batch 1940/2739 train_loss = 4.462\n",
      "Epoch  90 Batch 1990/2739 train_loss = 2.709\n",
      "Epoch  90 Batch 2040/2739 train_loss = 3.732\n",
      "Epoch  90 Batch 2090/2739 train_loss = 3.978\n",
      "Epoch  90 Batch 2140/2739 train_loss = 1.941\n",
      "Epoch  90 Batch 2190/2739 train_loss = 1.795\n",
      "Epoch  90 Batch 2240/2739 train_loss = 3.819\n",
      "Epoch  90 Batch 2290/2739 train_loss = 1.850\n",
      "Epoch  90 Batch 2340/2739 train_loss = 4.014\n",
      "Epoch  90 Batch 2390/2739 train_loss = 2.553\n",
      "Epoch  90 Batch 2440/2739 train_loss = 4.000\n",
      "Epoch  90 Batch 2490/2739 train_loss = 2.892\n",
      "Epoch  90 Batch 2540/2739 train_loss = 1.380\n",
      "Epoch  90 Batch 2590/2739 train_loss = 1.642\n",
      "Epoch  90 Batch 2640/2739 train_loss = 1.790\n",
      "Epoch  90 Batch 2690/2739 train_loss = 2.287\n",
      "Epoch  91 Batch    1/2739 train_loss = 2.148\n",
      "Epoch  91 Batch   51/2739 train_loss = 1.270\n",
      "Epoch  91 Batch  101/2739 train_loss = 2.186\n",
      "Epoch  91 Batch  151/2739 train_loss = 3.112\n",
      "Epoch  91 Batch  201/2739 train_loss = 1.566\n",
      "Epoch  91 Batch  251/2739 train_loss = 2.478\n",
      "Epoch  91 Batch  301/2739 train_loss = 3.781\n",
      "Epoch  91 Batch  351/2739 train_loss = 2.630\n",
      "Epoch  91 Batch  401/2739 train_loss = 2.836\n",
      "Epoch  91 Batch  451/2739 train_loss = 3.065\n",
      "Epoch  91 Batch  501/2739 train_loss = 1.079\n",
      "Epoch  91 Batch  551/2739 train_loss = 1.919\n",
      "Epoch  91 Batch  601/2739 train_loss = 2.254\n",
      "Epoch  91 Batch  651/2739 train_loss = 2.396\n",
      "Epoch  91 Batch  701/2739 train_loss = 3.032\n",
      "Epoch  91 Batch  751/2739 train_loss = 2.363\n",
      "Epoch  91 Batch  801/2739 train_loss = 2.738\n",
      "Epoch  91 Batch  851/2739 train_loss = 2.459\n",
      "Epoch  91 Batch  901/2739 train_loss = 4.207\n",
      "Epoch  91 Batch  951/2739 train_loss = 1.623\n",
      "Epoch  91 Batch 1001/2739 train_loss = 3.803\n",
      "Epoch  91 Batch 1051/2739 train_loss = 2.453\n",
      "Epoch  91 Batch 1101/2739 train_loss = 1.494\n",
      "Epoch  91 Batch 1151/2739 train_loss = 2.103\n",
      "Epoch  91 Batch 1201/2739 train_loss = 3.280\n",
      "Epoch  91 Batch 1251/2739 train_loss = 1.774\n",
      "Epoch  91 Batch 1301/2739 train_loss = 1.976\n",
      "Epoch  91 Batch 1351/2739 train_loss = 2.484\n",
      "Epoch  91 Batch 1401/2739 train_loss = 3.580\n",
      "Epoch  91 Batch 1451/2739 train_loss = 2.647\n",
      "Epoch  91 Batch 1501/2739 train_loss = 1.535\n",
      "Epoch  91 Batch 1551/2739 train_loss = 2.019\n",
      "Epoch  91 Batch 1601/2739 train_loss = 3.708\n",
      "Epoch  91 Batch 1651/2739 train_loss = 2.407\n",
      "Epoch  91 Batch 1701/2739 train_loss = 1.638\n",
      "Epoch  91 Batch 1751/2739 train_loss = 1.495\n",
      "Epoch  91 Batch 1801/2739 train_loss = 2.510\n",
      "Epoch  91 Batch 1851/2739 train_loss = 3.973\n",
      "Epoch  91 Batch 1901/2739 train_loss = 1.342\n",
      "Epoch  91 Batch 1951/2739 train_loss = 2.202\n",
      "Epoch  91 Batch 2001/2739 train_loss = 2.230\n",
      "Epoch  91 Batch 2051/2739 train_loss = 4.003\n",
      "Epoch  91 Batch 2101/2739 train_loss = 1.757\n",
      "Epoch  91 Batch 2151/2739 train_loss = 2.374\n",
      "Epoch  91 Batch 2201/2739 train_loss = 3.916\n",
      "Epoch  91 Batch 2251/2739 train_loss = 4.164\n",
      "Epoch  91 Batch 2301/2739 train_loss = 3.274\n",
      "Epoch  91 Batch 2351/2739 train_loss = 2.138\n",
      "Epoch  91 Batch 2401/2739 train_loss = 2.517\n",
      "Epoch  91 Batch 2451/2739 train_loss = 2.040\n",
      "Epoch  91 Batch 2501/2739 train_loss = 1.657\n",
      "Epoch  91 Batch 2551/2739 train_loss = 1.429\n",
      "Epoch  91 Batch 2601/2739 train_loss = 3.001\n",
      "Epoch  91 Batch 2651/2739 train_loss = 2.050\n",
      "Epoch  91 Batch 2701/2739 train_loss = 1.695\n",
      "Epoch  92 Batch   12/2739 train_loss = 2.809\n",
      "Epoch  92 Batch   62/2739 train_loss = 2.330\n",
      "Epoch  92 Batch  112/2739 train_loss = 2.959\n",
      "Epoch  92 Batch  162/2739 train_loss = 2.421\n",
      "Epoch  92 Batch  212/2739 train_loss = 2.405\n",
      "Epoch  92 Batch  262/2739 train_loss = 1.941\n",
      "Epoch  92 Batch  312/2739 train_loss = 1.931\n",
      "Epoch  92 Batch  362/2739 train_loss = 2.468\n",
      "Epoch  92 Batch  412/2739 train_loss = 2.905\n",
      "Epoch  92 Batch  462/2739 train_loss = 1.878\n",
      "Epoch  92 Batch  512/2739 train_loss = 2.576\n",
      "Epoch  92 Batch  562/2739 train_loss = 1.717\n",
      "Epoch  92 Batch  612/2739 train_loss = 2.090\n",
      "Epoch  92 Batch  662/2739 train_loss = 3.298\n",
      "Epoch  92 Batch  712/2739 train_loss = 3.770\n",
      "Epoch  92 Batch  762/2739 train_loss = 1.213\n",
      "Epoch  92 Batch  812/2739 train_loss = 1.824\n",
      "Epoch  92 Batch  862/2739 train_loss = 3.705\n",
      "Epoch  92 Batch  912/2739 train_loss = 1.592\n",
      "Epoch  92 Batch  962/2739 train_loss = 3.180\n",
      "Epoch  92 Batch 1012/2739 train_loss = 1.780\n",
      "Epoch  92 Batch 1062/2739 train_loss = 1.424\n",
      "Epoch  92 Batch 1112/2739 train_loss = 1.816\n",
      "Epoch  92 Batch 1162/2739 train_loss = 2.803\n",
      "Epoch  92 Batch 1212/2739 train_loss = 2.795\n",
      "Epoch  92 Batch 1262/2739 train_loss = 2.544\n",
      "Epoch  92 Batch 1312/2739 train_loss = 2.115\n",
      "Epoch  92 Batch 1362/2739 train_loss = 2.286\n",
      "Epoch  92 Batch 1412/2739 train_loss = 2.374\n",
      "Epoch  92 Batch 1462/2739 train_loss = 2.088\n",
      "Epoch  92 Batch 1512/2739 train_loss = 1.761\n",
      "Epoch  92 Batch 1562/2739 train_loss = 2.966\n",
      "Epoch  92 Batch 1612/2739 train_loss = 2.357\n",
      "Epoch  92 Batch 1662/2739 train_loss = 1.721\n",
      "Epoch  92 Batch 1712/2739 train_loss = 3.548\n",
      "Epoch  92 Batch 1762/2739 train_loss = 2.056\n",
      "Epoch  92 Batch 1812/2739 train_loss = 1.631\n",
      "Epoch  92 Batch 1862/2739 train_loss = 1.681\n",
      "Epoch  92 Batch 1912/2739 train_loss = 2.535\n",
      "Epoch  92 Batch 1962/2739 train_loss = 2.305\n",
      "Epoch  92 Batch 2012/2739 train_loss = 2.949\n",
      "Epoch  92 Batch 2062/2739 train_loss = 2.360\n",
      "Epoch  92 Batch 2112/2739 train_loss = 1.281\n",
      "Epoch  92 Batch 2162/2739 train_loss = 2.646\n",
      "Epoch  92 Batch 2212/2739 train_loss = 2.984\n",
      "Epoch  92 Batch 2262/2739 train_loss = 4.188\n",
      "Epoch  92 Batch 2312/2739 train_loss = 2.861\n",
      "Epoch  92 Batch 2362/2739 train_loss = 1.586\n",
      "Epoch  92 Batch 2412/2739 train_loss = 2.427\n",
      "Epoch  92 Batch 2462/2739 train_loss = 3.323\n",
      "Epoch  92 Batch 2512/2739 train_loss = 2.160\n",
      "Epoch  92 Batch 2562/2739 train_loss = 1.868\n",
      "Epoch  92 Batch 2612/2739 train_loss = 1.904\n",
      "Epoch  92 Batch 2662/2739 train_loss = 2.246\n",
      "Epoch  92 Batch 2712/2739 train_loss = 2.222\n",
      "Epoch  93 Batch   23/2739 train_loss = 1.607\n",
      "Epoch  93 Batch   73/2739 train_loss = 2.086\n",
      "Epoch  93 Batch  123/2739 train_loss = 2.099\n",
      "Epoch  93 Batch  173/2739 train_loss = 1.741\n",
      "Epoch  93 Batch  223/2739 train_loss = 1.850\n",
      "Epoch  93 Batch  273/2739 train_loss = 2.993\n",
      "Epoch  93 Batch  323/2739 train_loss = 3.155\n",
      "Epoch  93 Batch  373/2739 train_loss = 1.696\n",
      "Epoch  93 Batch  423/2739 train_loss = 1.541\n",
      "Epoch  93 Batch  473/2739 train_loss = 2.274\n",
      "Epoch  93 Batch  523/2739 train_loss = 1.333\n",
      "Epoch  93 Batch  573/2739 train_loss = 2.432\n",
      "Epoch  93 Batch  623/2739 train_loss = 2.088\n",
      "Epoch  93 Batch  673/2739 train_loss = 3.016\n",
      "Epoch  93 Batch  723/2739 train_loss = 3.259\n",
      "Epoch  93 Batch  773/2739 train_loss = 3.732\n",
      "Epoch  93 Batch  823/2739 train_loss = 2.207\n",
      "Epoch  93 Batch  873/2739 train_loss = 1.584\n",
      "Epoch  93 Batch  923/2739 train_loss = 3.013\n",
      "Epoch  93 Batch  973/2739 train_loss = 2.831\n",
      "Epoch  93 Batch 1023/2739 train_loss = 2.553\n",
      "Epoch  93 Batch 1073/2739 train_loss = 3.599\n",
      "Epoch  93 Batch 1123/2739 train_loss = 3.454\n",
      "Epoch  93 Batch 1173/2739 train_loss = 4.341\n",
      "Epoch  93 Batch 1223/2739 train_loss = 1.436\n",
      "Epoch  93 Batch 1273/2739 train_loss = 2.897\n",
      "Epoch  93 Batch 1323/2739 train_loss = 3.890\n",
      "Epoch  93 Batch 1373/2739 train_loss = 2.180\n",
      "Epoch  93 Batch 1423/2739 train_loss = 1.829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  93 Batch 1473/2739 train_loss = 2.287\n",
      "Epoch  93 Batch 1523/2739 train_loss = 1.057\n",
      "Epoch  93 Batch 1573/2739 train_loss = 3.910\n",
      "Epoch  93 Batch 1623/2739 train_loss = 3.755\n",
      "Epoch  93 Batch 1673/2739 train_loss = 1.611\n",
      "Epoch  93 Batch 1723/2739 train_loss = 3.940\n",
      "Epoch  93 Batch 1773/2739 train_loss = 2.495\n",
      "Epoch  93 Batch 1823/2739 train_loss = 2.052\n",
      "Epoch  93 Batch 1873/2739 train_loss = 3.572\n",
      "Epoch  93 Batch 1923/2739 train_loss = 1.535\n",
      "Epoch  93 Batch 1973/2739 train_loss = 1.762\n",
      "Epoch  93 Batch 2023/2739 train_loss = 2.379\n",
      "Epoch  93 Batch 2073/2739 train_loss = 3.350\n",
      "Epoch  93 Batch 2123/2739 train_loss = 1.939\n",
      "Epoch  93 Batch 2173/2739 train_loss = 2.999\n",
      "Epoch  93 Batch 2223/2739 train_loss = 1.593\n",
      "Epoch  93 Batch 2273/2739 train_loss = 2.254\n",
      "Epoch  93 Batch 2323/2739 train_loss = 4.017\n",
      "Epoch  93 Batch 2373/2739 train_loss = 2.658\n",
      "Epoch  93 Batch 2423/2739 train_loss = 1.513\n",
      "Epoch  93 Batch 2473/2739 train_loss = 2.460\n",
      "Epoch  93 Batch 2523/2739 train_loss = 2.546\n",
      "Epoch  93 Batch 2573/2739 train_loss = 1.719\n",
      "Epoch  93 Batch 2623/2739 train_loss = 2.098\n",
      "Epoch  93 Batch 2673/2739 train_loss = 2.287\n",
      "Epoch  93 Batch 2723/2739 train_loss = 1.468\n",
      "Epoch  94 Batch   34/2739 train_loss = 1.268\n",
      "Epoch  94 Batch   84/2739 train_loss = 2.341\n",
      "Epoch  94 Batch  134/2739 train_loss = 1.952\n",
      "Epoch  94 Batch  184/2739 train_loss = 2.205\n",
      "Epoch  94 Batch  234/2739 train_loss = 1.380\n",
      "Epoch  94 Batch  284/2739 train_loss = 1.957\n",
      "Epoch  94 Batch  334/2739 train_loss = 2.632\n",
      "Epoch  94 Batch  384/2739 train_loss = 2.018\n",
      "Epoch  94 Batch  434/2739 train_loss = 1.926\n",
      "Epoch  94 Batch  484/2739 train_loss = 2.167\n",
      "Epoch  94 Batch  534/2739 train_loss = 2.167\n",
      "Epoch  94 Batch  584/2739 train_loss = 1.810\n",
      "Epoch  94 Batch  634/2739 train_loss = 2.678\n",
      "Epoch  94 Batch  684/2739 train_loss = 2.997\n",
      "Epoch  94 Batch  734/2739 train_loss = 2.495\n",
      "Epoch  94 Batch  784/2739 train_loss = 3.707\n",
      "Epoch  94 Batch  834/2739 train_loss = 2.045\n",
      "Epoch  94 Batch  884/2739 train_loss = 2.408\n",
      "Epoch  94 Batch  934/2739 train_loss = 2.032\n",
      "Epoch  94 Batch  984/2739 train_loss = 1.782\n",
      "Epoch  94 Batch 1034/2739 train_loss = 1.759\n",
      "Epoch  94 Batch 1084/2739 train_loss = 1.244\n",
      "Epoch  94 Batch 1134/2739 train_loss = 2.759\n",
      "Epoch  94 Batch 1184/2739 train_loss = 1.692\n",
      "Epoch  94 Batch 1234/2739 train_loss = 3.026\n",
      "Epoch  94 Batch 1284/2739 train_loss = 1.614\n",
      "Epoch  94 Batch 1334/2739 train_loss = 3.735\n",
      "Epoch  94 Batch 1384/2739 train_loss = 2.550\n",
      "Epoch  94 Batch 1434/2739 train_loss = 1.563\n",
      "Epoch  94 Batch 1484/2739 train_loss = 1.465\n",
      "Epoch  94 Batch 1534/2739 train_loss = 1.983\n",
      "Epoch  94 Batch 1584/2739 train_loss = 1.492\n",
      "Epoch  94 Batch 1634/2739 train_loss = 1.715\n",
      "Epoch  94 Batch 1684/2739 train_loss = 3.848\n",
      "Epoch  94 Batch 1734/2739 train_loss = 2.163\n",
      "Epoch  94 Batch 1784/2739 train_loss = 2.168\n",
      "Epoch  94 Batch 1834/2739 train_loss = 3.787\n",
      "Epoch  94 Batch 1884/2739 train_loss = 3.604\n",
      "Epoch  94 Batch 1934/2739 train_loss = 2.360\n",
      "Epoch  94 Batch 1984/2739 train_loss = 2.950\n",
      "Epoch  94 Batch 2034/2739 train_loss = 4.036\n",
      "Epoch  94 Batch 2084/2739 train_loss = 2.766\n",
      "Epoch  94 Batch 2134/2739 train_loss = 3.488\n",
      "Epoch  94 Batch 2184/2739 train_loss = 2.075\n",
      "Epoch  94 Batch 2234/2739 train_loss = 3.746\n",
      "Epoch  94 Batch 2284/2739 train_loss = 1.705\n",
      "Epoch  94 Batch 2334/2739 train_loss = 2.246\n",
      "Epoch  94 Batch 2384/2739 train_loss = 1.306\n",
      "Epoch  94 Batch 2434/2739 train_loss = 4.183\n",
      "Epoch  94 Batch 2484/2739 train_loss = 1.224\n",
      "Epoch  94 Batch 2534/2739 train_loss = 3.998\n",
      "Epoch  94 Batch 2584/2739 train_loss = 6.864\n",
      "Epoch  94 Batch 2634/2739 train_loss = 4.115\n",
      "Epoch  94 Batch 2684/2739 train_loss = 3.645\n",
      "Epoch  94 Batch 2734/2739 train_loss = 3.113\n",
      "Epoch  95 Batch   45/2739 train_loss = 2.724\n",
      "Epoch  95 Batch   95/2739 train_loss = 2.000\n",
      "Epoch  95 Batch  145/2739 train_loss = 1.995\n",
      "Epoch  95 Batch  195/2739 train_loss = 2.830\n",
      "Epoch  95 Batch  245/2739 train_loss = 2.900\n",
      "Epoch  95 Batch  295/2739 train_loss = 2.865\n",
      "Epoch  95 Batch  345/2739 train_loss = 2.180\n",
      "Epoch  95 Batch  395/2739 train_loss = 2.605\n",
      "Epoch  95 Batch  445/2739 train_loss = 1.788\n",
      "Epoch  95 Batch  495/2739 train_loss = 2.981\n",
      "Epoch  95 Batch  545/2739 train_loss = 2.622\n",
      "Epoch  95 Batch  595/2739 train_loss = 2.897\n",
      "Epoch  95 Batch  645/2739 train_loss = 2.726\n",
      "Epoch  95 Batch  695/2739 train_loss = 2.055\n",
      "Epoch  95 Batch  745/2739 train_loss = 3.084\n",
      "Epoch  95 Batch  795/2739 train_loss = 2.456\n",
      "Epoch  95 Batch  845/2739 train_loss = 2.643\n",
      "Epoch  95 Batch  895/2739 train_loss = 2.081\n",
      "Epoch  95 Batch  945/2739 train_loss = 4.383\n",
      "Epoch  95 Batch  995/2739 train_loss = 3.054\n",
      "Epoch  95 Batch 1045/2739 train_loss = 2.691\n",
      "Epoch  95 Batch 1095/2739 train_loss = 1.262\n",
      "Epoch  95 Batch 1145/2739 train_loss = 3.995\n",
      "Epoch  95 Batch 1195/2739 train_loss = 2.479\n",
      "Epoch  95 Batch 1245/2739 train_loss = 1.926\n",
      "Epoch  95 Batch 1295/2739 train_loss = 1.328\n",
      "Epoch  95 Batch 1345/2739 train_loss = 2.387\n",
      "Epoch  95 Batch 1395/2739 train_loss = 2.418\n",
      "Epoch  95 Batch 1445/2739 train_loss = 1.292\n",
      "Epoch  95 Batch 1495/2739 train_loss = 2.609\n",
      "Epoch  95 Batch 1545/2739 train_loss = 3.342\n",
      "Epoch  95 Batch 1595/2739 train_loss = 2.172\n",
      "Epoch  95 Batch 1645/2739 train_loss = 4.343\n",
      "Epoch  95 Batch 1695/2739 train_loss = 2.653\n",
      "Epoch  95 Batch 1745/2739 train_loss = 4.374\n",
      "Epoch  95 Batch 1795/2739 train_loss = 2.683\n",
      "Epoch  95 Batch 1845/2739 train_loss = 1.573\n",
      "Epoch  95 Batch 1895/2739 train_loss = 2.134\n",
      "Epoch  95 Batch 1945/2739 train_loss = 1.461\n",
      "Epoch  95 Batch 1995/2739 train_loss = 1.001\n",
      "Epoch  95 Batch 2045/2739 train_loss = 4.316\n",
      "Epoch  95 Batch 2095/2739 train_loss = 2.432\n",
      "Epoch  95 Batch 2145/2739 train_loss = 2.896\n",
      "Epoch  95 Batch 2195/2739 train_loss = 4.624\n",
      "Epoch  95 Batch 2245/2739 train_loss = 4.187\n",
      "Epoch  95 Batch 2295/2739 train_loss = 3.606\n",
      "Epoch  95 Batch 2345/2739 train_loss = 2.697\n",
      "Epoch  95 Batch 2395/2739 train_loss = 2.697\n",
      "Epoch  95 Batch 2445/2739 train_loss = 2.788\n",
      "Epoch  95 Batch 2495/2739 train_loss = 1.959\n",
      "Epoch  95 Batch 2545/2739 train_loss = 1.816\n",
      "Epoch  95 Batch 2595/2739 train_loss = 4.041\n",
      "Epoch  95 Batch 2645/2739 train_loss = 1.660\n",
      "Epoch  95 Batch 2695/2739 train_loss = 2.450\n",
      "Epoch  96 Batch    6/2739 train_loss = 2.509\n",
      "Epoch  96 Batch   56/2739 train_loss = 2.485\n",
      "Epoch  96 Batch  106/2739 train_loss = 2.067\n",
      "Epoch  96 Batch  156/2739 train_loss = 2.631\n",
      "Epoch  96 Batch  206/2739 train_loss = 2.843\n",
      "Epoch  96 Batch  256/2739 train_loss = 2.350\n",
      "Epoch  96 Batch  306/2739 train_loss = 1.798\n",
      "Epoch  96 Batch  356/2739 train_loss = 0.986\n",
      "Epoch  96 Batch  406/2739 train_loss = 2.498\n",
      "Epoch  96 Batch  456/2739 train_loss = 3.252\n",
      "Epoch  96 Batch  506/2739 train_loss = 1.452\n",
      "Epoch  96 Batch  556/2739 train_loss = 2.107\n",
      "Epoch  96 Batch  606/2739 train_loss = 4.461\n",
      "Epoch  96 Batch  656/2739 train_loss = 3.563\n",
      "Epoch  96 Batch  706/2739 train_loss = 2.755\n",
      "Epoch  96 Batch  756/2739 train_loss = 3.826\n",
      "Epoch  96 Batch  806/2739 train_loss = 1.956\n",
      "Epoch  96 Batch  856/2739 train_loss = 2.212\n",
      "Epoch  96 Batch  906/2739 train_loss = 3.099\n",
      "Epoch  96 Batch  956/2739 train_loss = 4.234\n",
      "Epoch  96 Batch 1006/2739 train_loss = 2.637\n",
      "Epoch  96 Batch 1056/2739 train_loss = 2.138\n",
      "Epoch  96 Batch 1106/2739 train_loss = 1.644\n",
      "Epoch  96 Batch 1156/2739 train_loss = 2.671\n",
      "Epoch  96 Batch 1206/2739 train_loss = 2.840\n",
      "Epoch  96 Batch 1256/2739 train_loss = 2.224\n",
      "Epoch  96 Batch 1306/2739 train_loss = 2.232\n",
      "Epoch  96 Batch 1356/2739 train_loss = 1.628\n",
      "Epoch  96 Batch 1406/2739 train_loss = 3.739\n",
      "Epoch  96 Batch 1456/2739 train_loss = 2.311\n",
      "Epoch  96 Batch 1506/2739 train_loss = 2.241\n",
      "Epoch  96 Batch 1556/2739 train_loss = 2.070\n",
      "Epoch  96 Batch 1606/2739 train_loss = 1.708\n",
      "Epoch  96 Batch 1656/2739 train_loss = 3.630\n",
      "Epoch  96 Batch 1706/2739 train_loss = 2.089\n",
      "Epoch  96 Batch 1756/2739 train_loss = 2.130\n",
      "Epoch  96 Batch 1806/2739 train_loss = 4.173\n",
      "Epoch  96 Batch 1856/2739 train_loss = 3.962\n",
      "Epoch  96 Batch 1906/2739 train_loss = 2.875\n",
      "Epoch  96 Batch 1956/2739 train_loss = 2.577\n",
      "Epoch  96 Batch 2006/2739 train_loss = 3.163\n",
      "Epoch  96 Batch 2056/2739 train_loss = 2.785\n",
      "Epoch  96 Batch 2106/2739 train_loss = 1.895\n",
      "Epoch  96 Batch 2156/2739 train_loss = 2.580\n",
      "Epoch  96 Batch 2206/2739 train_loss = 3.824\n",
      "Epoch  96 Batch 2256/2739 train_loss = 1.850\n",
      "Epoch  96 Batch 2306/2739 train_loss = 4.135\n",
      "Epoch  96 Batch 2356/2739 train_loss = 2.404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  96 Batch 2406/2739 train_loss = 2.149\n",
      "Epoch  96 Batch 2456/2739 train_loss = 3.522\n",
      "Epoch  96 Batch 2506/2739 train_loss = 4.073\n",
      "Epoch  96 Batch 2556/2739 train_loss = 2.207\n",
      "Epoch  96 Batch 2606/2739 train_loss = 2.506\n",
      "Epoch  96 Batch 2656/2739 train_loss = 3.076\n",
      "Epoch  96 Batch 2706/2739 train_loss = 2.198\n",
      "Epoch  97 Batch   17/2739 train_loss = 2.231\n",
      "Epoch  97 Batch   67/2739 train_loss = 4.348\n",
      "Epoch  97 Batch  117/2739 train_loss = 2.897\n",
      "Epoch  97 Batch  167/2739 train_loss = 2.307\n",
      "Epoch  97 Batch  217/2739 train_loss = 2.460\n",
      "Epoch  97 Batch  267/2739 train_loss = 2.018\n",
      "Epoch  97 Batch  317/2739 train_loss = 2.527\n",
      "Epoch  97 Batch  367/2739 train_loss = 2.306\n",
      "Epoch  97 Batch  417/2739 train_loss = 2.109\n",
      "Epoch  97 Batch  467/2739 train_loss = 2.430\n",
      "Epoch  97 Batch  517/2739 train_loss = 2.293\n",
      "Epoch  97 Batch  567/2739 train_loss = 2.280\n",
      "Epoch  97 Batch  617/2739 train_loss = 4.006\n",
      "Epoch  97 Batch  667/2739 train_loss = 2.267\n",
      "Epoch  97 Batch  717/2739 train_loss = 2.171\n",
      "Epoch  97 Batch  767/2739 train_loss = 1.394\n",
      "Epoch  97 Batch  817/2739 train_loss = 2.823\n",
      "Epoch  97 Batch  867/2739 train_loss = 3.715\n",
      "Epoch  97 Batch  917/2739 train_loss = 1.809\n",
      "Epoch  97 Batch  967/2739 train_loss = 3.649\n",
      "Epoch  97 Batch 1017/2739 train_loss = 2.415\n",
      "Epoch  97 Batch 1067/2739 train_loss = 1.246\n",
      "Epoch  97 Batch 1117/2739 train_loss = 3.232\n",
      "Epoch  97 Batch 1167/2739 train_loss = 2.893\n",
      "Epoch  97 Batch 1217/2739 train_loss = 1.803\n",
      "Epoch  97 Batch 1267/2739 train_loss = 2.441\n",
      "Epoch  97 Batch 1317/2739 train_loss = 4.012\n",
      "Epoch  97 Batch 1367/2739 train_loss = 2.490\n",
      "Epoch  97 Batch 1417/2739 train_loss = 2.537\n",
      "Epoch  97 Batch 1467/2739 train_loss = 1.399\n",
      "Epoch  97 Batch 1517/2739 train_loss = 1.110\n",
      "Epoch  97 Batch 1567/2739 train_loss = 2.047\n",
      "Epoch  97 Batch 1617/2739 train_loss = 2.818\n",
      "Epoch  97 Batch 1667/2739 train_loss = 2.057\n",
      "Epoch  97 Batch 1717/2739 train_loss = 2.463\n",
      "Epoch  97 Batch 1767/2739 train_loss = 2.204\n",
      "Epoch  97 Batch 1817/2739 train_loss = 0.939\n",
      "Epoch  97 Batch 1867/2739 train_loss = 1.776\n",
      "Epoch  97 Batch 1917/2739 train_loss = 2.129\n",
      "Epoch  97 Batch 1967/2739 train_loss = 2.916\n",
      "Epoch  97 Batch 2017/2739 train_loss = 4.008\n",
      "Epoch  97 Batch 2067/2739 train_loss = 3.992\n",
      "Epoch  97 Batch 2117/2739 train_loss = 3.900\n",
      "Epoch  97 Batch 2167/2739 train_loss = 1.750\n",
      "Epoch  97 Batch 2217/2739 train_loss = 2.894\n",
      "Epoch  97 Batch 2267/2739 train_loss = 2.699\n",
      "Epoch  97 Batch 2317/2739 train_loss = 4.484\n",
      "Epoch  97 Batch 2367/2739 train_loss = 2.867\n",
      "Epoch  97 Batch 2417/2739 train_loss = 2.358\n",
      "Epoch  97 Batch 2467/2739 train_loss = 2.245\n",
      "Epoch  97 Batch 2517/2739 train_loss = 3.052\n",
      "Epoch  97 Batch 2567/2739 train_loss = 1.743\n",
      "Epoch  97 Batch 2617/2739 train_loss = 2.688\n",
      "Epoch  97 Batch 2667/2739 train_loss = 2.651\n",
      "Epoch  97 Batch 2717/2739 train_loss = 2.050\n",
      "Epoch  98 Batch   28/2739 train_loss = 2.334\n",
      "Epoch  98 Batch   78/2739 train_loss = 2.568\n",
      "Epoch  98 Batch  128/2739 train_loss = 2.181\n",
      "Epoch  98 Batch  178/2739 train_loss = 2.047\n",
      "Epoch  98 Batch  228/2739 train_loss = 1.616\n",
      "Epoch  98 Batch  278/2739 train_loss = 2.204\n",
      "Epoch  98 Batch  328/2739 train_loss = 3.964\n",
      "Epoch  98 Batch  378/2739 train_loss = 1.905\n",
      "Epoch  98 Batch  428/2739 train_loss = 1.605\n",
      "Epoch  98 Batch  478/2739 train_loss = 1.150\n",
      "Epoch  98 Batch  528/2739 train_loss = 1.869\n",
      "Epoch  98 Batch  578/2739 train_loss = 1.693\n",
      "Epoch  98 Batch  628/2739 train_loss = 2.770\n",
      "Epoch  98 Batch  678/2739 train_loss = 2.757\n",
      "Epoch  98 Batch  728/2739 train_loss = 1.572\n",
      "Epoch  98 Batch  778/2739 train_loss = 3.341\n",
      "Epoch  98 Batch  828/2739 train_loss = 2.768\n",
      "Epoch  98 Batch  878/2739 train_loss = 2.618\n",
      "Epoch  98 Batch  928/2739 train_loss = 1.779\n",
      "Epoch  98 Batch  978/2739 train_loss = 2.314\n",
      "Epoch  98 Batch 1028/2739 train_loss = 2.214\n",
      "Epoch  98 Batch 1078/2739 train_loss = 2.696\n",
      "Epoch  98 Batch 1128/2739 train_loss = 2.243\n",
      "Epoch  98 Batch 1178/2739 train_loss = 2.700\n",
      "Epoch  98 Batch 1228/2739 train_loss = 2.789\n",
      "Epoch  98 Batch 1278/2739 train_loss = 2.700\n",
      "Epoch  98 Batch 1328/2739 train_loss = 2.655\n",
      "Epoch  98 Batch 1378/2739 train_loss = 2.085\n",
      "Epoch  98 Batch 1428/2739 train_loss = 1.560\n",
      "Epoch  98 Batch 1478/2739 train_loss = 2.474\n",
      "Epoch  98 Batch 1528/2739 train_loss = 1.984\n",
      "Epoch  98 Batch 1578/2739 train_loss = 2.650\n",
      "Epoch  98 Batch 1628/2739 train_loss = 3.589\n",
      "Epoch  98 Batch 1678/2739 train_loss = 3.556\n",
      "Epoch  98 Batch 1728/2739 train_loss = 3.976\n",
      "Epoch  98 Batch 1778/2739 train_loss = 2.338\n",
      "Epoch  98 Batch 1828/2739 train_loss = 2.164\n",
      "Epoch  98 Batch 1878/2739 train_loss = 2.160\n",
      "Epoch  98 Batch 1928/2739 train_loss = 2.131\n",
      "Epoch  98 Batch 1978/2739 train_loss = 2.410\n",
      "Epoch  98 Batch 2028/2739 train_loss = 3.543\n",
      "Epoch  98 Batch 2078/2739 train_loss = 4.100\n",
      "Epoch  98 Batch 2128/2739 train_loss = 3.381\n",
      "Epoch  98 Batch 2178/2739 train_loss = 2.233\n",
      "Epoch  98 Batch 2228/2739 train_loss = 2.688\n",
      "Epoch  98 Batch 2278/2739 train_loss = 1.753\n",
      "Epoch  98 Batch 2328/2739 train_loss = 2.599\n",
      "Epoch  98 Batch 2378/2739 train_loss = 2.501\n",
      "Epoch  98 Batch 2428/2739 train_loss = 3.831\n",
      "Epoch  98 Batch 2478/2739 train_loss = 1.276\n",
      "Epoch  98 Batch 2528/2739 train_loss = 3.632\n",
      "Epoch  98 Batch 2578/2739 train_loss = 2.300\n",
      "Epoch  98 Batch 2628/2739 train_loss = 2.257\n",
      "Epoch  98 Batch 2678/2739 train_loss = 2.758\n",
      "Epoch  98 Batch 2728/2739 train_loss = 4.011\n",
      "Epoch  99 Batch   39/2739 train_loss = 3.932\n",
      "Epoch  99 Batch   89/2739 train_loss = 2.383\n",
      "Epoch  99 Batch  139/2739 train_loss = 2.097\n",
      "Epoch  99 Batch  189/2739 train_loss = 2.496\n",
      "Epoch  99 Batch  239/2739 train_loss = 2.153\n",
      "Epoch  99 Batch  289/2739 train_loss = 1.998\n",
      "Epoch  99 Batch  339/2739 train_loss = 4.033\n",
      "Epoch  99 Batch  389/2739 train_loss = 2.227\n",
      "Epoch  99 Batch  439/2739 train_loss = 2.588\n",
      "Epoch  99 Batch  489/2739 train_loss = 3.380\n",
      "Epoch  99 Batch  539/2739 train_loss = 1.624\n",
      "Epoch  99 Batch  589/2739 train_loss = 3.490\n",
      "Epoch  99 Batch  639/2739 train_loss = 2.059\n",
      "Epoch  99 Batch  689/2739 train_loss = 1.815\n",
      "Epoch  99 Batch  739/2739 train_loss = 3.392\n",
      "Epoch  99 Batch  789/2739 train_loss = 2.055\n",
      "Epoch  99 Batch  839/2739 train_loss = 2.383\n",
      "Epoch  99 Batch  889/2739 train_loss = 1.389\n",
      "Epoch  99 Batch  939/2739 train_loss = 1.349\n",
      "Epoch  99 Batch  989/2739 train_loss = 3.577\n",
      "Epoch  99 Batch 1039/2739 train_loss = 1.731\n",
      "Epoch  99 Batch 1089/2739 train_loss = 2.099\n",
      "Epoch  99 Batch 1139/2739 train_loss = 2.719\n",
      "Epoch  99 Batch 1189/2739 train_loss = 1.799\n",
      "Epoch  99 Batch 1239/2739 train_loss = 1.818\n",
      "Epoch  99 Batch 1289/2739 train_loss = 2.711\n",
      "Epoch  99 Batch 1339/2739 train_loss = 4.002\n",
      "Epoch  99 Batch 1389/2739 train_loss = 1.980\n",
      "Epoch  99 Batch 1439/2739 train_loss = 3.363\n",
      "Epoch  99 Batch 1489/2739 train_loss = 2.740\n",
      "Epoch  99 Batch 1539/2739 train_loss = 1.600\n",
      "Epoch  99 Batch 1589/2739 train_loss = 2.015\n",
      "Epoch  99 Batch 1639/2739 train_loss = 3.114\n",
      "Epoch  99 Batch 1689/2739 train_loss = 3.468\n",
      "Epoch  99 Batch 1739/2739 train_loss = 2.913\n",
      "Epoch  99 Batch 1789/2739 train_loss = 3.134\n",
      "Epoch  99 Batch 1839/2739 train_loss = 3.605\n",
      "Epoch  99 Batch 1889/2739 train_loss = 2.537\n",
      "Epoch  99 Batch 1939/2739 train_loss = 4.316\n",
      "Epoch  99 Batch 1989/2739 train_loss = 2.094\n",
      "Epoch  99 Batch 2039/2739 train_loss = 3.859\n",
      "Epoch  99 Batch 2089/2739 train_loss = 2.967\n",
      "Epoch  99 Batch 2139/2739 train_loss = 2.266\n",
      "Epoch  99 Batch 2189/2739 train_loss = 3.957\n",
      "Epoch  99 Batch 2239/2739 train_loss = 2.359\n",
      "Epoch  99 Batch 2289/2739 train_loss = 4.128\n",
      "Epoch  99 Batch 2339/2739 train_loss = 4.021\n",
      "Epoch  99 Batch 2389/2739 train_loss = 3.190\n",
      "Epoch  99 Batch 2439/2739 train_loss = 4.237\n",
      "Epoch  99 Batch 2489/2739 train_loss = 2.498\n",
      "Epoch  99 Batch 2539/2739 train_loss = 1.401\n",
      "Epoch  99 Batch 2589/2739 train_loss = 1.231\n",
      "Epoch  99 Batch 2639/2739 train_loss = 4.787\n",
      "Epoch  99 Batch 2689/2739 train_loss = 1.695\n",
      "Epoch 100 Batch    0/2739 train_loss = 1.852\n",
      "Epoch 100 Batch   50/2739 train_loss = 2.489\n",
      "Epoch 100 Batch  100/2739 train_loss = 2.539\n",
      "Epoch 100 Batch  150/2739 train_loss = 3.310\n",
      "Epoch 100 Batch  200/2739 train_loss = 2.412\n",
      "Epoch 100 Batch  250/2739 train_loss = 2.034\n",
      "Epoch 100 Batch  300/2739 train_loss = 2.499\n",
      "Epoch 100 Batch  350/2739 train_loss = 2.473\n",
      "Epoch 100 Batch  400/2739 train_loss = 1.645\n",
      "Epoch 100 Batch  450/2739 train_loss = 2.351\n",
      "Epoch 100 Batch  500/2739 train_loss = 2.697\n",
      "Epoch 100 Batch  550/2739 train_loss = 2.311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 Batch  600/2739 train_loss = 1.418\n",
      "Epoch 100 Batch  650/2739 train_loss = 2.049\n",
      "Epoch 100 Batch  700/2739 train_loss = 2.573\n",
      "Epoch 100 Batch  750/2739 train_loss = 3.181\n",
      "Epoch 100 Batch  800/2739 train_loss = 1.592\n",
      "Epoch 100 Batch  850/2739 train_loss = 2.194\n",
      "Epoch 100 Batch  900/2739 train_loss = 2.617\n",
      "Epoch 100 Batch  950/2739 train_loss = 1.487\n",
      "Epoch 100 Batch 1000/2739 train_loss = 2.107\n",
      "Epoch 100 Batch 1050/2739 train_loss = 3.484\n",
      "Epoch 100 Batch 1100/2739 train_loss = 3.103\n",
      "Epoch 100 Batch 1150/2739 train_loss = 2.384\n",
      "Epoch 100 Batch 1200/2739 train_loss = 1.508\n",
      "Epoch 100 Batch 1250/2739 train_loss = 1.628\n",
      "Epoch 100 Batch 1300/2739 train_loss = 1.958\n",
      "Epoch 100 Batch 1350/2739 train_loss = 2.129\n",
      "Epoch 100 Batch 1400/2739 train_loss = 3.655\n",
      "Epoch 100 Batch 1450/2739 train_loss = 1.966\n",
      "Epoch 100 Batch 1500/2739 train_loss = 2.215\n",
      "Epoch 100 Batch 1550/2739 train_loss = 2.591\n",
      "Epoch 100 Batch 1600/2739 train_loss = 3.914\n",
      "Epoch 100 Batch 1650/2739 train_loss = 2.588\n",
      "Epoch 100 Batch 1700/2739 train_loss = 2.791\n",
      "Epoch 100 Batch 1750/2739 train_loss = 3.137\n",
      "Epoch 100 Batch 1800/2739 train_loss = 2.116\n",
      "Epoch 100 Batch 1850/2739 train_loss = 1.071\n",
      "Epoch 100 Batch 1900/2739 train_loss = 1.403\n",
      "Epoch 100 Batch 1950/2739 train_loss = 1.393\n",
      "Epoch 100 Batch 2000/2739 train_loss = 2.434\n",
      "Epoch 100 Batch 2050/2739 train_loss = 3.857\n",
      "Epoch 100 Batch 2100/2739 train_loss = 3.171\n",
      "Epoch 100 Batch 2150/2739 train_loss = 3.716\n",
      "Epoch 100 Batch 2200/2739 train_loss = 3.619\n",
      "Epoch 100 Batch 2250/2739 train_loss = 1.921\n",
      "Epoch 100 Batch 2300/2739 train_loss = 3.182\n",
      "Epoch 100 Batch 2350/2739 train_loss = 2.640\n",
      "Epoch 100 Batch 2400/2739 train_loss = 2.705\n",
      "Epoch 100 Batch 2450/2739 train_loss = 2.524\n",
      "Epoch 100 Batch 2500/2739 train_loss = 3.534\n",
      "Epoch 100 Batch 2550/2739 train_loss = 2.271\n",
      "Epoch 100 Batch 2600/2739 train_loss = 1.269\n",
      "Epoch 100 Batch 2650/2739 train_loss = 2.327\n",
      "Epoch 100 Batch 2700/2739 train_loss = 3.778\n",
      "Epoch 101 Batch   11/2739 train_loss = 1.214\n",
      "Epoch 101 Batch   61/2739 train_loss = 2.699\n",
      "Epoch 101 Batch  111/2739 train_loss = 2.224\n",
      "Epoch 101 Batch  161/2739 train_loss = 2.483\n",
      "Epoch 101 Batch  211/2739 train_loss = 2.641\n",
      "Epoch 101 Batch  261/2739 train_loss = 1.774\n",
      "Epoch 101 Batch  311/2739 train_loss = 2.724\n",
      "Epoch 101 Batch  361/2739 train_loss = 1.650\n",
      "Epoch 101 Batch  411/2739 train_loss = 2.015\n",
      "Epoch 101 Batch  461/2739 train_loss = 2.477\n",
      "Epoch 101 Batch  511/2739 train_loss = 2.184\n",
      "Epoch 101 Batch  561/2739 train_loss = 1.906\n",
      "Epoch 101 Batch  611/2739 train_loss = 1.963\n",
      "Epoch 101 Batch  661/2739 train_loss = 1.742\n",
      "Epoch 101 Batch  711/2739 train_loss = 3.086\n",
      "Epoch 101 Batch  761/2739 train_loss = 3.491\n",
      "Epoch 101 Batch  811/2739 train_loss = 2.441\n",
      "Epoch 101 Batch  861/2739 train_loss = 2.627\n",
      "Epoch 101 Batch  911/2739 train_loss = 2.673\n",
      "Epoch 101 Batch  961/2739 train_loss = 3.914\n",
      "Epoch 101 Batch 1011/2739 train_loss = 2.051\n",
      "Epoch 101 Batch 1061/2739 train_loss = 1.087\n",
      "Epoch 101 Batch 1111/2739 train_loss = 2.420\n",
      "Epoch 101 Batch 1161/2739 train_loss = 1.730\n",
      "Epoch 101 Batch 1211/2739 train_loss = 1.607\n",
      "Epoch 101 Batch 1261/2739 train_loss = 1.584\n",
      "Epoch 101 Batch 1311/2739 train_loss = 2.745\n",
      "Epoch 101 Batch 1361/2739 train_loss = 1.511\n",
      "Epoch 101 Batch 1411/2739 train_loss = 4.012\n",
      "Epoch 101 Batch 1461/2739 train_loss = 1.915\n",
      "Epoch 101 Batch 1511/2739 train_loss = 1.530\n",
      "Epoch 101 Batch 1561/2739 train_loss = 3.366\n",
      "Epoch 101 Batch 1611/2739 train_loss = 2.216\n",
      "Epoch 101 Batch 1661/2739 train_loss = 2.017\n",
      "Epoch 101 Batch 1711/2739 train_loss = 1.796\n",
      "Epoch 101 Batch 1761/2739 train_loss = 4.039\n",
      "Epoch 101 Batch 1811/2739 train_loss = 2.680\n",
      "Epoch 101 Batch 1861/2739 train_loss = 3.621\n",
      "Epoch 101 Batch 1911/2739 train_loss = 3.649\n",
      "Epoch 101 Batch 1961/2739 train_loss = 3.105\n",
      "Epoch 101 Batch 2011/2739 train_loss = 16.743\n",
      "Epoch 101 Batch 2061/2739 train_loss = 3.733\n",
      "Epoch 101 Batch 2111/2739 train_loss = 5.739\n",
      "Epoch 101 Batch 2161/2739 train_loss = 4.435\n",
      "Epoch 101 Batch 2211/2739 train_loss = 3.283\n",
      "Epoch 101 Batch 2261/2739 train_loss = 4.632\n",
      "Epoch 101 Batch 2311/2739 train_loss = 4.165\n",
      "Epoch 101 Batch 2361/2739 train_loss = 3.436\n",
      "Epoch 101 Batch 2411/2739 train_loss = 3.755\n",
      "Epoch 101 Batch 2461/2739 train_loss = 3.871\n",
      "Epoch 101 Batch 2511/2739 train_loss = 3.602\n",
      "Epoch 101 Batch 2561/2739 train_loss = 3.533\n",
      "Epoch 101 Batch 2611/2739 train_loss = 2.735\n",
      "Epoch 101 Batch 2661/2739 train_loss = 2.324\n",
      "Epoch 101 Batch 2711/2739 train_loss = 5.124\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-99d3eed1512d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             }\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch_i\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mshow_every_n_batches\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 print('Epoch {:>3} Batch {:>4}/{} train_loss = {:.3f}'.format(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_batches, y_batches, x_batches_lengths, y_batches_lengths = get_batches(text, batch_size, \n",
    "                                                                         vocab_to_int, vocab_to_int['PAD'], \n",
    "                                                                         vocab_to_int['EOS'])\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: x_batches[0]})\n",
    "        for batch_i in range(len(x_batches)):\n",
    "            feed = {\n",
    "                input_text: x_batches[batch_i],\n",
    "                targets: y_batches[batch_i],\n",
    "                initial_state: state,\n",
    "                lr: learning_rate\n",
    "            }\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "            if (epoch_i * len(x_batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{} train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(x_batches),\n",
    "                    train_loss))\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
